"""
PDF ê¸°ë°˜ RAG ì‹œìŠ¤í…œ êµ¬í˜„
LangChain + Hugging Face Llama ëª¨ë¸ ì‚¬ìš©
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import warnings

warnings.filterwarnings("ignore")

# LangChain imports
from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Hugging Face imports
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
    BitsAndBytesConfig,
)
import torch

os.environ["TOKENIZERS_PARALLELISM"] = "true"  # í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # GPU ì‚¬ìš© ì„¤ì • (1ë²ˆ GPU)
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # CUDA ë™ê¸°í™” ëª¨ë“œ (ë””ë²„ê¹… ìš©ë„)
# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PDFRAGSystem:
    """PDF ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        pdf_dir: str = "./data/pdfs/",
        model_name: str = "meta-llama/Llama-2-7b-chat-hf",  # ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        vector_store_path: str = "./data/rag_vector_store/",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        top_k: int = 5,
        use_quantization: bool = True,
    ):
        self.pdf_dir = Path(pdf_dir)
        self.model_name = model_name
        self.embedding_model = embedding_model
        self.vector_store_path = Path(vector_store_path)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.top_k = top_k
        self.use_quantization = use_quantization

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.documents = None
        self.vectorstore = None
        self.llm = None
        self.qa_chain = None

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.vector_store_path.mkdir(parents=True, exist_ok=True)

    def load_pdfs(self) -> List[Any]:
        """PDF íŒŒì¼ë“¤ ë¡œë“œ"""
        logger.info(f"ğŸ“š Loading PDFs from {self.pdf_dir}")

        # DirectoryLoaderë¡œ ëª¨ë“  PDF íŒŒì¼ ë¡œë“œ
        loader = DirectoryLoader(
            str(self.pdf_dir),
            glob="**/*.pdf",
            loader_cls=PyMuPDFLoader,
            show_progress=True,
        )

        documents = loader.load()
        logger.info(f"âœ… Loaded {len(documents)} PDF pages")

        return documents

    def split_documents(self, documents: List[Any]) -> List[Any]:
        """ë¬¸ì„œ ì²­í‚¹"""
        logger.info("ğŸ”ª Splitting documents into chunks")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", " ", ""],
        )

        chunks = text_splitter.split_documents(documents)
        logger.info(f"âœ… Created {len(chunks)} chunks")

        return chunks

    def create_embeddings(self) -> HuggingFaceEmbeddings:
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info(f"ğŸ§® Initializing embedding model: {self.embedding_model}")

        embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model,
            model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
            encode_kwargs={"normalize_embeddings": True},
        )

        return embeddings

    def create_vector_store(
        self, chunks: List[Any], embeddings: HuggingFaceEmbeddings
    ) -> FAISS:
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë“œ"""
        vector_store_file = self.vector_store_path / "faiss_index"

        if vector_store_file.exists():
            logger.info("ğŸ“‚ Loading existing vector store")
            vectorstore = FAISS.load_local(
                str(vector_store_file), embeddings, allow_dangerous_deserialization=True
            )
        else:
            logger.info("ğŸ—ï¸ Creating new vector store")
            vectorstore = FAISS.from_documents(chunks, embeddings)
            vectorstore.save_local(str(vector_store_file))
            logger.info(f"ğŸ’¾ Vector store saved to {vector_store_file}")

        return vectorstore

    def setup_llm(self) -> HuggingFacePipeline:
        """Llama ëª¨ë¸ ì„¤ì •"""
        logger.info(f"ğŸ¤– Setting up LLM: {self.model_name}")

        # ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
        quantization_config = None
        if self.use_quantization:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="left",  # ìƒì„±ì„ ìœ„í•´ left padding
            clean_up_tokenization_spaces=True,
        )

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )

        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.1,
            do_sample=False,
            repetition_penalty=1.1,
            return_full_text=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

        # LangChain LLM ë˜í¼
        llm = HuggingFacePipeline(pipeline=pipe)

        return llm

    def create_qa_chain(
        self, llm: HuggingFacePipeline, vectorstore: FAISS
    ) -> RetrievalQA:
        """QA ì²´ì¸ ìƒì„±"""
        logger.info("ğŸ”— Creating QA chain")

        prompt_template = """Please answer the following question in Korean based on the provided context. 
Provide a comprehensive and technical answer that is easy to understand.

Context: {context}

Question: {question}

Please provide your answer in Korean:"""

        PROMPT = PromptTemplate(
            template=prompt_template, input_variables=["context", "question"]
        )

        # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        retriever = vectorstore.as_retriever(
            search_type="similarity", search_kwargs={"k": self.top_k}
        )

        # QA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True,
        )

        return qa_chain

    def initialize(self):
        """RAG ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”"""
        logger.info("ğŸš€ Initializing PDF RAG System")

        # 1. PDF ë¡œë“œ
        self.documents = self.load_pdfs()

        # 2. ë¬¸ì„œ ì²­í‚¹
        chunks = self.split_documents(self.documents)

        # 3. ì„ë² ë”© ìƒì„±
        embeddings = self.create_embeddings()

        # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vectorstore = self.create_vector_store(chunks, embeddings)

        # 5. LLM ì„¤ì •
        self.llm = self.setup_llm()

        # 6. QA ì²´ì¸ ìƒì„±
        self.qa_chain = self.create_qa_chain(self.llm, self.vectorstore)

        logger.info("âœ… PDF RAG System initialized successfully!")

    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        if self.qa_chain is None:
            raise ValueError("RAG system not initialized. Call initialize() first.")

        logger.info(f"â“ Processing question: {question}")

        # try:
        result = self.qa_chain({"query": question})

        response = {
            "question": question,
            "answer": result["result"],
            "source_documents": [
                {
                    "content": doc.page_content[:200] + "...",
                    "source": doc.metadata.get("source", "Unknown"),
                    "page": doc.metadata.get("page", "Unknown"),
                }
                for doc in result["source_documents"]
            ],
        }

        return response

    # except Exception as e:
    #     logger.error(f"âŒ Error processing question: {e}")
    #     return {
    #         "question": question,
    #         "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
    #         "source_documents": [],
    #     }

    def add_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì¶”ê°€ (ì„ íƒì‚¬í•­)"""
        memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )
        return memory


def main():
    import argparse

    parser = argparse.ArgumentParser(description="PDF ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì‹¤í–‰")
    parser.add_argument(
        "--pdf_dir", type=str, default="./data/pdfs/", help="PDF íŒŒì¼ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--queries", type=str, default="./queries.txt", help="query text íŒŒì¼"
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="meta-llama/Llama-3.1-8B-Instruct",
        help="LLM ëª¨ë¸ ì´ë¦„",
    )
    parser.add_argument(
        "--embedding_model",
        type=str,
        default="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        help="ì„ë² ë”© ëª¨ë¸ ì´ë¦„",
    )
    args = parser.parse_args()

    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = PDFRAGSystem(
        pdf_dir=args.pdf_dir,
        model_name=args.model_name,  # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¡œ ë³€ê²½
        embedding_model=args.embedding_model,
        use_quantization=True,  # GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ True
    )

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìµœì´ˆ ì‹¤í–‰ì‹œë§Œ)
    rag_system.initialize()

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    with open(args.queries, "r", encoding="utf-8") as f:
        test_questions = [line.strip() for line in f if line.strip()]

    # ì§ˆë¬¸ ì²˜ë¦¬
    for question in test_questions:
        print(f"\n{'='*60}")
        print(f"ì§ˆë¬¸: {question}")
        print(f"{'='*60}")

        result = rag_system.query(question)

        print(f"ë‹µë³€: {result['answer']}")
        print(f"\nì°¸ê³  ë¬¸ì„œ:")
        for i, doc in enumerate(result["source_documents"], 1):
            print(f"{i}. {doc['source']} (í˜ì´ì§€: {doc['page']})")
            print(f"   ë‚´ìš©: {doc['content']}")

        result = rag_system.query(question)
        print(result)

        import pdb

        pdb.set_trace()
        print(f"\në‹µë³€: {result['answer']}")


if __name__ == "__main__":
    main()
