# retrieval_debug_with_logging.py - ì¶œë ¥ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë””ë²„ê¹…

import logging
from pathlib import Path
import sys
from datetime import datetime


class TeeOutput:
    """ì½˜ì†”ê³¼ íŒŒì¼ì— ë™ì‹œ ì¶œë ¥í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, filename):
        self.terminal = sys.stdout
        self.logfile = open(filename, "w", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.logfile.write(message)
        self.logfile.flush()  # ì¦‰ì‹œ íŒŒì¼ì— ì“°ê¸°

    def flush(self):
        self.terminal.flush()
        self.logfile.flush()

    def close(self):
        self.logfile.close()


def setup_logging_to_file():
    """ì¶œë ¥ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ë„ë¡ ì„¤ì •"""

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("debug_logs")
    log_dir.mkdir(exist_ok=True)

    # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ íŒŒì¼ëª…
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = log_dir / f"retrieval_debug_{timestamp}.txt"

    print(f"ğŸ“ ì¶œë ¥ì´ ë‹¤ìŒ íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤: {log_filename}")

    # stdoutì„ íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œ ì¶œë ¥í•˜ë„ë¡ ë³€ê²½
    sys.stdout = TeeOutput(log_filename)

    return log_filename


def debug_retrieval_process(question: str):
    """Retrieval ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë””ë²„ê¹… (ì¶•ì•½ëœ ì¶œë ¥)"""

    print(f"ğŸ” ë””ë²„ê¹… ì§ˆë¬¸: {question}")
    print("=" * 80)

    try:
        from src.graphrag.langchain.custom_retriever import create_graphrag_retriever

        # Retriever ìƒì„±
        retriever = create_graphrag_retriever(
            unified_graph_path="data/processed/graphs/unified/unified_knowledge_graph.json",
            vector_store_path="data/processed/vector_store",
            embedding_model="auto",
            max_docs=10,
            min_relevance_score=0.1,
            enable_caching=False,
        )

        print("âœ… Retriever ìƒì„± ì™„ë£Œ")

        # ê²€ìƒ‰ ì‹¤í–‰
        print("\nğŸ“‹ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
        documents = retriever.get_relevant_documents(question)
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

        if documents:
            main_doc = documents[0]
            total_nodes = main_doc.metadata.get("total_nodes", 0)
            confidence = main_doc.metadata.get("confidence_score", 0.0)

            print(f"âœ… ê²€ìƒ‰ ì„±ê³µ!")
            print(f"   ë…¸ë“œ ìˆ˜: {total_nodes}")
            print(f"   ì‹ ë¢°ë„: {confidence:.3f}")

            # ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°ë§Œ (ì „ì²´ëŠ” íŒŒì¼ì—ë§Œ)
            content_preview = main_doc.page_content[:100]
            print(f"   ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {content_preview}...")

            # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ëŠ” íŒŒì¼ì—ë§Œ ì €ì¥
            print(f"\nğŸ“„ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ (íŒŒì¼ì—ë§Œ ì €ì¥):")
            print("=" * 60)
            print(main_doc.page_content)
            print("=" * 60)

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            print(f"\nğŸ¯ í”„ë¡¬í”„íŠ¸ ìƒì„±...")
            from src.graphrag.langchain.prompt_templates import GraphRAGPromptTemplates

            prompt_builder = GraphRAGPromptTemplates()
            prompt_template = prompt_builder.create_langchain_prompt()

            context = "\n\n".join([doc.page_content for doc in documents])
            formatted_prompt = prompt_template.format(
                context=context, question=question
            )

            print(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(formatted_prompt)} ë¬¸ì")
            print(f"í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {formatted_prompt[:150]}...")

            # ì „ì²´ í”„ë¡¬í”„íŠ¸ëŠ” íŒŒì¼ì—ë§Œ ì €ì¥
            print(f"\nğŸ“„ ì „ì²´ í”„ë¡¬í”„íŠ¸ (íŒŒì¼ì—ë§Œ ì €ì¥):")
            print("=" * 60)
            print(formatted_prompt)
            print("=" * 60)

            return True
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return False

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_different_models_quick():
    """ë¹ ë¥¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    print(f"\nğŸ¤– ë¹ ë¥¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    models = [
        "paraphrase-multilingual-mpnet-base-v2",  # 768ì°¨ì›
        "all-MiniLM-L6-v2",  # 384ì°¨ì›
        "auto",  # ìë™ ì„ íƒ
    ]

    question = "battery machine learning"

    for model in models:
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë¸: {model}")

        try:
            from src.graphrag.langchain.custom_retriever import (
                create_graphrag_retriever,
            )

            retriever = create_graphrag_retriever(
                unified_graph_path="data/processed/graphs/unified/unified_knowledge_graph.json",
                vector_store_path="data/processed/vector_store",
                embedding_model=model,
                max_docs=5,
                min_relevance_score=0.1,
                enable_caching=False,
            )

            docs = retriever.get_relevant_documents(question)

            if docs:
                total_nodes = docs[0].metadata.get("total_nodes", 0)
                confidence = docs[0].metadata.get("confidence_score", 0.0)
                print(f"   âœ… ì„±ê³µ: {total_nodes}ê°œ ë…¸ë“œ, ì‹ ë¢°ë„: {confidence:.3f}")
            else:
                print(f"   âŒ ì‹¤íŒ¨: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        except Exception as e:
            print(f"   âŒ ì—ëŸ¬: {str(e)[:50]}...")


def test_config_manager_usage():
    """Config Manager ì‚¬ìš© í…ŒìŠ¤íŠ¸"""

    print(f"\nğŸ”§ Config Manager ì‚¬ìš© í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    try:
        from src.graphrag.config_manager import GraphRAGConfigManager

        config_manager = GraphRAGConfigManager("graphrag_config.yaml")
        embedding_config = config_manager.get_embeddings_config()

        print(f"âœ… Config Manager ë¡œë“œ ì„±ê³µ")
        print(f"   YAML ëª¨ë¸: {embedding_config['model_name']}")

        # Config Managerë¥¼ ì‚¬ìš©í•œ Retriever í…ŒìŠ¤íŠ¸
        from src.graphrag.langchain.custom_retriever import create_graphrag_retriever

        retriever = create_graphrag_retriever(
            unified_graph_path="data/processed/graphs/unified/unified_knowledge_graph.json",
            vector_store_path="data/processed/vector_store",
            embedding_model="auto",
            config_manager=config_manager,  # í•µì‹¬: config_manager ì „ë‹¬
            max_docs=5,
            min_relevance_score=0.1,
            enable_caching=False,
        )

        docs = retriever.get_relevant_documents("battery prediction")

        if docs and docs[0].metadata.get("total_nodes", 0) > 0:
            print(f"âœ… Config Manager ê¸°ë°˜ ê²€ìƒ‰ ì„±ê³µ!")
            total_nodes = docs[0].metadata.get("total_nodes", 0)
            print(f"   ë…¸ë“œ: {total_nodes}ê°œ")
            return True
        else:
            print(f"âŒ Config Manager ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"âŒ Config Manager í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # íŒŒì¼ ë¡œê¹… ì„¤ì •
    log_filename = setup_logging_to_file()

    print("ğŸš€ GraphRAG Retrieval ë””ë²„ê¹… ì‹œì‘ (íŒŒì¼ ì €ì¥ ë²„ì „)")
    print("=" * 80)
    print(f"ğŸ“ ìƒì„¸ ì¶œë ¥ì€ {log_filename}ì— ì €ì¥ë©ë‹ˆë‹¤")
    print("ğŸ’¡ ì½˜ì†”ì—ëŠ” ìš”ì•½ë§Œ ì¶œë ¥ë©ë‹ˆë‹¤")
    print("=" * 80)

    try:
        # 1. Config Manager í…ŒìŠ¤íŠ¸
        config_success = test_config_manager_usage()

        # 2. ë¹ ë¥¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        test_different_models_quick()

        # 3. ì „ì²´ retrieval í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸ (ì„±ê³µí•œ ê²½ìš°ë§Œ)
        if config_success:
            question = (
                "What machine learning techniques are used for battery SoC prediction?"
            )
            print(f"\nğŸ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸: {question}")
            success = debug_retrieval_process(question)

            if success:
                print(f"\nğŸ‰ ë””ë²„ê¹… ì„±ê³µ! ì´ì œ ì •ìƒì ì¸ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                print(f"\nâš ï¸ ë””ë²„ê¹…ì—ì„œ ë¬¸ì œ ë°œê²¬ë¨")
        else:
            print(f"\nâš ï¸ Config Manager ë¬¸ì œë¡œ ì „ì²´ í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")

        print(f"\nğŸ“ ìƒì„¸ ë¡œê·¸ëŠ” ë‹¤ìŒ íŒŒì¼ì—ì„œ í™•ì¸í•˜ì„¸ìš”:")
        print(f"   {log_filename}")

    finally:
        # stdout ë³µì›
        if hasattr(sys.stdout, "close"):
            sys.stdout.close()
            sys.stdout = sys.__stdout__


if __name__ == "__main__":
    main()
