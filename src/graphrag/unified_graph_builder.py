"""
í†µí•© ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Unified Knowledge Graph Builder Module

6ê°œì˜ ê°œë³„ ê·¸ë˜í”„ë¥¼ í•˜ë‚˜ì˜ í†µí•©ëœ ì§€ì‹ ê·¸ë˜í”„ë¡œ ê²°í•©
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from tqdm import tqdm
import logging
from typing import Dict, List, Set, Tuple, Optional, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UnifiedKnowledgeGraphBuilder:
    """6ê°œì˜ ê°œë³„ ê·¸ë˜í”„ë¥¼ í†µí•©ëœ ì§€ì‹ ê·¸ë˜í”„ë¡œ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, graphs_dir: Path):
        """
        Args:
            graphs_dir (Path): ê°œë³„ ê·¸ë˜í”„ íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
        """
        self.graphs_dir = Path(graphs_dir)
        self.unified_graph = nx.MultiDiGraph()  # ë‹¤ì¤‘ ì—£ì§€ + ë°©í–¥ì„± ì§€ì›

        # ê·¸ë˜í”„ë³„ íŒŒì¼ ê²½ë¡œ ì •ì˜
        self.graph_files = {
            "citation": "citation_network_graph.json",
            "keyword": "keyword_cooccurrence_graph.json",
            "semantic": "semantic_similarity_network_graph.json",
            "author_collab": "author_collaboration_graph.json",
            "author_paper": "author_paper_graph.json",
            "journal_paper": "journal_paper_graph.json",
        }

        # ë…¸ë“œ íƒ€ì…ë³„ í†µê³„
        self.node_stats = defaultdict(int)
        self.edge_stats = defaultdict(int)

        # í†µí•© ê³¼ì •ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ ì¶”ì 
        self.integration_issues = {
            "missing_files": [],
            "node_conflicts": [],
            "data_inconsistencies": [],
        }

    def load_individual_graph(self, graph_name: str) -> Optional[nx.Graph]:
        """ê°œë³„ ê·¸ë˜í”„ íŒŒì¼ ë¡œë“œ"""
        file_path = self.graphs_dir / self.graph_files[graph_name]

        if not file_path.exists():
            logger.warning(f"Graph file not found: {file_path}")
            self.integration_issues["missing_files"].append(str(file_path))
            return None

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                graph_data = json.load(f)

            # NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜
            if graph_name in ["citation", "semantic"]:
                G = nx.DiGraph()  # ë°©í–¥ ê·¸ë˜í”„
            else:
                G = nx.Graph()  # ë¬´ë°©í–¥ ê·¸ë˜í”„

            # ë…¸ë“œ ì¶”ê°€
            for node_data in graph_data.get("nodes", []):
                node_id = node_data["id"]
                attributes = {k: v for k, v in node_data.items() if k != "id"}
                G.add_node(node_id, **attributes)

            # ì—£ì§€ ì¶”ê°€
            for edge_data in graph_data.get("edges", []):
                source = edge_data["source"]
                target = edge_data["target"]
                attributes = {
                    k: v for k, v in edge_data.items() if k not in ["source", "target"]
                }
                G.add_edge(source, target, **attributes)

            logger.info(
                f"âœ… {graph_name} graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges"
            )
            return G

        except Exception as e:
            logger.error(f"âŒ Error loading {graph_name} graph: {e}")
            return None

    def standardize_node_attributes(
        self, node_id: str, attributes: Dict[str, Any], source_graph: str
    ) -> Dict[str, Any]:
        """ë…¸ë“œ ì†ì„± í‘œì¤€í™” ë° ë³´ê°•"""
        standardized = attributes.copy()

        # ê³µí†µ ì†ì„± ì¶”ê°€
        standardized["source_graphs"] = [source_graph]
        standardized["integration_timestamp"] = pd.Timestamp.now().isoformat()

        # ë…¸ë“œ íƒ€ì…ë³„ í‘œì¤€í™”
        if "node_type" not in standardized:
            # ë…¸ë“œ íƒ€ì… ì¶”ë¡ 
            if node_id.startswith("paper_"):
                standardized["node_type"] = "paper"
            elif any(char.isalpha() and char.islower() for char in node_id):
                if " " in node_id:
                    standardized["node_type"] = "author"
                else:
                    standardized["node_type"] = "keyword"
            else:
                standardized["node_type"] = "journal"

        # íƒ€ì…ë³„ í•„ìˆ˜ ì†ì„± í™•ì¸
        node_type = standardized["node_type"]

        if node_type == "paper":
            required_attrs = ["title", "year", "authors"]
            for attr in required_attrs:
                if attr not in standardized:
                    standardized[attr] = "Unknown"

        elif node_type == "author":
            if "name" not in standardized:
                standardized["name"] = node_id

        elif node_type == "keyword":
            if "frequency" not in standardized:
                standardized["frequency"] = 1

        elif node_type == "journal":
            if "name" not in standardized:
                standardized["name"] = node_id

        return standardized

    def merge_duplicate_nodes(
        self,
        node_id: str,
        new_attributes: Dict[str, Any],
        existing_attributes: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ì¤‘ë³µ ë…¸ë“œì˜ ì†ì„± ë³‘í•©"""
        merged = existing_attributes.copy()

        # source_graphs ë³‘í•©
        existing_sources = set(merged.get("source_graphs", []))
        new_sources = set(new_attributes.get("source_graphs", []))
        merged["source_graphs"] = list(existing_sources | new_sources)

        # ë…¸ë“œ íƒ€ì…ë³„ íŠ¹ë³„ ì²˜ë¦¬
        node_type = merged.get("node_type", "unknown")

        if node_type == "paper":
            # ë…¼ë¬¸ ì •ë³´ëŠ” ë” ì™„ì „í•œ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
            for key in ["title", "authors", "keywords", "abstract"]:
                if key in new_attributes and key in merged:
                    # ë” ê¸´/ìƒì„¸í•œ ì •ë³´ ì„ íƒ
                    if len(str(new_attributes[key])) > len(str(merged[key])):
                        merged[key] = new_attributes[key]
                elif key in new_attributes:
                    merged[key] = new_attributes[key]

        elif node_type == "author":
            # ì €ì í†µê³„ ì •ë³´ ë³‘í•© (ìµœëŒ€ê°’ ì„ íƒ)
            numeric_fields = ["paper_count", "collaborator_count", "first_author_count"]
            for field in numeric_fields:
                if field in new_attributes and field in merged:
                    merged[field] = max(merged[field], new_attributes[field])
                elif field in new_attributes:
                    merged[field] = new_attributes[field]

        elif node_type == "keyword":
            # í‚¤ì›Œë“œ ë¹ˆë„ í•©ì‚°
            if "frequency" in new_attributes and "frequency" in merged:
                merged["frequency"] = merged["frequency"] + new_attributes["frequency"]

        # ê¸°íƒ€ ìƒˆë¡œìš´ ì†ì„± ì¶”ê°€
        for key, value in new_attributes.items():
            if key not in merged and key not in ["source_graphs"]:
                merged[key] = value

        return merged

    def standardize_edge_attributes(
        self, edge_data: Dict[str, Any], source_graph: str
    ) -> Dict[str, Any]:
        """ì—£ì§€ ì†ì„± í‘œì¤€í™”"""
        standardized = edge_data.copy()

        # ê³µí†µ ì†ì„± ì¶”ê°€
        standardized["source_graph"] = source_graph
        standardized["integration_timestamp"] = pd.Timestamp.now().isoformat()

        # edge_type í‘œì¤€í™”
        if "edge_type" not in standardized:
            # ì†ŒìŠ¤ ê·¸ë˜í”„ì— ë”°ë¥¸ ê¸°ë³¸ edge_type ì„¤ì •
            edge_type_mapping = {
                "citation": "cites",
                "keyword": "co_occurs_with",
                "semantic": "semantically_similar_to",
                "author_collab": "collaborates_with",
                "author_paper": "authored_by",
                "journal_paper": "published_in",
            }
            standardized["edge_type"] = edge_type_mapping.get(
                source_graph, "related_to"
            )

        # ê°€ì¤‘ì¹˜ í‘œì¤€í™” (0-1 ë²”ìœ„ë¡œ)
        if "weight" in standardized:
            weight = standardized["weight"]
            if isinstance(weight, (int, float)) and weight > 1:
                # 1ë³´ë‹¤ í° ê°€ì¤‘ì¹˜ëŠ” ì •ê·œí™” (ë¡œê·¸ ìŠ¤ì¼€ì¼ ê³ ë ¤)
                standardized["normalized_weight"] = min(1.0, weight / 100.0)
            else:
                standardized["normalized_weight"] = float(weight)
        else:
            standardized["normalized_weight"] = 1.0

        return standardized

    def add_cross_graph_edges(self):
        """ê·¸ë˜í”„ ê°„ ì¶”ê°€ ì—°ê²° ì—£ì§€ ìƒì„±"""
        logger.info("ğŸ”— Creating cross-graph connections...")

        # ë…¼ë¬¸-í‚¤ì›Œë“œ ì—°ê²° (keywords ì†ì„± ê¸°ë°˜)
        self._connect_papers_to_keywords()

        # ì €ì-í‚¤ì›Œë“œ ì—°ê²° (ì €ìì˜ ë…¼ë¬¸ í‚¤ì›Œë“œ ê¸°ë°˜)
        self._connect_authors_to_keywords()

        # ì €ë„-í‚¤ì›Œë“œ ì—°ê²° (ì €ë„ ë…¼ë¬¸ë“¤ì˜ í‚¤ì›Œë“œ ê¸°ë°˜)
        self._connect_journals_to_keywords()

        # ì‹œê°„ì  ì—°ê²° (ê°™ì€ ì—°ë„ ë…¼ë¬¸ë“¤)
        self._create_temporal_connections()

    def _connect_papers_to_keywords(self):
        """ë…¼ë¬¸ê³¼ í‚¤ì›Œë“œ ê°„ ì—°ê²° ìƒì„±"""
        paper_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "paper"
        ]
        keyword_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "keyword"
        ]

        connections_added = 0

        for paper_id in paper_nodes:
            paper_data = self.unified_graph.nodes[paper_id]
            paper_keywords = paper_data.get("keywords", [])

            if isinstance(paper_keywords, str):
                paper_keywords = [kw.strip() for kw in paper_keywords.split(";")]

            for keyword in paper_keywords:
                keyword_clean = keyword.lower().strip()

                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë…¸ë“œ ì°¾ê¸°
                matching_keyword = None
                for kw_node in keyword_nodes:
                    if kw_node.lower() == keyword_clean:
                        matching_keyword = kw_node
                        break

                if matching_keyword and not self.unified_graph.has_edge(
                    paper_id, matching_keyword
                ):
                    self.unified_graph.add_edge(
                        paper_id,
                        matching_keyword,
                        edge_type="has_keyword",
                        source_graph="cross_connection",
                        weight=1.0,
                        normalized_weight=1.0,
                    )
                    connections_added += 1

        logger.info(f"ğŸ“ Added {connections_added} paper-keyword connections")

    def _connect_authors_to_keywords(self):
        """ì €ìì™€ í‚¤ì›Œë“œ ê°„ ì—°ê²° ìƒì„± (ì €ì ë…¼ë¬¸ì˜ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        author_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "author"
        ]

        connections_added = 0

        for author_id in author_nodes:
            # ì €ìì˜ ëª¨ë“  ë…¼ë¬¸ ì°¾ê¸°
            author_papers = []
            for edge in self.unified_graph.edges(data=True):
                if (
                    edge[2].get("edge_type") == "authored_by" and edge[1] == author_id
                ):  # targetì´ author
                    author_papers.append(edge[0])  # sourceëŠ” paper

            # ì €ì ë…¼ë¬¸ë“¤ì˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
            author_keywords = Counter()
            for paper_id in author_papers:
                paper_data = self.unified_graph.nodes.get(paper_id, {})
                paper_keywords = paper_data.get("keywords", [])

                if isinstance(paper_keywords, str):
                    paper_keywords = [kw.strip() for kw in paper_keywords.split(";")]

                for keyword in paper_keywords:
                    if keyword.strip():
                        author_keywords[keyword.lower().strip()] += 1

            # ë¹ˆë„ ë†’ì€ í‚¤ì›Œë“œì™€ ì—°ê²° (ë¹ˆë„ 2 ì´ìƒ)
            for keyword, freq in author_keywords.items():
                if freq >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ì‚¬ìš©í•œ í‚¤ì›Œë“œ
                    # í•´ë‹¹ í‚¤ì›Œë“œ ë…¸ë“œ ì°¾ê¸°
                    keyword_nodes = [
                        n
                        for n in self.unified_graph.nodes()
                        if (
                            self.unified_graph.nodes[n].get("node_type") == "keyword"
                            and n.lower() == keyword
                        )
                    ]

                    for kw_node in keyword_nodes:
                        if not self.unified_graph.has_edge(author_id, kw_node):
                            self.unified_graph.add_edge(
                                author_id,
                                kw_node,
                                edge_type="specializes_in",
                                source_graph="cross_connection",
                                frequency=freq,
                                weight=min(1.0, freq / 10.0),  # ë¹ˆë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜
                                normalized_weight=min(1.0, freq / 10.0),
                            )
                            connections_added += 1

        logger.info(f"ğŸ‘¥ Added {connections_added} author-keyword connections")

    def _connect_journals_to_keywords(self):
        """ì €ë„ê³¼ í‚¤ì›Œë“œ ê°„ ì—°ê²° ìƒì„±"""
        journal_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "journal"
        ]

        connections_added = 0

        for journal_id in journal_nodes:
            # ì €ë„ì˜ ëª¨ë“  ë…¼ë¬¸ ì°¾ê¸°
            journal_papers = []
            for edge in self.unified_graph.edges(data=True):
                if (
                    edge[2].get("edge_type") == "published_in" and edge[1] == journal_id
                ):  # targetì´ journal
                    journal_papers.append(edge[0])  # sourceëŠ” paper

            # ì €ë„ ë…¼ë¬¸ë“¤ì˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
            journal_keywords = Counter()
            for paper_id in journal_papers:
                paper_data = self.unified_graph.nodes.get(paper_id, {})
                paper_keywords = paper_data.get("keywords", [])

                if isinstance(paper_keywords, str):
                    paper_keywords = [kw.strip() for kw in paper_keywords.split(";")]

                for keyword in paper_keywords:
                    if keyword.strip():
                        journal_keywords[keyword.lower().strip()] += 1

            # ìƒìœ„ í‚¤ì›Œë“œë“¤ê³¼ ì—°ê²° (ìƒìœ„ 20%ë§Œ)
            if journal_keywords:
                top_keywords = journal_keywords.most_common(
                    max(5, len(journal_keywords) // 5)
                )

                for keyword, freq in top_keywords:
                    keyword_nodes = [
                        n
                        for n in self.unified_graph.nodes()
                        if (
                            self.unified_graph.nodes[n].get("node_type") == "keyword"
                            and n.lower() == keyword
                        )
                    ]

                    for kw_node in keyword_nodes:
                        if not self.unified_graph.has_edge(journal_id, kw_node):
                            self.unified_graph.add_edge(
                                journal_id,
                                kw_node,
                                edge_type="focuses_on",
                                source_graph="cross_connection",
                                frequency=freq,
                                weight=min(1.0, freq / max(journal_keywords.values())),
                                normalized_weight=min(
                                    1.0, freq / max(journal_keywords.values())
                                ),
                            )
                            connections_added += 1

        logger.info(f"ğŸ“° Added {connections_added} journal-keyword connections")

    def _create_temporal_connections(self):
        """ì‹œê°„ì  ê·¼ì ‘ì„± ê¸°ë°˜ ì—°ê²° ìƒì„±"""
        paper_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "paper"
        ]

        # ì—°ë„ë³„ ë…¼ë¬¸ ê·¸ë£¹í™”
        papers_by_year = defaultdict(list)
        for paper_id in paper_nodes:
            year = self.unified_graph.nodes[paper_id].get("year", "")
            if year and str(year).isdigit():
                papers_by_year[int(year)].append(paper_id)

        connections_added = 0

        # ê°™ì€ ì—°ë„ ë…¼ë¬¸ë“¤ ê°„ ì•½í•œ ì—°ê²° (ìƒ˜í”Œë§)
        for year, papers in papers_by_year.items():
            if len(papers) > 1:
                # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§ (ìµœëŒ€ 50ê°œ ë…¼ë¬¸ë§Œ)
                if len(papers) > 50:
                    import random

                    papers = random.sample(papers, 50)

                # ëª¨ë“  ìŒì´ ì•„ë‹Œ ì¼ë¶€ë§Œ ì—°ê²° (computational cost ê³ ë ¤)
                for i in range(min(10, len(papers))):  # ê° ë…¼ë¬¸ë‹¹ ìµœëŒ€ 10ê°œë§Œ ì—°ê²°
                    for j in range(i + 1, min(i + 11, len(papers))):
                        paper1, paper2 = papers[i], papers[j]

                        if not self.unified_graph.has_edge(paper1, paper2):
                            self.unified_graph.add_edge(
                                paper1,
                                paper2,
                                edge_type="temporal_proximity",
                                source_graph="cross_connection",
                                year=year,
                                weight=0.1,  # ì•½í•œ ì—°ê²°
                                normalized_weight=0.1,
                            )
                            connections_added += 1

        logger.info(f"â° Added {connections_added} temporal proximity connections")

    def calculate_unified_statistics(self):
        """í†µí•© ê·¸ë˜í”„ í†µê³„ ê³„ì‚°"""
        logger.info("ğŸ“Š Calculating unified graph statistics...")

        # ë…¸ë“œ íƒ€ì…ë³„ í†µê³„
        node_types = defaultdict(int)
        for node in self.unified_graph.nodes():
            node_type = self.unified_graph.nodes[node].get("node_type", "unknown")
            node_types[node_type] += 1

        # ì—£ì§€ íƒ€ì…ë³„ í†µê³„
        edge_types = defaultdict(int)
        for edge in self.unified_graph.edges(data=True):
            edge_type = edge[2].get("edge_type", "unknown")
            edge_types[edge_type] += 1

        # ì†ŒìŠ¤ ê·¸ë˜í”„ë³„ ê¸°ì—¬ë„
        source_contributions = defaultdict(lambda: {"nodes": 0, "edges": 0})

        for node in self.unified_graph.nodes():
            sources = self.unified_graph.nodes[node].get("source_graphs", ["unknown"])
            for source in sources:
                source_contributions[source]["nodes"] += 1

        for edge in self.unified_graph.edges(data=True):
            source = edge[2].get("source_graph", "unknown")
            source_contributions[source]["edges"] += 1

        # ì—°ê²°ì„± ë¶„ì„
        if nx.is_connected(self.unified_graph.to_undirected()):
            connectivity = "fully_connected"
            largest_component_size = self.unified_graph.number_of_nodes()
        else:
            components = list(
                nx.connected_components(self.unified_graph.to_undirected())
            )
            connectivity = "disconnected"
            largest_component_size = len(max(components, key=len))

        stats = {
            "basic_info": {
                "total_nodes": self.unified_graph.number_of_nodes(),
                "total_edges": self.unified_graph.number_of_edges(),
                "density": nx.density(self.unified_graph),
                "connectivity": connectivity,
                "largest_component_size": largest_component_size,
            },
            "node_types": dict(node_types),
            "edge_types": dict(edge_types),
            "source_contributions": dict(source_contributions),
            "integration_issues": self.integration_issues,
        }

        return stats

    def save_unified_graph(
        self, output_dir: Path, save_formats: List[str] = ["json", "graphml"]
    ):
        """í†µí•© ê·¸ë˜í”„ ì €ì¥"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        saved_files = []

        # JSON í˜•íƒœë¡œ ì €ì¥ (GraphRAGì—ì„œ ì‚¬ìš©)
        if "json" in save_formats:
            graph_data = {"nodes": [], "edges": []}

            # ë…¸ë“œ ì •ë³´
            for node in self.unified_graph.nodes():
                node_data = self.unified_graph.nodes[node].copy()
                node_data["id"] = node

                # ë¦¬ìŠ¤íŠ¸/ë³µì¡í•œ íƒ€ì… ì²˜ë¦¬
                for key, value in node_data.items():
                    if isinstance(value, (list, set)):
                        node_data[key] = list(value)
                    elif not isinstance(value, (str, int, float, bool, type(None))):
                        node_data[key] = str(value)

                graph_data["nodes"].append(node_data)

            # ì—£ì§€ ì •ë³´
            for edge in self.unified_graph.edges(data=True):
                edge_data = edge[2].copy()
                edge_data["source"] = edge[0]
                edge_data["target"] = edge[1]

                # ë¦¬ìŠ¤íŠ¸/ë³µì¡í•œ íƒ€ì… ì²˜ë¦¬
                for key, value in edge_data.items():
                    if isinstance(value, (list, set)):
                        edge_data[key] = list(value)
                    elif not isinstance(value, (str, int, float, bool, type(None))):
                        edge_data[key] = str(value)

                graph_data["edges"].append(edge_data)

            json_file = output_dir / "unified_knowledge_graph.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(graph_data, f, ensure_ascii=False, indent=2)

            saved_files.append(json_file)
            logger.info(f"ğŸ’¾ JSON graph saved: {json_file}")

        # GraphML í˜•íƒœë¡œ ì €ì¥ (Gephi, Cytoscape ë“±ì—ì„œ ì‚¬ìš©)
        if "graphml" in save_formats:
            try:
                # GraphML í˜¸í™˜ì„ ìœ„í•´ ë³µì¡í•œ ì†ì„± ë¬¸ìì—´í™”
                G_graphml = self.unified_graph.copy()

                for node in G_graphml.nodes():
                    for attr_name, attr_value in G_graphml.nodes[node].items():
                        if isinstance(attr_value, (list, set)):
                            G_graphml.nodes[node][attr_name] = ";".join(
                                str(v) for v in attr_value
                            )
                        elif not isinstance(attr_value, (str, int, float, bool)):
                            G_graphml.nodes[node][attr_name] = str(attr_value)

                for edge in G_graphml.edges(data=True):
                    edge_data = edge[2]
                    for attr_name, attr_value in edge_data.items():
                        if isinstance(attr_value, (list, set)):
                            edge_data[attr_name] = ";".join(str(v) for v in attr_value)
                        elif not isinstance(attr_value, (str, int, float, bool)):
                            edge_data[attr_name] = str(attr_value)

                graphml_file = output_dir / "unified_knowledge_graph.graphml"
                nx.write_graphml(G_graphml, graphml_file)
                saved_files.append(graphml_file)
                logger.info(f"ğŸ’¾ GraphML graph saved: {graphml_file}")

            except Exception as e:
                logger.warning(f"âš ï¸ GraphML ì €ì¥ ì‹¤íŒ¨: {e}")

        return saved_files

    def build_unified_graph(
        self, save_output: bool = True, output_dir: Optional[Path] = None
    ) -> nx.MultiDiGraph:
        """ì „ì²´ í†µí•© ê·¸ë˜í”„ êµ¬ì¶• íŒŒì´í”„ë¼ì¸"""
        logger.info("ğŸš€ Starting unified knowledge graph construction...")

        # 1. ê°œë³„ ê·¸ë˜í”„ë“¤ ë¡œë“œ
        individual_graphs = {}
        for graph_name in self.graph_files.keys():
            logger.info(f"ğŸ“‚ Loading {graph_name} graph...")
            graph = self.load_individual_graph(graph_name)
            if graph:
                individual_graphs[graph_name] = graph

        if not individual_graphs:
            raise ValueError("âŒ No graphs loaded successfully!")

        logger.info(f"âœ… Loaded {len(individual_graphs)} graphs successfully")

        # 2. ë…¸ë“œ í†µí•©
        logger.info("ğŸ”„ Integrating nodes...")
        nodes_added = 0
        nodes_merged = 0

        for graph_name, graph in individual_graphs.items():
            for node_id in tqdm(graph.nodes(), desc=f"Processing {graph_name} nodes"):
                node_attrs = graph.nodes[node_id]
                standardized_attrs = self.standardize_node_attributes(
                    node_id, node_attrs, graph_name
                )

                if self.unified_graph.has_node(node_id):
                    # ê¸°ì¡´ ë…¸ë“œì™€ ë³‘í•©
                    existing_attrs = self.unified_graph.nodes[node_id]
                    merged_attrs = self.merge_duplicate_nodes(
                        node_id, standardized_attrs, existing_attrs
                    )
                    self.unified_graph.nodes[node_id].update(merged_attrs)
                    nodes_merged += 1
                else:
                    # ìƒˆ ë…¸ë“œ ì¶”ê°€
                    self.unified_graph.add_node(node_id, **standardized_attrs)
                    nodes_added += 1

        logger.info(f"ğŸ“ Nodes: {nodes_added} added, {nodes_merged} merged")

        # 3. ì—£ì§€ í†µí•©
        logger.info("ğŸ”— Integrating edges...")
        edges_added = 0

        for graph_name, graph in individual_graphs.items():
            for source, target, edge_attrs in tqdm(
                graph.edges(data=True), desc=f"Processing {graph_name} edges"
            ):
                standardized_attrs = self.standardize_edge_attributes(
                    edge_attrs, graph_name
                )

                # MultiDiGraphì´ë¯€ë¡œ ì¤‘ë³µ ì—£ì§€ë„ ì¶”ê°€ ê°€ëŠ¥ (keyë¡œ êµ¬ë¶„)
                edge_key = (
                    f"{graph_name}_{standardized_attrs.get('edge_type', 'unknown')}"
                )
                self.unified_graph.add_edge(
                    source, target, key=edge_key, **standardized_attrs
                )
                edges_added += 1

        logger.info(f"ğŸ”— Added {edges_added} edges")

        # 4. ê·¸ë˜í”„ ê°„ ì¶”ê°€ ì—°ê²° ìƒì„±
        self.add_cross_graph_edges()

        # 5. í†µê³„ ê³„ì‚°
        stats = self.calculate_unified_statistics()

        # 6. ê²°ê³¼ ì €ì¥
        if save_output:
            if not output_dir:
                output_dir = self.graphs_dir / "unified"

            saved_files = self.save_unified_graph(output_dir)

            # í†µê³„ë„ ì €ì¥
            stats_file = output_dir / "unified_graph_statistics.json"
            with open(stats_file, "w", encoding="utf-8") as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“Š Statistics saved: {stats_file}")

        # 7. ìµœì¢… ìš”ì•½
        logger.info("ğŸ‰ Unified Knowledge Graph Construction Complete!")
        logger.info(f"ğŸ“Š Final Stats:")
        logger.info(f"   Total Nodes: {stats['basic_info']['total_nodes']:,}")
        logger.info(f"   Total Edges: {stats['basic_info']['total_edges']:,}")
        logger.info(f"   Graph Density: {stats['basic_info']['density']:.6f}")
        logger.info(f"   Node Types: {stats['node_types']}")
        logger.info(f"   Edge Types: {len(stats['edge_types'])} types")

        return self.unified_graph


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import GRAPHS_DIR

    # Unified Knowledge Graph Builder ì´ˆê¸°í™”
    builder = UnifiedKnowledgeGraphBuilder(GRAPHS_DIR)

    # í†µí•© ê·¸ë˜í”„ êµ¬ì¶•
    unified_graph = builder.build_unified_graph(save_output=True)

    print(f"\nâœ… Unified Knowledge Graph êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {GRAPHS_DIR / 'unified'}")
    print(f"ğŸš€ ë‹¤ìŒ ë‹¨ê³„: Query Analyzer êµ¬ì¶•")

    return unified_graph


if __name__ == "__main__":
    main()
