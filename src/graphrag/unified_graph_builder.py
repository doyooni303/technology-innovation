"""
í†µí•© ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Unified Knowledge Graph Builder Module

6ê°œì˜ ê°œë³„ ê·¸ë˜í”„ë¥¼ í•˜ë‚˜ì˜ í†µí•©ëœ ì§€ì‹ ê·¸ë˜í”„ë¡œ ê²°í•©
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from tqdm import tqdm
import logging
from typing import Dict, List, Set, Tuple, Optional, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UnifiedKnowledgeGraphBuilder:
    """6ê°œì˜ ê°œë³„ ê·¸ë˜í”„ë¥¼ í†µí•©ëœ ì§€ì‹ ê·¸ë˜í”„ë¡œ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, graphs_dir: Path):
        """
        Args:
            graphs_dir (Path): ê°œë³„ ê·¸ë˜í”„ íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
        """
        self.graphs_dir = Path(graphs_dir)
        self.unified_graph = nx.MultiDiGraph()  # ë‹¤ì¤‘ ì—£ì§€ + ë°©í–¥ì„± ì§€ì›

        # ê·¸ë˜í”„ë³„ íŒŒì¼ ê²½ë¡œ ì •ì˜
        self.graph_files = {
            "citation": "citation_network_graph.json",
            "keyword": "keyword_cooccurrence_graph.json",
            "semantic": "semantic_similarity_network_graph.json",
            "author_collab": "author_collaboration_graph.json",
            "author_paper": "author_paper_graph.json",
            "journal_paper": "journal_paper_graph.json",
        }

        # ë…¸ë“œ íƒ€ì…ë³„ í†µê³„
        self.node_stats = defaultdict(int)
        self.edge_stats = defaultdict(int)

        # í†µí•© ê³¼ì •ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ ì¶”ì 
        self.integration_issues = {
            "missing_files": [],
            "node_conflicts": [],
            "data_inconsistencies": [],
        }

    def load_individual_graph(self, graph_name: str) -> Optional[nx.Graph]:
        """ê°œë³„ ê·¸ë˜í”„ íŒŒì¼ ë¡œë“œ"""
        file_path = self.graphs_dir / self.graph_files[graph_name]

        if not file_path.exists():
            logger.warning(f"Graph file not found: {file_path}")
            self.integration_issues["missing_files"].append(str(file_path))
            return None

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                graph_data = json.load(f)

            # NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜
            if graph_name in ["citation", "semantic"]:
                G = nx.DiGraph()  # ë°©í–¥ ê·¸ë˜í”„
            else:
                G = nx.Graph()  # ë¬´ë°©í–¥ ê·¸ë˜í”„

            # ë…¸ë“œ ì¶”ê°€
            for node_data in graph_data.get("nodes", []):
                node_id = node_data["id"]
                attributes = {k: v for k, v in node_data.items() if k != "id"}
                G.add_node(node_id, **attributes)

            # ì—£ì§€ ì¶”ê°€
            for edge_data in graph_data.get("edges", []):
                source = edge_data["source"]
                target = edge_data["target"]
                attributes = {
                    k: v for k, v in edge_data.items() if k not in ["source", "target"]
                }
                G.add_edge(source, target, **attributes)

            logger.info(
                f"âœ… {graph_name} graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges"
            )
            return G

        except Exception as e:
            logger.error(f"âŒ Error loading {graph_name} graph: {e}")
            return None

    def standardize_node_attributes(
        self, node_id: str, node_data: Dict[str, Any], source_graph: str
    ) -> Dict[str, Any]:
        """ë…¸ë“œ ì†ì„± í‘œì¤€í™” - Abstract í¬í•¨ ë²„ì „"""

        standardized = node_data.copy()

        # ê³µí†µ ì†ì„± ì¶”ê°€
        standardized["source_graphs"] = [source_graph]
        standardized["integration_timestamp"] = pd.Timestamp.now().isoformat()

        # node_type í‘œì¤€í™” (í•„ìˆ˜)
        if "node_type" not in standardized:
            # ë…¸ë“œ IDë‚˜ source_graphë¡œë¶€í„° ì¶”ë¡ 
            if "paper" in node_id.lower() or "paper" in source_graph:
                standardized["node_type"] = "paper"
            elif "author" in node_id.lower() or "author" in source_graph:
                standardized["node_type"] = "author"
            elif "keyword" in node_id.lower() or "keyword" in source_graph:
                standardized["node_type"] = "keyword"
            elif "journal" in node_id.lower() or "journal" in source_graph:
                standardized["node_type"] = "journal"
            else:
                standardized["node_type"] = "unknown"

        # ë…¸ë“œ íƒ€ì…ë³„ íŠ¹ë³„ ì²˜ë¦¬
        node_type = standardized.get("node_type", "unknown")

        if node_type == "paper":
            # âœ… ë…¼ë¬¸ ë…¸ë“œì— Abstract ê´€ë ¨ ì²˜ë¦¬ ì¶”ê°€

            # 1. ê¸°ë³¸ í•„ë“œë“¤ ì •ë¦¬
            essential_fields = ["title", "authors", "year", "journal", "keywords"]
            for field in essential_fields:
                if field not in standardized:
                    standardized[field] = ""

            # 2. âœ… Abstract ì²˜ë¦¬ (í•µì‹¬ ì¶”ê°€)
            abstract_sources = [
                "abstract",  # ì§ì ‘ì ì¸ abstract í•„ë“œ
                "description",  # ì¼ë¶€ ì†ŒìŠ¤ì—ì„œ ì‚¬ìš©
                "summary",  # ìš”ì•½ í•„ë“œ
                "content",  # ì¼ë°˜ì ì¸ ë‚´ìš© í•„ë“œ
            ]

            abstract_content = ""
            for field in abstract_sources:
                if field in node_data and node_data[field]:
                    content = str(node_data[field]).strip()
                    if len(content) > len(abstract_content):
                        abstract_content = content

            standardized["abstract"] = abstract_content
            standardized["has_abstract"] = bool(abstract_content)

            # 3. âœ… Abstract í’ˆì§ˆ ë¶„ì„
            if abstract_content:
                # Abstract ê¸¸ì´ ë¶„ì„
                standardized["abstract_length"] = len(abstract_content)
                standardized["abstract_word_count"] = len(abstract_content.split())

                # Abstract í’ˆì§ˆ ì ìˆ˜ (ê¸¸ì´ ê¸°ë°˜)
                if len(abstract_content) > 100:
                    standardized["abstract_quality"] = "good"
                elif len(abstract_content) > 50:
                    standardized["abstract_quality"] = "fair"
                else:
                    standardized["abstract_quality"] = "poor"
            else:
                standardized["abstract_length"] = 0
                standardized["abstract_word_count"] = 0
                standardized["abstract_quality"] = "none"

            # 4. í‚¤ì›Œë“œ ì²˜ë¦¬ ê°œì„ 
            keywords = standardized.get("keywords", "")
            if isinstance(keywords, list):
                keywords = "; ".join(str(k) for k in keywords)
            elif not isinstance(keywords, str):
                keywords = str(keywords)

            # í‚¤ì›Œë“œ ì •ì œ
            if keywords:
                keyword_list = [kw.strip() for kw in keywords.split(";") if kw.strip()]
                standardized["keywords"] = "; ".join(keyword_list)
                standardized["keyword_count"] = len(keyword_list)
            else:
                standardized["keywords"] = ""
                standardized["keyword_count"] = 0

            # 5. ì €ì ì²˜ë¦¬ ê°œì„ 
            authors = standardized.get("authors", [])
            if isinstance(authors, str):
                authors = [a.strip() for a in authors.split(",") if a.strip()]
            elif not isinstance(authors, list):
                authors = [str(authors)]

            standardized["authors"] = authors
            standardized["author_count"] = len(authors)

            # ì €ì ê´€ë ¨ ë©”íƒ€ë°ì´í„°
            if len(authors) == 1:
                standardized["collaboration_type"] = "Single Author"
            elif len(authors) <= 3:
                standardized["collaboration_type"] = "Small Team"
            else:
                standardized["collaboration_type"] = "Large Team"

            # 6. ì—°ë„ ì •ê·œí™”
            year = standardized.get("year", "")
            if year:
                try:
                    year_int = int(str(year))
                    if 1900 <= year_int <= 2030:  # í•©ë¦¬ì ì¸ ë²”ìœ„
                        standardized["year"] = year_int
                    else:
                        standardized["year"] = None
                except:
                    standardized["year"] = None
            else:
                standardized["year"] = None

            # 7. âœ… ë…¼ë¬¸ ë¶„ë¥˜ ë° íŠ¹ì„± ë¶„ì„
            title = standardized.get("title", "").lower()
            abstract_lower = abstract_content.lower()

            # ML/AI ê´€ë ¨ í‚¤ì›Œë“œ íƒì§€
            ml_keywords = [
                "machine learning",
                "deep learning",
                "neural network",
                "artificial intelligence",
                "reinforcement learning",
                "supervised learning",
                "unsupervised learning",
                "classification",
                "regression",
                "clustering",
                "algorithm",
            ]

            battery_keywords = [
                "battery",
                "lithium",
                "soc",
                "state of charge",
                "electric vehicle",
                "energy storage",
                "charging",
                "power management",
                "thermal management",
            ]

            ml_score = sum(
                1 for kw in ml_keywords if kw in title or kw in abstract_lower
            )
            battery_score = sum(
                1 for kw in battery_keywords if kw in title or kw in abstract_lower
            )

            standardized["ml_relevance_score"] = ml_score
            standardized["battery_relevance_score"] = battery_score
            standardized["is_interdisciplinary"] = ml_score > 0 and battery_score > 0

            # ê²½í—˜ìˆëŠ” ì €ì ì—¬ë¶€ (íœ´ë¦¬ìŠ¤í‹±)
            experienced_indicators = ["professor", "dr.", "phd", "senior", "lead"]
            author_text = " ".join(authors).lower()
            standardized["has_experienced_authors"] = any(
                indicator in author_text for indicator in experienced_indicators
            )

        elif node_type == "author":
            # ì €ì ë…¸ë“œ ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
            essential_fields = ["name", "paper_count", "collaborator_count"]
            for field in essential_fields:
                if field not in standardized:
                    if field == "name":
                        standardized[field] = node_id
                    else:
                        standardized[field] = 0

        elif node_type == "keyword":
            # í‚¤ì›Œë“œ ë…¸ë“œ ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
            if "name" not in standardized:
                standardized["name"] = node_id
            if "frequency" not in standardized:
                standardized["frequency"] = 1

        elif node_type == "journal":
            # ì €ë„ ë…¸ë“œ ì²˜ë¦¬ (ê¸°ì¡´ ìœ ì§€)
            essential_fields = ["name", "paper_count"]
            for field in essential_fields:
                if field not in standardized:
                    if field == "name":
                        standardized[field] = node_id
                    else:
                        standardized[field] = 0

        # ID ì •ê·œí™”
        standardized["id"] = node_id

        return standardized

    def merge_duplicate_nodes(
        self,
        node_id: str,
        new_attributes: Dict[str, Any],
        existing_attributes: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ì¤‘ë³µ ë…¸ë“œ ë³‘í•© - Abstract ê³ ë ¤ ë²„ì „"""

        merged = existing_attributes.copy()

        # source_graphs ë³‘í•©
        existing_sources = set(merged.get("source_graphs", []))
        new_sources = set(new_attributes.get("source_graphs", []))
        merged["source_graphs"] = list(existing_sources | new_sources)

        # ë…¸ë“œ íƒ€ì…ë³„ íŠ¹ë³„ ì²˜ë¦¬
        node_type = merged.get("node_type", "unknown")

        if node_type == "paper":
            # âœ… ë…¼ë¬¸ ì •ë³´ ë³‘í•© - Abstract ìš°ì„  ì²˜ë¦¬

            # Abstract ë³‘í•© (ë” ê¸´ ê²ƒ ì„ íƒ)
            existing_abstract = merged.get("abstract", "")
            new_abstract = new_attributes.get("abstract", "")

            if len(new_abstract) > len(existing_abstract):
                merged["abstract"] = new_abstract
                merged["has_abstract"] = bool(new_abstract)
                merged["abstract_length"] = len(new_abstract)
                merged["abstract_word_count"] = len(new_abstract.split())

                # Abstract í’ˆì§ˆ ì¬ê³„ì‚°
                if len(new_abstract) > 100:
                    merged["abstract_quality"] = "good"
                elif len(new_abstract) > 50:
                    merged["abstract_quality"] = "fair"
                else:
                    merged["abstract_quality"] = "poor"

            # ë‹¤ë¥¸ í…ìŠ¤íŠ¸ í•„ë“œë“¤ë„ ë” ì™„ì „í•œ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
            text_fields = ["title", "keywords"]
            for field in text_fields:
                if field in new_attributes and field in merged:
                    if len(str(new_attributes[field])) > len(str(merged[field])):
                        merged[field] = new_attributes[field]
                elif field in new_attributes:
                    merged[field] = new_attributes[field]

            # ì €ì ì •ë³´ ë³‘í•© (ë” ë§ì€ ì €ì ì •ë³´ ì„ íƒ)
            if "authors" in new_attributes and "authors" in merged:
                existing_authors = (
                    merged["authors"] if isinstance(merged["authors"], list) else []
                )
                new_authors = (
                    new_attributes["authors"]
                    if isinstance(new_attributes["authors"], list)
                    else []
                )

                if len(new_authors) > len(existing_authors):
                    merged["authors"] = new_authors
                    merged["author_count"] = len(new_authors)

            # ìˆ˜ì¹˜í˜• í•„ë“œë“¤ì€ ë” ë†’ì€ ê°’ ì„ íƒ
            numeric_fields = [
                "ml_relevance_score",
                "battery_relevance_score",
                "keyword_count",
            ]
            for field in numeric_fields:
                if field in new_attributes and field in merged:
                    merged[field] = max(
                        merged.get(field, 0), new_attributes.get(field, 0)
                    )
                elif field in new_attributes:
                    merged[field] = new_attributes[field]

            # Boolean í•„ë“œë“¤ì€ OR ì—°ì‚°
            boolean_fields = ["is_interdisciplinary", "has_experienced_authors"]
            for field in boolean_fields:
                if field in new_attributes and field in merged:
                    merged[field] = merged.get(field, False) or new_attributes.get(
                        field, False
                    )
                elif field in new_attributes:
                    merged[field] = new_attributes[field]

        elif node_type == "author":
            # ì €ì í†µê³„ ì •ë³´ ë³‘í•© (ìµœëŒ€ê°’ ì„ íƒ)
            numeric_fields = ["paper_count", "collaborator_count", "first_author_count"]
            for field in numeric_fields:
                if field in new_attributes and field in merged:
                    merged[field] = max(merged[field], new_attributes[field])
                elif field in new_attributes:
                    merged[field] = new_attributes[field]

        elif node_type == "keyword":
            # í‚¤ì›Œë“œ ë¹ˆë„ í•©ì‚°
            if "frequency" in new_attributes and "frequency" in merged:
                merged["frequency"] = merged["frequency"] + new_attributes["frequency"]

        # ê¸°íƒ€ ìƒˆë¡œìš´ ì†ì„± ì¶”ê°€
        for key, value in new_attributes.items():
            if key not in merged and key not in ["source_graphs"]:
                merged[key] = value

        return merged

    def standardize_edge_attributes(
        self, edge_data: Dict[str, Any], source_graph: str
    ) -> Dict[str, Any]:
        """ì—£ì§€ ì†ì„± í‘œì¤€í™”"""
        standardized = edge_data.copy()

        # ê³µí†µ ì†ì„± ì¶”ê°€
        standardized["source_graph"] = source_graph
        standardized["integration_timestamp"] = pd.Timestamp.now().isoformat()

        # edge_type í‘œì¤€í™”
        if "edge_type" not in standardized:
            # ì†ŒìŠ¤ ê·¸ë˜í”„ì— ë”°ë¥¸ ê¸°ë³¸ edge_type ì„¤ì •
            edge_type_mapping = {
                "citation": "cites",
                "keyword": "co_occurs_with",
                "semantic": "semantically_similar_to",
                "author_collab": "collaborates_with",
                "author_paper": "authored_by",
                "journal_paper": "published_in",
            }
            standardized["edge_type"] = edge_type_mapping.get(
                source_graph, "related_to"
            )

        # ê°€ì¤‘ì¹˜ í‘œì¤€í™” (0-1 ë²”ìœ„ë¡œ)
        if "weight" in standardized:
            weight = standardized["weight"]
            if isinstance(weight, (int, float)) and weight > 1:
                # 1ë³´ë‹¤ í° ê°€ì¤‘ì¹˜ëŠ” ì •ê·œí™” (ë¡œê·¸ ìŠ¤ì¼€ì¼ ê³ ë ¤)
                standardized["normalized_weight"] = min(1.0, weight / 100.0)
            else:
                standardized["normalized_weight"] = float(weight)
        else:
            standardized["normalized_weight"] = 1.0

        return standardized

    def add_cross_graph_edges(self):
        """ê·¸ë˜í”„ ê°„ ì¶”ê°€ ì—°ê²° ì—£ì§€ ìƒì„±"""
        logger.info("ğŸ”— Creating cross-graph connections...")

        # ë…¼ë¬¸-í‚¤ì›Œë“œ ì—°ê²° (keywords ì†ì„± ê¸°ë°˜)
        self._connect_papers_to_keywords()

        # ì €ì-í‚¤ì›Œë“œ ì—°ê²° (ì €ìì˜ ë…¼ë¬¸ í‚¤ì›Œë“œ ê¸°ë°˜)
        self._connect_authors_to_keywords()

        # ì €ë„-í‚¤ì›Œë“œ ì—°ê²° (ì €ë„ ë…¼ë¬¸ë“¤ì˜ í‚¤ì›Œë“œ ê¸°ë°˜)
        self._connect_journals_to_keywords()

        # ì‹œê°„ì  ì—°ê²° (ê°™ì€ ì—°ë„ ë…¼ë¬¸ë“¤)
        self._create_temporal_connections()

    def _connect_papers_to_keywords(self):
        """ë…¼ë¬¸ê³¼ í‚¤ì›Œë“œ ê°„ ì—°ê²° ìƒì„±"""
        paper_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "paper"
        ]
        keyword_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "keyword"
        ]

        connections_added = 0

        for paper_id in paper_nodes:
            paper_data = self.unified_graph.nodes[paper_id]
            paper_keywords = paper_data.get("keywords", [])

            if isinstance(paper_keywords, str):
                paper_keywords = [kw.strip() for kw in paper_keywords.split(";")]

            for keyword in paper_keywords:
                keyword_clean = keyword.lower().strip()

                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë…¸ë“œ ì°¾ê¸°
                matching_keyword = None
                for kw_node in keyword_nodes:
                    if kw_node.lower() == keyword_clean:
                        matching_keyword = kw_node
                        break

                if matching_keyword and not self.unified_graph.has_edge(
                    paper_id, matching_keyword
                ):
                    self.unified_graph.add_edge(
                        paper_id,
                        matching_keyword,
                        edge_type="has_keyword",
                        source_graph="cross_connection",
                        weight=1.0,
                        normalized_weight=1.0,
                    )
                    connections_added += 1

        logger.info(f"ğŸ“ Added {connections_added} paper-keyword connections")

    def _connect_authors_to_keywords(self):
        """ì €ìì™€ í‚¤ì›Œë“œ ê°„ ì—°ê²° ìƒì„± (ì €ì ë…¼ë¬¸ì˜ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        author_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "author"
        ]

        connections_added = 0

        for author_id in author_nodes:
            # ì €ìì˜ ëª¨ë“  ë…¼ë¬¸ ì°¾ê¸°
            author_papers = []
            for edge in self.unified_graph.edges(data=True):
                if (
                    edge[2].get("edge_type") == "authored_by" and edge[1] == author_id
                ):  # targetì´ author
                    author_papers.append(edge[0])  # sourceëŠ” paper

            # ì €ì ë…¼ë¬¸ë“¤ì˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
            author_keywords = Counter()
            for paper_id in author_papers:
                paper_data = self.unified_graph.nodes.get(paper_id, {})
                paper_keywords = paper_data.get("keywords", [])

                if isinstance(paper_keywords, str):
                    paper_keywords = [kw.strip() for kw in paper_keywords.split(";")]

                for keyword in paper_keywords:
                    if keyword.strip():
                        author_keywords[keyword.lower().strip()] += 1

            # ë¹ˆë„ ë†’ì€ í‚¤ì›Œë“œì™€ ì—°ê²° (ë¹ˆë„ 2 ì´ìƒ)
            for keyword, freq in author_keywords.items():
                if freq >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ì‚¬ìš©í•œ í‚¤ì›Œë“œ
                    # í•´ë‹¹ í‚¤ì›Œë“œ ë…¸ë“œ ì°¾ê¸°
                    keyword_nodes = [
                        n
                        for n in self.unified_graph.nodes()
                        if (
                            self.unified_graph.nodes[n].get("node_type") == "keyword"
                            and n.lower() == keyword
                        )
                    ]

                    for kw_node in keyword_nodes:
                        if not self.unified_graph.has_edge(author_id, kw_node):
                            self.unified_graph.add_edge(
                                author_id,
                                kw_node,
                                edge_type="specializes_in",
                                source_graph="cross_connection",
                                frequency=freq,
                                weight=min(1.0, freq / 10.0),  # ë¹ˆë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜
                                normalized_weight=min(1.0, freq / 10.0),
                            )
                            connections_added += 1

        logger.info(f"ğŸ‘¥ Added {connections_added} author-keyword connections")

    def _connect_journals_to_keywords(self):
        """ì €ë„ê³¼ í‚¤ì›Œë“œ ê°„ ì—°ê²° ìƒì„±"""
        journal_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "journal"
        ]

        connections_added = 0

        for journal_id in journal_nodes:
            # ì €ë„ì˜ ëª¨ë“  ë…¼ë¬¸ ì°¾ê¸°
            journal_papers = []
            for edge in self.unified_graph.edges(data=True):
                if (
                    edge[2].get("edge_type") == "published_in" and edge[1] == journal_id
                ):  # targetì´ journal
                    journal_papers.append(edge[0])  # sourceëŠ” paper

            # ì €ë„ ë…¼ë¬¸ë“¤ì˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
            journal_keywords = Counter()
            for paper_id in journal_papers:
                paper_data = self.unified_graph.nodes.get(paper_id, {})
                paper_keywords = paper_data.get("keywords", [])

                if isinstance(paper_keywords, str):
                    paper_keywords = [kw.strip() for kw in paper_keywords.split(";")]

                for keyword in paper_keywords:
                    if keyword.strip():
                        journal_keywords[keyword.lower().strip()] += 1

            # ìƒìœ„ í‚¤ì›Œë“œë“¤ê³¼ ì—°ê²° (ìƒìœ„ 20%ë§Œ)
            if journal_keywords:
                top_keywords = journal_keywords.most_common(
                    max(5, len(journal_keywords) // 5)
                )

                for keyword, freq in top_keywords:
                    keyword_nodes = [
                        n
                        for n in self.unified_graph.nodes()
                        if (
                            self.unified_graph.nodes[n].get("node_type") == "keyword"
                            and n.lower() == keyword
                        )
                    ]

                    for kw_node in keyword_nodes:
                        if not self.unified_graph.has_edge(journal_id, kw_node):
                            self.unified_graph.add_edge(
                                journal_id,
                                kw_node,
                                edge_type="focuses_on",
                                source_graph="cross_connection",
                                frequency=freq,
                                weight=min(1.0, freq / max(journal_keywords.values())),
                                normalized_weight=min(
                                    1.0, freq / max(journal_keywords.values())
                                ),
                            )
                            connections_added += 1

        logger.info(f"ğŸ“° Added {connections_added} journal-keyword connections")

    def _create_temporal_connections(self):
        """ì‹œê°„ì  ê·¼ì ‘ì„± ê¸°ë°˜ ì—°ê²° ìƒì„±"""
        paper_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "paper"
        ]

        # ì—°ë„ë³„ ë…¼ë¬¸ ê·¸ë£¹í™”
        papers_by_year = defaultdict(list)
        for paper_id in paper_nodes:
            year = self.unified_graph.nodes[paper_id].get("year", "")
            if year and str(year).isdigit():
                papers_by_year[int(year)].append(paper_id)

        connections_added = 0

        # ê°™ì€ ì—°ë„ ë…¼ë¬¸ë“¤ ê°„ ì•½í•œ ì—°ê²° (ìƒ˜í”Œë§)
        for year, papers in papers_by_year.items():
            if len(papers) > 1:
                # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§ (ìµœëŒ€ 50ê°œ ë…¼ë¬¸ë§Œ)
                if len(papers) > 50:
                    import random

                    papers = random.sample(papers, 50)

                # ëª¨ë“  ìŒì´ ì•„ë‹Œ ì¼ë¶€ë§Œ ì—°ê²° (computational cost ê³ ë ¤)
                for i in range(min(10, len(papers))):  # ê° ë…¼ë¬¸ë‹¹ ìµœëŒ€ 10ê°œë§Œ ì—°ê²°
                    for j in range(i + 1, min(i + 11, len(papers))):
                        paper1, paper2 = papers[i], papers[j]

                        if not self.unified_graph.has_edge(paper1, paper2):
                            self.unified_graph.add_edge(
                                paper1,
                                paper2,
                                edge_type="temporal_proximity",
                                source_graph="cross_connection",
                                year=year,
                                weight=0.1,  # ì•½í•œ ì—°ê²°
                                normalized_weight=0.1,
                            )
                            connections_added += 1

        logger.info(f"â° Added {connections_added} temporal proximity connections")

    def calculate_unified_statistics(self):
        """í†µí•© ê·¸ë˜í”„ í†µê³„ ê³„ì‚°"""
        logger.info("ğŸ“Š Calculating unified graph statistics...")

        # ë…¸ë“œ íƒ€ì…ë³„ í†µê³„
        node_types = defaultdict(int)
        for node in self.unified_graph.nodes():
            node_type = self.unified_graph.nodes[node].get("node_type", "unknown")
            node_types[node_type] += 1
        # âœ… Abstract ê´€ë ¨ í†µê³„ ì¶”ê°€
        abstract_stats = {
            "papers_with_abstract": 0,
            "papers_without_abstract": 0,
            "total_abstract_length": 0,
            "average_abstract_length": 0,
            "abstract_quality_distribution": {
                "good": 0,
                "fair": 0,
                "poor": 0,
                "none": 0,
            },
        }

        paper_nodes = [
            n
            for n in self.unified_graph.nodes()
            if self.unified_graph.nodes[n].get("node_type") == "paper"
        ]

        for paper_id in paper_nodes:
            paper_data = self.unified_graph.nodes[paper_id]
            has_abstract = paper_data.get("has_abstract", False)
            abstract_length = paper_data.get("abstract_length", 0)
            abstract_quality = paper_data.get("abstract_quality", "none")

            if has_abstract:
                abstract_stats["papers_with_abstract"] += 1
                abstract_stats["total_abstract_length"] += abstract_length
            else:
                abstract_stats["papers_without_abstract"] += 1

            abstract_stats["abstract_quality_distribution"][abstract_quality] += 1

        if abstract_stats["papers_with_abstract"] > 0:
            abstract_stats["average_abstract_length"] = (
                abstract_stats["total_abstract_length"]
                / abstract_stats["papers_with_abstract"]
            )

        # ì—£ì§€ íƒ€ì…ë³„ í†µê³„
        edge_types = defaultdict(int)
        for edge in self.unified_graph.edges(data=True):
            edge_type = edge[2].get("edge_type", "unknown")
            edge_types[edge_type] += 1

        # ì†ŒìŠ¤ ê·¸ë˜í”„ë³„ ê¸°ì—¬ë„
        source_contributions = defaultdict(lambda: {"nodes": 0, "edges": 0})

        for node in self.unified_graph.nodes():
            sources = self.unified_graph.nodes[node].get("source_graphs", ["unknown"])
            for source in sources:
                source_contributions[source]["nodes"] += 1

        for edge in self.unified_graph.edges(data=True):
            source = edge[2].get("source_graph", "unknown")
            source_contributions[source]["edges"] += 1

        # ì—°ê²°ì„± ë¶„ì„
        if nx.is_connected(self.unified_graph.to_undirected()):
            connectivity = "fully_connected"
            largest_component_size = self.unified_graph.number_of_nodes()
        else:
            components = list(
                nx.connected_components(self.unified_graph.to_undirected())
            )
            connectivity = "disconnected"
            largest_component_size = len(max(components, key=len))

        stats = {
            "basic_info": {
                "total_nodes": self.unified_graph.number_of_nodes(),
                "total_edges": self.unified_graph.number_of_edges(),
                "density": nx.density(self.unified_graph),
                "connectivity": connectivity,
                "largest_component_size": largest_component_size,
            },
            "node_types": dict(node_types),
            "edge_types": dict(edge_types),
            "abstract_statistics": abstract_stats,  # âœ… ìƒˆë¡œ ì¶”ê°€
            "source_contributions": dict(source_contributions),
            "integration_issues": self.integration_issues,
        }
        # Abstract í†µê³„ ë¡œê¹…
        logger.info(f"ğŸ“„ Abstract Statistics:")
        logger.info(
            f"   Papers with abstract: {abstract_stats['papers_with_abstract']}"
        )
        logger.info(
            f"   Papers without abstract: {abstract_stats['papers_without_abstract']}"
        )
        logger.info(
            f"   Average abstract length: {abstract_stats['average_abstract_length']:.1f} chars"
        )
        return stats

    def save_unified_graph(
        self, output_dir: Path, save_formats: List[str] = ["json", "graphml"]
    ):
        """í†µí•© ê·¸ë˜í”„ ì €ì¥"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        saved_files = []

        # JSON í˜•íƒœë¡œ ì €ì¥ (GraphRAGì—ì„œ ì‚¬ìš©)
        if "json" in save_formats:
            graph_data = {"nodes": [], "edges": []}

            # ë…¸ë“œ ì •ë³´
            for node in self.unified_graph.nodes():
                node_data = self.unified_graph.nodes[node].copy()
                node_data["id"] = node

                # ë¦¬ìŠ¤íŠ¸/ë³µì¡í•œ íƒ€ì… ì²˜ë¦¬
                for key, value in node_data.items():
                    if isinstance(value, (list, set)):
                        node_data[key] = list(value)
                    elif not isinstance(value, (str, int, float, bool, type(None))):
                        node_data[key] = str(value)

                graph_data["nodes"].append(node_data)

            # ì—£ì§€ ì •ë³´
            for edge in self.unified_graph.edges(data=True):
                edge_data = edge[2].copy()
                edge_data["source"] = edge[0]
                edge_data["target"] = edge[1]

                # ë¦¬ìŠ¤íŠ¸/ë³µì¡í•œ íƒ€ì… ì²˜ë¦¬
                for key, value in edge_data.items():
                    if isinstance(value, (list, set)):
                        edge_data[key] = list(value)
                    elif not isinstance(value, (str, int, float, bool, type(None))):
                        edge_data[key] = str(value)

                graph_data["edges"].append(edge_data)

            json_file = output_dir / "unified_knowledge_graph.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(graph_data, f, ensure_ascii=False, indent=2)

            saved_files.append(json_file)
            logger.info(f"ğŸ’¾ JSON graph saved: {json_file}")

        # GraphML í˜•íƒœë¡œ ì €ì¥ (Gephi, Cytoscape ë“±ì—ì„œ ì‚¬ìš©)
        if "graphml" in save_formats:
            try:
                # GraphML í˜¸í™˜ì„ ìœ„í•´ ë³µì¡í•œ ì†ì„± ë¬¸ìì—´í™”
                G_graphml = self.unified_graph.copy()

                for node in G_graphml.nodes():
                    for attr_name, attr_value in G_graphml.nodes[node].items():
                        if isinstance(attr_value, (list, set)):
                            G_graphml.nodes[node][attr_name] = ";".join(
                                str(v) for v in attr_value
                            )
                        elif not isinstance(attr_value, (str, int, float, bool)):
                            G_graphml.nodes[node][attr_name] = str(attr_value)

                for edge in G_graphml.edges(data=True):
                    edge_data = edge[2]
                    for attr_name, attr_value in edge_data.items():
                        if isinstance(attr_value, (list, set)):
                            edge_data[attr_name] = ";".join(str(v) for v in attr_value)
                        elif not isinstance(attr_value, (str, int, float, bool)):
                            edge_data[attr_name] = str(attr_value)

                graphml_file = output_dir / "unified_knowledge_graph.graphml"
                nx.write_graphml(G_graphml, graphml_file)
                saved_files.append(graphml_file)
                logger.info(f"ğŸ’¾ GraphML graph saved: {graphml_file}")

            except Exception as e:
                logger.warning(f"âš ï¸ GraphML ì €ì¥ ì‹¤íŒ¨: {e}")

        return saved_files

    def build_unified_graph(
        self, save_output: bool = True, output_dir: Optional[Path] = None
    ) -> nx.MultiDiGraph:
        """ì „ì²´ í†µí•© ê·¸ë˜í”„ êµ¬ì¶• íŒŒì´í”„ë¼ì¸"""
        logger.info("ğŸš€ Starting unified knowledge graph construction...")

        # 1. ê°œë³„ ê·¸ë˜í”„ë“¤ ë¡œë“œ
        individual_graphs = {}
        for graph_name in self.graph_files.keys():
            logger.info(f"ğŸ“‚ Loading {graph_name} graph...")
            graph = self.load_individual_graph(graph_name)
            if graph:
                individual_graphs[graph_name] = graph

        if not individual_graphs:
            raise ValueError("âŒ No graphs loaded successfully!")

        logger.info(f"âœ… Loaded {len(individual_graphs)} graphs successfully")

        # 2. ë…¸ë“œ í†µí•©
        logger.info("ğŸ”„ Integrating nodes...")
        nodes_added = 0
        nodes_merged = 0

        for graph_name, graph in individual_graphs.items():
            for node_id in tqdm(graph.nodes(), desc=f"Processing {graph_name} nodes"):
                node_attrs = graph.nodes[node_id]
                standardized_attrs = self.standardize_node_attributes(
                    node_id, node_attrs, graph_name
                )

                if self.unified_graph.has_node(node_id):
                    # ê¸°ì¡´ ë…¸ë“œì™€ ë³‘í•©
                    existing_attrs = self.unified_graph.nodes[node_id]
                    merged_attrs = self.merge_duplicate_nodes(
                        node_id, standardized_attrs, existing_attrs
                    )
                    self.unified_graph.nodes[node_id].update(merged_attrs)
                    nodes_merged += 1
                else:
                    # ìƒˆ ë…¸ë“œ ì¶”ê°€
                    self.unified_graph.add_node(node_id, **standardized_attrs)
                    nodes_added += 1

        logger.info(f"ğŸ“ Nodes: {nodes_added} added, {nodes_merged} merged")

        # 3. ì—£ì§€ í†µí•©
        logger.info("ğŸ”— Integrating edges...")
        edges_added = 0

        for graph_name, graph in individual_graphs.items():
            for source, target, edge_attrs in tqdm(
                graph.edges(data=True), desc=f"Processing {graph_name} edges"
            ):
                standardized_attrs = self.standardize_edge_attributes(
                    edge_attrs, graph_name
                )

                # MultiDiGraphì´ë¯€ë¡œ ì¤‘ë³µ ì—£ì§€ë„ ì¶”ê°€ ê°€ëŠ¥ (keyë¡œ êµ¬ë¶„)
                edge_key = (
                    f"{graph_name}_{standardized_attrs.get('edge_type', 'unknown')}"
                )
                self.unified_graph.add_edge(
                    source, target, key=edge_key, **standardized_attrs
                )
                edges_added += 1

        logger.info(f"ğŸ”— Added {edges_added} edges")

        # 4. ê·¸ë˜í”„ ê°„ ì¶”ê°€ ì—°ê²° ìƒì„±
        self.add_cross_graph_edges()

        # 5. í†µê³„ ê³„ì‚°
        stats = self.calculate_unified_statistics()

        # 6. ê²°ê³¼ ì €ì¥
        if save_output:
            if not output_dir:
                output_dir = self.graphs_dir / "unified"

            saved_files = self.save_unified_graph(output_dir)

            # í†µê³„ë„ ì €ì¥
            stats_file = output_dir / "unified_graph_statistics.json"
            with open(stats_file, "w", encoding="utf-8") as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“Š Statistics saved: {stats_file}")

        # 7. ìµœì¢… ìš”ì•½
        logger.info("ğŸ‰ Unified Knowledge Graph Construction Complete!")
        logger.info(f"ğŸ“Š Final Stats:")
        logger.info(f"   Total Nodes: {stats['basic_info']['total_nodes']:,}")
        logger.info(f"   Total Edges: {stats['basic_info']['total_edges']:,}")
        logger.info(f"   Graph Density: {stats['basic_info']['density']:.6f}")
        logger.info(f"   Node Types: {stats['node_types']}")
        logger.info(f"   Edge Types: {len(stats['edge_types'])} types")

        return self.unified_graph


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import GRAPHS_DIR

    # Unified Knowledge Graph Builder ì´ˆê¸°í™”
    builder = UnifiedKnowledgeGraphBuilder(GRAPHS_DIR)

    # í†µí•© ê·¸ë˜í”„ êµ¬ì¶•
    unified_graph = builder.build_unified_graph(save_output=True)

    print(f"\nâœ… Unified Knowledge Graph êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {GRAPHS_DIR / 'unified'}")
    print(f"ğŸš€ ë‹¤ìŒ ë‹¨ê³„: Query Analyzer êµ¬ì¶•")

    return unified_graph


if __name__ == "__main__":
    main()
