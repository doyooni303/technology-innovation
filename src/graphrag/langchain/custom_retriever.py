"""
GraphRAG LangChain ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„ - ìˆ˜ì •ëœ ë²„ì „
Custom GraphRAG Retriever for LangChain Integration

LangChain BaseRetrieverë¥¼ ìƒì†ë°›ì•„ GraphRAG ì‹œìŠ¤í…œê³¼ ì™„ì „ í†µí•©
- LangChain v0.1+ í˜¸í™˜ì„± ìˆ˜ì •
- í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„ ì™„ë£Œ
- embeddings í´ë” êµ¬ì¡° ì§€ì›
"""

import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Callable
from pydantic import Field
import warnings

# LangChain imports - ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
try:
    from langchain_core.retrievers import BaseRetriever
    from langchain_core.documents import Document
    from langchain_core.callbacks import (
        CallbackManagerForRetrieverRun,
        AsyncCallbackManagerForRetrieverRun,
    )

    # LangChain ë²„ì „ í™•ì¸ ì‹œë„
    try:
        # v0.2+ ì—ì„œëŠ” _get_relevant_documentsê°€ ì—†ì–´ì§
        from langchain_core.retrievers import BaseRetriever as _TestRetriever

        _langchain_new_version = not hasattr(_TestRetriever, "_get_relevant_documents")
    except:
        _langchain_new_version = False

    _langchain_available = True
except ImportError:
    _langchain_available = False
    warnings.warn(
        "LangChain not available. Install with: pip install langchain langchain-core"
    )

    # Placeholder classes for type hints
    class BaseRetriever:
        def __init__(self, *args, **kwargs):
            raise ImportError("LangChain is required but not installed")

    class Document:
        def __init__(self, *args, **kwargs):
            pass

    class CallbackManagerForRetrieverRun:
        pass

    class AsyncCallbackManagerForRetrieverRun:
        pass

    _langchain_new_version = False


# GraphRAG imports
try:
    from ..query_analyzer import QueryAnalyzer, QueryAnalysisResult
    from ..embeddings.subgraph_extractor import SubgraphExtractor, SubgraphResult
    from ..embeddings.context_serializer import ContextSerializer, SerializedContext
    from ..embeddings.vector_store_manager import VectorStoreManager
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class GraphRAGRetriever(BaseRetriever):
    """GraphRAG ì»¤ìŠ¤í…€ LangChain ë¦¬íŠ¸ë¦¬ë²„ - ìˆ˜ì •ëœ ë²„ì „"""

    # Pydantic í•„ë“œ ì •ì˜ (LangChain v0.1+ í˜¸í™˜)
    unified_graph_path: str = Field(description="í†µí•© ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ")
    vector_store_path: str = Field(description="ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ")
    embedding_model: str = Field(default="auto", description="ì„ë² ë”© ëª¨ë¸ëª…")

    # êµ¬ì„±ìš”ì†Œ ì„¤ì •
    max_docs: int = Field(default=10, description="ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜")
    min_relevance_score: float = Field(default=0.3, description="ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜")
    enable_query_analysis: bool = Field(default=True, description="ì¿¼ë¦¬ ë¶„ì„ í™œì„±í™”")

    # ë‚´ë¶€ ì»´í¬ë„ŒíŠ¸ë“¤ (ì´ˆê¸°í™” í›„ ì„¤ì •)
    query_analyzer: Optional[QueryAnalyzer] = Field(default=None, exclude=True)
    subgraph_extractor: Optional[SubgraphExtractor] = Field(default=None, exclude=True)
    context_serializer: Optional[ContextSerializer] = Field(default=None, exclude=True)

    # ìºì‹œ ë° ì„±ëŠ¥ ì„¤ì •
    enable_caching: bool = Field(default=True, description="ê²°ê³¼ ìºì‹± í™œì„±í™”")
    cache_ttl_seconds: int = Field(default=3600, description="ìºì‹œ ìœ ì§€ ì‹œê°„(ì´ˆ)")

    # ë‚´ë¶€ ìºì‹œ ì €ì¥ì†Œ
    query_cache: Dict[str, List[Document]] = Field(default_factory=dict, exclude=True)
    cache_timestamps: Dict[str, float] = Field(default_factory=dict, exclude=True)

    class Config:
        """Pydantic ì„¤ì •"""

        arbitrary_types_allowed = True

    def __init__(self, **kwargs):
        """GraphRAGRetriever ì´ˆê¸°í™”"""
        super().__init__(**kwargs)

        if not _langchain_available:
            raise ImportError(
                "LangChain is required for GraphRAGRetriever.\n"
                "Install with: pip install langchain langchain-core"
            )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” í”Œë˜ê·¸
        self._initialized = False

        logger.info("âœ… GraphRAGRetriever created")
        logger.info(f"   ğŸ“ Graph: {self.unified_graph_path}")
        logger.info(f"   ğŸ—„ï¸ Vector Store: {self.vector_store_path}")
        logger.info(f"   ğŸ¤– Model: {self.embedding_model}")

    def _lazy_init(self) -> None:
        """ì§€ì—° ì´ˆê¸°í™” - ì²« ë²ˆì§¸ ì¿¼ë¦¬ ì‹œ ì‹¤í–‰"""
        if self._initialized:
            return

        logger.info("ğŸ”§ Initializing GraphRAG components...")

        try:
            # 1. Query Analyzer ì´ˆê¸°í™”
            if self.enable_query_analysis:
                self.query_analyzer = QueryAnalyzer()
                logger.info("âœ… QueryAnalyzer initialized")

            # 2. SubgraphExtractor ì´ˆê¸°í™” - embeddings í´ë” êµ¬ì¡° ì§€ì›
            self.subgraph_extractor = SubgraphExtractor(
                unified_graph_path=self.unified_graph_path,
                vector_store_path=self.vector_store_path,
                embedding_model=self.embedding_model,
            )
            logger.info("âœ… SubgraphExtractor initialized")

            # 3. ContextSerializer ì´ˆê¸°í™”
            self.context_serializer = ContextSerializer()
            logger.info("âœ… ContextSerializer initialized")

            self._initialized = True
            logger.info("ğŸš€ GraphRAG components ready!")

        except Exception as e:
            logger.error(f"âŒ Component initialization failed: {e}")
            raise

    # =======================================================================
    # LangChain í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œë“¤ - ë²„ì „ë³„ ëŒ€ì‘
    # =======================================================================

    def get_relevant_documents(
        self, query: str, *, callbacks: Optional[Any] = None
    ) -> List[Document]:
        """LangChain v0.2+ í˜¸í™˜ìš© ë©”ì„œë“œ"""
        # CallbackManager ë³€í™˜
        run_manager = None
        if callbacks:
            try:
                # ì½œë°±ì„ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜
                run_manager = callbacks
            except:
                run_manager = None

        return self._get_docs_impl(query, run_manager)

    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: Optional[CallbackManagerForRetrieverRun] = None,
    ) -> List[Document]:
        """LangChain v0.1 í˜¸í™˜ìš© ë©”ì„œë“œ"""
        return self._get_docs_impl(query, run_manager)

    async def aget_relevant_documents(
        self, query: str, *, callbacks: Optional[Any] = None
    ) -> List[Document]:
        """ë¹„ë™ê¸° ë²„ì „ - v0.2+ í˜¸í™˜"""
        run_manager = None
        if callbacks:
            try:
                run_manager = callbacks
            except:
                run_manager = None

        return await self._aget_docs_impl(query, run_manager)

    async def _aget_relevant_documents(
        self,
        query: str,
        *,
        run_manager: Optional[AsyncCallbackManagerForRetrieverRun] = None,
    ) -> List[Document]:
        """ë¹„ë™ê¸° ë²„ì „ - v0.1 í˜¸í™˜"""
        return await self._aget_docs_impl(query, run_manager)

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ë„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    def _get_docs(
        self,
        query: str,
        *,
        run_manager: Optional[CallbackManagerForRetrieverRun] = None,
    ) -> List[Document]:
        """ê¸°ì¡´ ë™ê¸° ë©”ì„œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return self._get_docs_impl(query, run_manager)

    async def _aget_docs(
        self,
        query: str,
        *,
        run_manager: Optional[AsyncCallbackManagerForRetrieverRun] = None,
    ) -> List[Document]:
        """ê¸°ì¡´ ë¹„ë™ê¸° ë©”ì„œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self._aget_docs_impl(query, run_manager)

    # =======================================================================
    # ì‹¤ì œ êµ¬í˜„ ë©”ì„œë“œë“¤
    # =======================================================================

    def _get_docs_impl(
        self, query: str, run_manager: Optional[Any] = None
    ) -> List[Document]:
        """ì‹¤ì œ ë¬¸ì„œ ê²€ìƒ‰ êµ¬í˜„"""
        # ì§€ì—° ì´ˆê¸°í™”
        self._lazy_init()

        # ìºì‹œ í™•ì¸
        if self.enable_caching:
            cached_result = self._get_from_cache(query)
            if cached_result:
                logger.info("âœ… Using cached result")
                return cached_result

        try:
            logger.info(f"ğŸ” Processing query: '{query[:50]}...'")

            # 1. ì¿¼ë¦¬ ë¶„ì„ (ì„ íƒì )
            query_analysis = None
            if self.enable_query_analysis and self.query_analyzer:
                logger.info("ğŸ“Š Analyzing query...")
                query_analysis = self.query_analyzer.analyze(query)
                logger.info(f"   Complexity: {query_analysis.complexity.value}")
                logger.info(f"   Type: {query_analysis.query_type.value}")

            # 2. ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
            logger.info("ğŸ” Extracting relevant subgraph...")
            subgraph_result = self.subgraph_extractor.extract_subgraph(
                query=query, query_analysis=query_analysis
            )

            # 3. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”
            logger.info("ğŸ“ Serializing context...")
            serialized_context = self.context_serializer.serialize(
                subgraph_result=subgraph_result, query_analysis=query_analysis
            )

            # 4. LangChain Document í˜•íƒœë¡œ ë³€í™˜
            documents = self._convert_to_langchain_documents(
                serialized_context, subgraph_result, query_analysis
            )

            # 5. ê´€ë ¨ì„± ì ìˆ˜ë¡œ í•„í„°ë§ ë° ì •ë ¬
            filtered_docs = self._filter_and_rank_documents(documents)

            # 6. ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache(query, filtered_docs)

            logger.info(f"âœ… Retrieved {len(filtered_docs)} documents")
            return filtered_docs

        except Exception as e:
            logger.error(f"âŒ Document retrieval failed: {e}")
            # ì—ëŸ¬ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì²´ì¸ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡)
            return []

    async def _aget_docs_impl(
        self, query: str, run_manager: Optional[Any] = None
    ) -> List[Document]:
        """ë¹„ë™ê¸° ë¬¸ì„œ ê²€ìƒ‰ êµ¬í˜„"""
        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._get_docs_impl, query, run_manager)

    def _convert_to_langchain_documents(
        self,
        serialized_context: SerializedContext,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
    ) -> List[Document]:
        """SerializedContextë¥¼ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""

        documents = []

        # 1. ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ
        main_doc = Document(
            page_content=serialized_context.main_text,
            metadata={
                "source": "graphrag_main_context",
                "query": serialized_context.query,
                "total_nodes": serialized_context.included_nodes,
                "total_edges": serialized_context.included_edges,
                "confidence_score": getattr(
                    serialized_context, "confidence_score", 0.0
                ),
                "language": serialized_context.language,
                "extraction_strategy": subgraph_result.extraction_strategy.value,
                "processing_time": subgraph_result.extraction_time,
                "document_type": "main_context",
            },
        )
        documents.append(main_doc)

        # 2. ê°œë³„ ë…¸ë“œ ë¬¸ì„œë“¤ (ìƒìœ„ ê´€ë ¨ì„±ë§Œ)
        top_relevant_nodes = sorted(
            subgraph_result.relevance_scores.items(), key=lambda x: x[1], reverse=True
        )[: self.max_docs]

        for node_id, relevance_score in top_relevant_nodes:
            if relevance_score < self.min_relevance_score:
                continue

            node_data = subgraph_result.nodes.get(node_id, {})
            node_type = node_data.get("node_type", "unknown")

            # ë…¸ë“œë³„ í…ìŠ¤íŠ¸ ìƒì„±
            content = self._generate_node_content(node_data)

            if content:
                node_doc = Document(
                    page_content=content,
                    metadata={
                        "source": f"graphrag_node_{node_id}",
                        "node_id": node_id,
                        "node_type": node_type,
                        "relevance_score": relevance_score,
                        "document_type": "individual_node",
                        "query": serialized_context.query,
                    },
                )
                documents.append(node_doc)

        return documents

    def _generate_node_content(self, node_data: Dict[str, Any]) -> str:
        """ê°œë³„ ë…¸ë“œìš© ì»¨í…ì¸  ìƒì„±"""
        node_type = node_data.get("node_type", "unknown")
        node_id = node_data.get("id", "unknown")

        if node_type == "paper":
            title = node_data.get("title", "Unknown Title")
            abstract = node_data.get("abstract", "")  # âœ… abstract ê°€ì ¸ì˜¤ê¸°
            abstract = node_data.get("abstract", "")
            authors = node_data.get("authors", [])
            year = node_data.get("year", "")
            keywords = node_data.get("keywords", [])

            content = f"Paper: {title}"
            if year:
                content += f" ({year})"
            if authors:
                author_list = authors if isinstance(authors, list) else [authors]
                content += f"\nAuthors: {', '.join(str(a) for a in author_list[:3])}"
            if abstract:
                content += f"\nAbstract: {abstract[:300]}..."
        # âœ… Abstract ì¶”ê°€ (ê°€ì¥ ì¤‘ìš”!)
        if abstract:
            abstract_clean = abstract.replace("\n", " ").strip()
            if len(abstract_clean) > 200:
                abstract_clean = abstract_clean[:200] + "..."
            content += f"\nAbstract: {abstract_clean}"

        # âœ… Keywords ì¶”ê°€
        if keywords:
            if isinstance(keywords, str):
                keyword_list = [kw.strip() for kw in keywords.split(";")][:5]
            elif isinstance(keywords, list):
                keyword_list = [str(kw).strip() for kw in keywords[:5]]
            else:
                keyword_list = []

            if keyword_list:
                content += f"\nKeywords: {', '.join(keyword_list)}"

        elif node_type == "author":
            name = node_data.get("name", node_id)
            paper_count = node_data.get("paper_count", 0)
            top_keywords = node_data.get("top_keywords", [])

            content = f"Author: {name}"
            if paper_count:
                content += f"\nPublications: {paper_count} papers"
            if top_keywords:
                if (
                    isinstance(top_keywords[0], (list, tuple))
                    if top_keywords
                    else False
                ):
                    keywords = [str(kw[0]) for kw in top_keywords[:5]]
                else:
                    keywords = [str(kw) for kw in top_keywords[:5]]
                content += f"\nResearch Areas: {', '.join(keywords)}"

        elif node_type == "keyword":
            keyword = node_data.get("name", node_id)
            frequency = node_data.get("frequency", 0)

            content = f"Keyword: {keyword}"
            if frequency:
                content += f"\nFrequency: {frequency} papers"

        elif node_type == "journal":
            name = node_data.get("name", node_id)
            paper_count = node_data.get("paper_count", 0)

            content = f"Journal: {name}"
            if paper_count:
                content += f"\nPublications: {paper_count} papers"
        else:
            content = f"{node_type}: {node_id}"

        return content

    def _filter_and_rank_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ í•„í„°ë§ ë° ë­í‚¹"""

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ í•„í„°ë§
        filtered_docs = []
        for doc in documents:
            relevance_score = doc.metadata.get("relevance_score", 1.0)

            # ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ëŠ” í•­ìƒ í¬í•¨
            if doc.metadata.get("document_type") == "main_context":
                filtered_docs.append(doc)
            # ê°œë³„ ë…¸ë“œëŠ” ì„ê³„ê°’ ì ìš©
            elif relevance_score >= self.min_relevance_score:
                filtered_docs.append(doc)

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ëŠ” ë§¨ ì•)
        def sort_key(doc):
            doc_type = doc.metadata.get("document_type", "unknown")
            relevance = doc.metadata.get("relevance_score", 0.0)

            if doc_type == "main_context":
                return (2, relevance)  # ë†’ì€ ìš°ì„ ìˆœìœ„
            else:
                return (1, relevance)  # ê´€ë ¨ì„± ì ìˆ˜ ìˆœ

        filtered_docs.sort(key=sort_key, reverse=True)

        # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì œí•œ
        return filtered_docs[: self.max_docs]

    def _get_cache_key(self, query: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib

        key_data = f"{query}_{self.max_docs}_{self.min_relevance_score}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _get_from_cache(self, query: str) -> Optional[List[Document]]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        if not self.enable_caching:
            return None

        cache_key = self._get_cache_key(query)

        if cache_key in self.query_cache:
            # TTL í™•ì¸
            import time

            cache_time = self.cache_timestamps.get(cache_key, 0)

            if time.time() - cache_time < self.cache_ttl_seconds:
                return self.query_cache[cache_key]
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self.query_cache[cache_key]
                del self.cache_timestamps[cache_key]

        return None

    def _save_to_cache(self, query: str, documents: List[Document]) -> None:
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        if not self.enable_caching:
            return

        cache_key = self._get_cache_key(query)

        import time

        self.query_cache[cache_key] = documents
        self.cache_timestamps[cache_key] = time.time()

        # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
        if len(self.query_cache) > 100:
            # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì œê±°
            oldest_key = min(self.cache_timestamps, key=self.cache_timestamps.get)
            del self.query_cache[oldest_key]
            del self.cache_timestamps[oldest_key]

    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.query_cache.clear()
        self.cache_timestamps.clear()
        logger.info("ğŸ—‘ï¸ Cache cleared")

    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return {
            "cached_queries": len(self.query_cache),
            "cache_size_mb": sum(len(str(docs)) for docs in self.query_cache.values())
            / 1024
            / 1024,
            "cache_hit_ratio": getattr(self, "_cache_hits", 0)
            / max(1, getattr(self, "_total_queries", 1)),
        }

    def update_config(self, **kwargs) -> None:
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
                logger.info(f"ğŸ“ Updated {key} = {value}")

        # ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
        if any(
            key in kwargs
            for key in ["unified_graph_path", "vector_store_path", "embedding_model"]
        ):
            self._initialized = False
            logger.info("ğŸ”„ Components will be reinitialized on next query")


def create_graphrag_retriever(
    unified_graph_path: str,
    vector_store_path: str,
    embedding_model: str = "auto",
    config_manager: Optional[object] = None,  # ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°
    **kwargs,
) -> GraphRAGRetriever:
    """GraphRAGRetriever íŒ©í† ë¦¬ í•¨ìˆ˜ - Config Manager ì§€ì›"""

    # embedding_modelì´ "auto"ì¼ ë•Œ config_managerì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if embedding_model == "auto" and config_manager is not None:
        try:
            # YAML ì„¤ì •ì—ì„œ ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            embedding_config = config_manager.get_embeddings_config()
            embedding_model = embedding_config["model_name"]
            logger.info(f"ğŸ¯ Using embedding model from config: {embedding_model}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to get embedding model from config: {e}")
            logger.info("ğŸ¯ Falling back to auto selection")
            # auto ì„ íƒ ë¡œì§ ê·¸ëŒ€ë¡œ ì§„í–‰

    return GraphRAGRetriever(
        unified_graph_path=unified_graph_path,
        vector_store_path=vector_store_path,
        embedding_model=embedding_model,
        **kwargs,
    )


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    """GraphRAGRetriever ì‚¬ìš© ì˜ˆì‹œ"""

    if not _langchain_available:
        print("âŒ LangChain not available for testing")
        return

    print("ğŸ§ª Testing Fixed GraphRAGRetriever...")

    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    from pathlib import Path

    base_dir = Path(__file__).parent.parent.parent.parent

    try:
        # GraphRAGRetriever ìƒì„±
        retriever = create_graphrag_retriever(
            unified_graph_path=str(
                base_dir
                / "data"
                / "processed"
                / "graphs"
                / "unified"
                / "unified_knowledge_graph.json"
            ),
            vector_store_path=str(base_dir / "data" / "processed" / "vector_store"),
            embedding_model="auto",
            max_docs=5,
            min_relevance_score=0.3,
        )

        # í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
        print(f"âœ… Retriever created successfully")
        print(f"   LangChain new version: {_langchain_new_version}")

        # ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
        methods_to_check = [
            "get_relevant_documents",
            "_get_relevant_documents",
            "aget_relevant_documents",
            "_aget_relevant_documents",
        ]

        for method_name in methods_to_check:
            exists = hasattr(retriever, method_name)
            print(f"   {method_name}: {'âœ…' if exists else 'âŒ'}")

        print(f"\nâœ… Fixed GraphRAGRetriever test completed!")

    except Exception as e:
        print(f"âŒ Test setup failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
