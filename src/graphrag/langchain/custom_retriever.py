"""
GraphRAG LangChain ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„
Custom GraphRAG Retriever for LangChain Integration

LangChain BaseRetrieverë¥¼ ìƒì†ë°›ì•„ GraphRAG ì‹œìŠ¤í…œê³¼ ì™„ì „ í†µí•©
- ì¿¼ë¦¬ ë¶„ì„ â†’ ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ â†’ ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™” íŒŒì´í”„ë¼ì¸
- LangChain Document í˜•íƒœë¡œ ê²°ê³¼ ë°˜í™˜
- ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
- ë©”íƒ€ë°ì´í„° ë° ì†ŒìŠ¤ ì¶”ì 
"""

import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Callable
from pydantic import Field
import warnings

# LangChain imports
try:
    from langchain_core.retrievers import BaseRetriever
    from langchain_core.documents import Document
    from langchain_core.callbacks import (
        CallbackManagerForRetrieverRun,
        AsyncCallbackManagerForRetrieverRun,
    )

    _langchain_available = True
except ImportError:
    _langchain_available = False
    warnings.warn(
        "LangChain not available. Install with: pip install langchain langchain-core"
    )

    # Placeholder classes for type hints
    class BaseRetriever:
        def __init__(self, *args, **kwargs):
            raise ImportError("LangChain is required but not installed")

    class Document:
        def __init__(self, *args, **kwargs):
            pass

    class CallbackManagerForRetrieverRun:
        pass

    class AsyncCallbackManagerForRetrieverRun:
        pass


# GraphRAG imports
try:
    from ..query_analyzer import QueryAnalyzer, QueryAnalysisResult
    from ..embeddings.subgraph_extractor import SubgraphExtractor, SubgraphResult
    from ..embeddings.context_serializer import ContextSerializer, SerializedContext
    from ..embeddings.vector_store_manager import VectorStoreManager
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class GraphRAGRetriever(BaseRetriever):
    """GraphRAG ì»¤ìŠ¤í…€ LangChain ë¦¬íŠ¸ë¦¬ë²„

    LangChainì˜ BaseRetrieverë¥¼ ìƒì†ë°›ì•„ GraphRAG ì‹œìŠ¤í…œì„
    LangChain ì²´ì¸ì—ì„œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì–´ëŒ‘í„° í´ë˜ìŠ¤
    """

    # Pydantic í•„ë“œ ì •ì˜ (LangChain v0.1+ í˜¸í™˜)
    unified_graph_path: str = Field(description="í†µí•© ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ")
    vector_store_path: str = Field(description="ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ")
    embedding_model: str = Field(default="auto", description="ì„ë² ë”© ëª¨ë¸ëª…")

    # êµ¬ì„±ìš”ì†Œ ì„¤ì •
    max_docs: int = Field(default=10, description="ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜")
    min_relevance_score: float = Field(default=0.3, description="ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜")
    enable_query_analysis: bool = Field(default=True, description="ì¿¼ë¦¬ ë¶„ì„ í™œì„±í™”")

    # ë‚´ë¶€ ì»´í¬ë„ŒíŠ¸ë“¤ (ì´ˆê¸°í™” í›„ ì„¤ì •)
    query_analyzer: Optional[QueryAnalyzer] = Field(default=None, exclude=True)
    subgraph_extractor: Optional[SubgraphExtractor] = Field(default=None, exclude=True)
    context_serializer: Optional[ContextSerializer] = Field(default=None, exclude=True)

    # ìºì‹œ ë° ì„±ëŠ¥ ì„¤ì •
    enable_caching: bool = Field(default=True, description="ê²°ê³¼ ìºì‹± í™œì„±í™”")
    cache_ttl_seconds: int = Field(default=3600, description="ìºì‹œ ìœ ì§€ ì‹œê°„(ì´ˆ)")

    # ë‚´ë¶€ ìºì‹œ ì €ì¥ì†Œ
    query_cache: Dict[str, Document] = Field(default_factory=dict, exclude=True)
    cache_timestamps: Dict[str, float] = Field(default_factory=dict, exclude=True)

    class Config:
        """Pydantic ì„¤ì •"""

        arbitrary_types_allowed = True
        exclude = {
            "query_analyzer",
            "subgraph_extractor",
            "context_serializer",
            "query_cache",
            "cache_timestamps",
        }

    def __init__(self, **kwargs):
        """GraphRAGRetriever ì´ˆê¸°í™”"""
        super().__init__(**kwargs)

        if not _langchain_available:
            raise ImportError(
                "LangChain is required for GraphRAGRetriever.\n"
                "Install with: pip install langchain langchain-core"
            )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” í”Œë˜ê·¸
        self._initialized = False

        logger.info("âœ… GraphRAGRetriever created")
        logger.info(f"   ğŸ“ Graph: {self.unified_graph_path}")
        logger.info(f"   ğŸ—„ï¸ Vector Store: {self.vector_store_path}")
        logger.info(f"   ğŸ¤– Model: {self.embedding_model}")

    def _lazy_init(self) -> None:
        """ì§€ì—° ì´ˆê¸°í™” - ì²« ë²ˆì§¸ ì¿¼ë¦¬ ì‹œ ì‹¤í–‰"""
        if self._initialized:
            return

        logger.info("ğŸ”§ Initializing GraphRAG components...")

        try:
            # 1. Query Analyzer ì´ˆê¸°í™”
            if self.enable_query_analysis:
                self.query_analyzer = QueryAnalyzer()
                logger.info("âœ… QueryAnalyzer initialized")

            # 2. SubgraphExtractor ì´ˆê¸°í™”
            self.subgraph_extractor = SubgraphExtractor(
                unified_graph_path=self.unified_graph_path,
                vector_store_path=self.vector_store_path,
                embedding_model=self.embedding_model,
            )
            logger.info("âœ… SubgraphExtractor initialized")

            # 3. ContextSerializer ì´ˆê¸°í™”
            self.context_serializer = ContextSerializer()
            logger.info("âœ… ContextSerializer initialized")

            self._initialized = True
            logger.info("ğŸš€ GraphRAG components ready!")

        except Exception as e:
            logger.error(f"âŒ Component initialization failed: {e}")
            raise

    def _get_docs(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """ë™ê¸° ë¬¸ì„œ ê²€ìƒ‰ (LangChain BaseRetriever ì¸í„°í˜ì´ìŠ¤)"""

        # ì§€ì—° ì´ˆê¸°í™”
        self._lazy_init()

        # ìºì‹œ í™•ì¸
        if self.enable_caching:
            cached_result = self._get_from_cache(query)
            if cached_result:
                logger.info("âœ… Using cached result")
                return cached_result

        try:
            logger.info(f"ğŸ” Processing query: '{query[:50]}...'")

            # 1. ì¿¼ë¦¬ ë¶„ì„ (ì„ íƒì )
            query_analysis = None
            if self.enable_query_analysis and self.query_analyzer:
                logger.info("ğŸ“Š Analyzing query...")
                query_analysis = self.query_analyzer.analyze(query)
                logger.info(f"   Complexity: {query_analysis.complexity.value}")
                logger.info(f"   Type: {query_analysis.query_type.value}")

            # 2. ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
            logger.info("ğŸ” Extracting relevant subgraph...")
            subgraph_result = self.subgraph_extractor.extract_subgraph(
                query=query, query_analysis=query_analysis
            )

            # 3. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”
            logger.info("ğŸ“ Serializing context...")
            serialized_context = self.context_serializer.serialize(
                subgraph_result=subgraph_result, query_analysis=query_analysis
            )

            # 4. LangChain Document í˜•íƒœë¡œ ë³€í™˜
            documents = self._convert_to_langchain_documents(
                serialized_context, subgraph_result, query_analysis
            )

            # 5. ê´€ë ¨ì„± ì ìˆ˜ë¡œ í•„í„°ë§ ë° ì •ë ¬
            filtered_docs = self._filter_and_rank_documents(documents)

            # 6. ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache(query, filtered_docs)

            logger.info(f"âœ… Retrieved {len(filtered_docs)} documents")
            return filtered_docs

        except Exception as e:
            logger.error(f"âŒ Document retrieval failed: {e}")
            # ì—ëŸ¬ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì²´ì¸ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡)
            return []

    async def _aget_docs(
        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
    ) -> List[Document]:
        """ë¹„ë™ê¸° ë¬¸ì„œ ê²€ìƒ‰"""

        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        loop = asyncio.get_event_loop()

        # CallbackManager íƒ€ì… ë³€í™˜ (ë™ê¸° ë²„ì „ìœ¼ë¡œ)
        sync_run_manager = None  # í•„ìš”ì‹œ êµ¬í˜„

        return await loop.run_in_executor(None, self._get_docs, query, sync_run_manager)

    def _convert_to_langchain_documents(
        self,
        serialized_context: SerializedContext,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
    ) -> List[Document]:
        """SerializedContextë¥¼ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""

        documents = []

        # 1. ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ
        main_doc = Document(
            page_content=serialized_context.main_text,
            metadata={
                "source": "graphrag_main_context",
                "query": serialized_context.query,
                "total_nodes": serialized_context.included_nodes,
                "total_edges": serialized_context.included_edges,
                "confidence_score": serialized_context.confidence_score,
                "language": serialized_context.language,
                "extraction_strategy": subgraph_result.extraction_strategy.value,
                "processing_time": subgraph_result.extraction_time,
                "document_type": "main_context",
            },
        )
        documents.append(main_doc)

        # 2. ê°œë³„ ë…¸ë“œ ë¬¸ì„œë“¤ (ìƒìœ„ ê´€ë ¨ì„±ë§Œ)
        top_relevant_nodes = sorted(
            subgraph_result.relevance_scores.items(), key=lambda x: x[1], reverse=True
        )[: self.max_docs]

        for node_id, relevance_score in top_relevant_nodes:
            if relevance_score < self.min_relevance_score:
                continue

            node_data = subgraph_result.nodes.get(node_id, {})
            node_type = node_data.get("node_type", "unknown")

            # ë…¸ë“œë³„ í…ìŠ¤íŠ¸ ìƒì„±
            content = self._generate_node_content(node_data)

            if content:
                node_doc = Document(
                    page_content=content,
                    metadata={
                        "source": f"graphrag_node_{node_id}",
                        "node_id": node_id,
                        "node_type": node_type,
                        "relevance_score": relevance_score,
                        "document_type": "individual_node",
                        "query": serialized_context.query,
                    },
                )
                documents.append(node_doc)

        return documents

    def _generate_node_content(self, node_data: Dict[str, Any]) -> str:
        """ê°œë³„ ë…¸ë“œìš© ì»¨í…ì¸  ìƒì„±"""
        node_type = node_data.get("node_type", "unknown")
        node_id = node_data.get("id", "unknown")

        if node_type == "paper":
            title = node_data.get("title", "Unknown Title")
            abstract = node_data.get("abstract", "")
            authors = node_data.get("authors", [])
            year = node_data.get("year", "")

            content = f"Paper: {title}"
            if year:
                content += f" ({year})"
            if authors:
                author_list = authors if isinstance(authors, list) else [authors]
                content += f"\nAuthors: {', '.join(author_list[:3])}"
            if abstract:
                content += f"\nAbstract: {abstract[:300]}..."

        elif node_type == "author":
            name = node_data.get("name", node_id)
            paper_count = node_data.get("paper_count", 0)
            top_keywords = node_data.get("top_keywords", [])

            content = f"Author: {name}"
            if paper_count:
                content += f"\nPublications: {paper_count} papers"
            if top_keywords:
                if isinstance(top_keywords[0], (list, tuple)):
                    keywords = [kw[0] for kw in top_keywords[:5]]
                else:
                    keywords = top_keywords[:5]
                content += f"\nResearch Areas: {', '.join(keywords)}"

        elif node_type == "keyword":
            keyword = node_data.get("name", node_id)
            frequency = node_data.get("frequency", 0)

            content = f"Keyword: {keyword}"
            if frequency:
                content += f"\nFrequency: {frequency} papers"

        elif node_type == "journal":
            name = node_data.get("name", node_id)
            paper_count = node_data.get("paper_count", 0)

            content = f"Journal: {name}"
            if paper_count:
                content += f"\nPublications: {paper_count} papers"
        else:
            content = f"{node_type}: {node_id}"

        return content

    def _filter_and_rank_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ í•„í„°ë§ ë° ë­í‚¹"""

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ í•„í„°ë§
        filtered_docs = []
        for doc in documents:
            relevance_score = doc.metadata.get("relevance_score", 1.0)

            # ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ëŠ” í•­ìƒ í¬í•¨
            if doc.metadata.get("document_type") == "main_context":
                filtered_docs.append(doc)
            # ê°œë³„ ë…¸ë“œëŠ” ì„ê³„ê°’ ì ìš©
            elif relevance_score >= self.min_relevance_score:
                filtered_docs.append(doc)

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ëŠ” ë§¨ ì•)
        def sort_key(doc):
            doc_type = doc.metadata.get("document_type", "unknown")
            relevance = doc.metadata.get("relevance_score", 0.0)

            if doc_type == "main_context":
                return (2, relevance)  # ë†’ì€ ìš°ì„ ìˆœìœ„
            else:
                return (1, relevance)  # ê´€ë ¨ì„± ì ìˆ˜ ìˆœ

        filtered_docs.sort(key=sort_key, reverse=True)

        # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì œí•œ
        return filtered_docs[: self.max_docs]

    def _get_cache_key(self, query: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib

        key_data = f"{query}_{self.max_docs}_{self.min_relevance_score}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _get_from_cache(self, query: str) -> Optional[List[Document]]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        if not self.enable_caching:
            return None

        cache_key = self._get_cache_key(query)

        if cache_key in self.query_cache:
            # TTL í™•ì¸
            import time

            cache_time = self.cache_timestamps.get(cache_key, 0)

            if time.time() - cache_time < self.cache_ttl_seconds:
                return self.query_cache[cache_key]
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self.query_cache[cache_key]
                del self.cache_timestamps[cache_key]

        return None

    def _save_to_cache(self, query: str, documents: List[Document]) -> None:
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        if not self.enable_caching:
            return

        cache_key = self._get_cache_key(query)

        import time

        self.query_cache[cache_key] = documents
        self.cache_timestamps[cache_key] = time.time()

        # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
        if len(self.query_cache) > 100:
            # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì œê±°
            oldest_key = min(self.cache_timestamps, key=self.cache_timestamps.get)
            del self.query_cache[oldest_key]
            del self.cache_timestamps[oldest_key]

    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.query_cache.clear()
        self.cache_timestamps.clear()
        logger.info("ğŸ—‘ï¸ Cache cleared")

    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return {
            "cached_queries": len(self.query_cache),
            "cache_size_mb": sum(len(str(docs)) for docs in self.query_cache.values())
            / 1024
            / 1024,
            "cache_hit_ratio": getattr(self, "_cache_hits", 0)
            / max(1, getattr(self, "_total_queries", 1)),
        }

    def update_config(self, **kwargs) -> None:
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
                logger.info(f"ğŸ“ Updated {key} = {value}")

        # ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
        if any(
            key in kwargs
            for key in ["unified_graph_path", "vector_store_path", "embedding_model"]
        ):
            self._initialized = False
            logger.info("ğŸ”„ Components will be reinitialized on next query")


def create_graphrag_retriever(
    unified_graph_path: str,
    vector_store_path: str,
    embedding_model: str = "auto",
    **kwargs,
) -> GraphRAGRetriever:
    """GraphRAGRetriever íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        unified_graph_path: í†µí•© ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
        vector_store_path: ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ
        embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
        **kwargs: ì¶”ê°€ ì„¤ì •

    Returns:
        GraphRAGRetriever ì¸ìŠ¤í„´ìŠ¤
    """
    return GraphRAGRetriever(
        unified_graph_path=unified_graph_path,
        vector_store_path=vector_store_path,
        embedding_model=embedding_model,
        **kwargs,
    )


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    """GraphRAGRetriever ì‚¬ìš© ì˜ˆì‹œ"""

    if not _langchain_available:
        print("âŒ LangChain not available for testing")
        return

    print("ğŸ§ª Testing GraphRAGRetriever...")

    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    from pathlib import Path

    base_dir = Path(__file__).parent.parent.parent.parent

    try:
        # GraphRAGRetriever ìƒì„±
        retriever = create_graphrag_retriever(
            unified_graph_path=str(
                base_dir / "graphs" / "unified" / "unified_knowledge_graph.json"
            ),
            vector_store_path=str(base_dir / "graphs" / "embeddings"),
            embedding_model="auto",
            max_docs=5,
            min_relevance_score=0.3,
        )

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_queries = [
            "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì€?",
            "ê¹€ì² ìˆ˜ êµìˆ˜ì˜ ì—°êµ¬ ë¶„ì•¼ëŠ”?",
            "ì „ê¸°ì°¨ ì¶©ì „ ê´€ë ¨ ìµœì‹  ì—°êµ¬ ë™í–¥ì€?",
        ]

        for query in test_queries[:1]:  # ì²« ë²ˆì§¸ë§Œ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ“ Query: {query}")

            try:
                # ë¬¸ì„œ ê²€ìƒ‰
                documents = retriever._get_docs(query, run_manager=None)

                print(f"âœ… Retrieved {len(documents)} documents:")
                for i, doc in enumerate(documents):
                    doc_type = doc.metadata.get("document_type", "unknown")
                    relevance = doc.metadata.get("relevance_score", "N/A")
                    print(f"   ğŸ“„ Doc {i+1} ({doc_type}): relevance={relevance}")
                    print(f"      Content: {doc.page_content[:100]}...")

                # ìºì‹œ í†µê³„
                cache_stats = retriever.get_cache_stats()
                print(f"ğŸ“Š Cache stats: {cache_stats}")

            except Exception as e:
                print(f"âŒ Query failed: {e}")

        print(f"\nâœ… GraphRAGRetriever test completed!")

    except Exception as e:
        print(f"âŒ Test setup failed: {e}")


if __name__ == "__main__":
    main()
