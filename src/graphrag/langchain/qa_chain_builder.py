"""
GraphRAG QA ì²´ì¸ ë¹Œë”
QA Chain Builder for GraphRAG System

LangChain ê¸°ë°˜ ìµœì í™”ëœ QA ì²´ì¸ êµ¬ì¶•
- Custom GraphRAG Retriever í†µí•©
- ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì²´ì¸
- ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ëŒ€í™” íˆìŠ¤í† ë¦¬
- ë¡œì»¬ LLM ìµœì í™” ë° ì—ëŸ¬ í•¸ë“¤ë§
- ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹± ì§€ì›
"""

import logging
import warnings
from typing import Dict, List, Any, Optional, Union, Type, Callable
from pathlib import Path
from pydantic import Field
from dataclasses import dataclass
from enum import Enum

# LangChain Core imports
try:
    from langchain_core.language_models import BaseLanguageModel
    from langchain_core.prompts import BasePromptTemplate
    from langchain_core.retrievers import BaseRetriever
    from langchain_core.memory import BaseMemory
    from langchain_core.callbacks import CallbackManagerForChainRun
    from langchain_core.documents import Document
    from langchain_core.runnables import Runnable, RunnablePassthrough, RunnableLambda
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables.utils import Input, Output

    # LangChain Community imports
    from langchain.chains import RetrievalQA
    from langchain.chains.base import Chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain.chains.retrieval import createretrieval_chain
    from langchain.chains.conversational_retrieval.base import (
        ConversationalRetrievalChain,
    )
    from langchain.memory import (
        ConversationBufferMemory,
        ConversationSummaryBufferMemory,
    )

    _langchain_available = True
except ImportError as e:
    _langchain_available = False
    warnings.warn(f"LangChain not available: {e}")

# GraphRAG imports
try:
    from .custom_retriever import GraphRAGRetriever, create_graphrag_retriever
    from .prompt_templates import (
        GraphRAGPromptTemplates,
        create_query_prompt,
        create_chat_prompt,
    )
    from .memory_manager import GraphRAGMemoryManager, create_memory_manager
    from ..query_analyzer import QueryAnalyzer, QueryAnalysisResult
    from ..graphrag_pipeline import LocalLLMManager
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class ChainType(Enum):
    """QA ì²´ì¸ íƒ€ì…"""

    BASIC_QA = "basic_qa"  # ê¸°ë³¸ QA ì²´ì¸
    RETRIEVAL_QA = "retrieval_qa"  # ê²€ìƒ‰ ê¸°ë°˜ QA
    CONVERSATIONAL_QA = "conversational_qa"  # ëŒ€í™”í˜• QA
    GRAPH_ENHANCED_QA = "graph_enhanced_qa"  # GraphRAG íŠ¹í™” QA
    MULTI_QUERY_QA = "multi_query_qa"  # ë‹¤ì¤‘ ì¿¼ë¦¬ QA
    STREAMING_QA = "streaming_qa"  # ìŠ¤íŠ¸ë¦¬ë° QA


@dataclass
class QAChainConfig:
    """QA ì²´ì¸ ì„¤ì •"""

    # ì²´ì¸ íƒ€ì… ë° ê¸°ë³¸ ì„¤ì •
    chain_type: ChainType = ChainType.GRAPH_ENHANCED_QA
    return_source_documents: bool = True
    verbose: bool = False

    # ê²€ìƒ‰ ì„¤ì •
    search_kwargs: Dict[str, Any] = None
    max_docs_for_context: int = 10
    min_relevance_score: float = 0.3

    # ë©”ëª¨ë¦¬ ì„¤ì •
    enable_memory: bool = True
    memory_type: str = "summary_buffer"
    max_memory_tokens: int = 4000

    # ì‘ë‹µ ì„¤ì •
    max_answer_tokens: int = 1000
    temperature: float = 0.1
    streaming: bool = False

    # ì—ëŸ¬ í•¸ë“¤ë§
    max_retries: int = 3
    timeout_seconds: int = 300
    fallback_to_simple: bool = True

    def __post_init__(self):
        if self.search_kwargs is None:
            self.search_kwargs = {
                "k": self.max_docs_for_context,
                "score_threshold": self.min_relevance_score,
            }


class GraphRAGQAChain(Chain):
    """GraphRAG íŠ¹í™” QA ì²´ì¸ (LangChain Chain ìƒì†) - Pydantic í˜¸í™˜"""

    # LangChain Chain í•„ìˆ˜ ì†ì„±ë“¤
    input_keys: List[str] = ["question"]
    output_keys: List[str] = ["answer"]

    # Pydantic í•„ë“œë¡œ ëª¨ë“  ì†ì„± ì •ì˜
    retriever: BaseRetriever = Field(description="GraphRAG ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„")
    llm: BaseLanguageModel = Field(description="ì–¸ì–´ ëª¨ë¸")
    prompt_template: BasePromptTemplate = Field(description="í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿")
    memory: Optional[BaseMemory] = Field(default=None, description="ë©”ëª¨ë¦¬ ê´€ë¦¬ì")
    query_analyzer: Optional[QueryAnalyzer] = Field(
        default=None, description="ì¿¼ë¦¬ ë¶„ì„ê¸°"
    )
    config: Optional[QAChainConfig] = Field(default=None, description="QA ì²´ì¸ ì„¤ì •")

    # ë‚´ë¶€ ì²´ì¸ë“¤ (exclude=Trueë¡œ ì§ë ¬í™”ì—ì„œ ì œì™¸)
    base_chain: Optional[Any] = Field(default=None, exclude=True)
    conversation_chain: Optional[Any] = Field(default=None, exclude=True)
    retrieval_chain: Optional[Any] = Field(default=None, exclude=True)

    # í†µê³„ ë° ìºì‹± (exclude=True)
    query_count: int = Field(default=0, exclude=True)
    cache: Optional[Dict[str, Any]] = Field(default=None, exclude=True)

    class Config:
        """Pydantic ì„¤ì •"""

        arbitrary_types_allowed = True

    def __init__(self, **kwargs):
        """Pydantic ë°©ì‹ìœ¼ë¡œ ì´ˆê¸°í™”"""
        # config ê¸°ë³¸ê°’ ì„¤ì •
        if "config" not in kwargs or kwargs["config"] is None:
            kwargs["config"] = QAChainConfig()

        # cache ê¸°ë³¸ê°’ ì„¤ì •
        if kwargs["config"].enable_memory:
            kwargs["cache"] = {}

        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” (Pydantic ë°©ì‹)
        super().__init__(**kwargs)

        if not _langchain_available:
            raise ImportError("LangChain is required for QA Chain Builder")

        # ë‚´ë¶€ ì²´ì¸ë“¤ ì´ˆê¸°í™”
        self._initialize_chains()

        logger.info("âœ… GraphRAGQAChain initialized")
        logger.info(f"   ğŸ”— Chain type: {self.config.chain_type.value}")
        logger.info(f"   ğŸ§  Memory enabled: {self.config.enable_memory}")
        logger.info(f"   ğŸ“„ Max docs: {self.config.max_docs_for_context}")

    def _initialize_chains(self) -> None:
        """ë‚´ë¶€ ì²´ì¸ë“¤ ì´ˆê¸°í™”"""

        # 1. ê¸°ë³¸ ë¬¸ì„œ ê²°í•© ì²´ì¸ ìƒì„±
        try:
            self.base_chain = create_stuff_documents_chain(
                llm=self.llm,
                prompt=self.prompt_template,
                document_variable_name="context",
                verbose=self.config.verbose,
            )

            # 2. ê²€ìƒ‰ ì²´ì¸ ìƒì„±
            self.retrieval_chain = createretrieval_chain(
                retriever=self.retriever, combine_docs_chain=self.base_chain
            )

            # 3. ëŒ€í™”í˜• ì²´ì¸ ìƒì„± (ë©”ëª¨ë¦¬ê°€ ìˆëŠ” ê²½ìš°)
            if self.memory and self.config.enable_memory:
                try:
                    self.conversation_chain = ConversationalRetrievalChain.from_llm(
                        llm=self.llm,
                        retriever=self.retriever,
                        memory=self.memory,
                        return_source_documents=self.config.return_source_documents,
                        verbose=self.config.verbose,
                        combine_docs_chain_kwargs={"prompt": self.prompt_template},
                    )
                    logger.info("âœ… Conversational chain initialized")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to initialize conversational chain: {e}")
                    self.conversation_chain = None

        except Exception as e:
            logger.error(f"âŒ Failed to initialize chains: {e}")
            # ìµœì†Œí•œì˜ ì²´ì¸ì´ë¼ë„ ë§Œë“¤ê¸°
            self.base_chain = None
            self.retrieval_chain = None
            self.conversation_chain = None

    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        """ë©”ì¸ ì²´ì¸ ì‹¤í–‰ (LangChain Chain ì¸í„°í˜ì´ìŠ¤)"""

        question = inputs.get("question", "")
        if not question:
            return {"answer": "ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        self.query_count += 1
        logger.info(f"ğŸ” Processing query #{self.query_count}: {question[:50]}...")

        try:
            # 1. ì¿¼ë¦¬ ë¶„ì„ (ì„ íƒì )
            query_analysis = None
            if self.query_analyzer:
                query_analysis = self.query_analyzer.analyze(question)
                logger.debug(f"ğŸ“Š Query analysis: {query_analysis.query_type.value}")

            # 2. ì²´ì¸ íƒ€ì…ë³„ ì²˜ë¦¬
            if (
                self.config.chain_type == ChainType.CONVERSATIONAL_QA
                and self.conversation_chain
            ):
                result = self._process_conversational_qa(question, run_manager)
            elif self.config.chain_type == ChainType.GRAPH_ENHANCED_QA:
                result = self._process_graph_enhanced_qa(
                    question, query_analysis, run_manager
                )
            else:
                result = self._process_basic_qa(question, run_manager)

            # 3. ê²°ê³¼ í›„ì²˜ë¦¬
            processed_result = self._post_process_result(
                result, question, query_analysis
            )

            logger.info(f"âœ… Query #{self.query_count} completed")
            return processed_result

        except Exception as e:
            logger.error(f"âŒ Query #{self.query_count} failed: {e}")

            # í´ë°± ì²˜ë¦¬
            if self.config.fallback_to_simple:
                return self._fallback_answer(question, str(e))
            else:
                raise

    # ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì€ ë™ì¼...
    def _process_conversational_qa(
        self, question: str, run_manager: Optional[CallbackManagerForChainRun]
    ) -> Dict[str, Any]:
        """ëŒ€í™”í˜• QA ì²˜ë¦¬"""

        logger.debug("ğŸ’¬ Processing conversational QA")

        if not self.conversation_chain:
            # í´ë°±: ê¸°ë³¸ QAë¡œ ì²˜ë¦¬
            return self._process_basic_qa(question, run_manager)

        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê³ ë ¤í•œ ì²˜ë¦¬
            result = self.conversation_chain(
                {
                    "question": question,
                    "chat_history": (
                        self.memory.chat_memory.messages if self.memory else []
                    ),
                }
            )

            return {
                "answer": result.get("answer", ""),
                "source_documents": result.get("source_documents", []),
                "chat_history": result.get("chat_history", []),
            }

        except Exception as e:
            logger.warning(f"âš ï¸ Conversational QA failed: {e}, falling back to basic QA")
            return self._process_basic_qa(question, run_manager)

    def _process_graph_enhanced_qa(
        self,
        question: str,
        query_analysis: Optional[QueryAnalysisResult],
        run_manager: Optional[CallbackManagerForChainRun],
    ) -> Dict[str, Any]:
        """GraphRAG íŠ¹í™” QA ì²˜ë¦¬"""

        logger.debug("ğŸ•¸ï¸ Processing graph-enhanced QA")

        try:
            # 1. ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ í™œìš©)
            if query_analysis and hasattr(self.retriever, "update_config"):
                # ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ê²€ìƒ‰ ì„¤ì • ì¡°ì •
                search_config = self._adapt_search_config(query_analysis)
                self.retriever.update_config(**search_config)

            # 2. ê²€ìƒ‰ ì²´ì¸ ì‹¤í–‰
            if self.retrieval_chain:
                result = self.retrieval_chain.invoke({"input": question})
            else:
                # ê²€ìƒ‰ ì²´ì¸ì´ ì—†ìœ¼ë©´ ì§ì ‘ ê²€ìƒ‰
                docs = self.retriever.get_relevant_documents(question)
                context = "\n\n".join([doc.page_content for doc in docs])

                # LLMìœ¼ë¡œ ì§ì ‘ ë‹µë³€ ìƒì„±
                prompt_text = f"ì»¨í…ìŠ¤íŠ¸: {context}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"
                answer = self.llm.invoke(prompt_text)

                result = {
                    "answer": answer if isinstance(answer, str) else str(answer),
                    "context": docs,
                }

            return {
                "answer": result.get("answer", ""),
                "source_documents": result.get("context", []),
                "query_analysis": query_analysis,
            }

        except Exception as e:
            logger.warning(f"âš ï¸ Graph-enhanced QA failed: {e}, falling back to basic QA")
            return self._process_basic_qa(question, run_manager)

    def _process_basic_qa(
        self, question: str, run_manager: Optional[CallbackManagerForChainRun]
    ) -> Dict[str, Any]:
        """ê¸°ë³¸ QA ì²˜ë¦¬"""

        logger.debug("ğŸ“ Processing basic QA")

        try:
            if self.retrieval_chain:
                # ê²€ìƒ‰ ì²´ì¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                result = self.retrieval_chain.invoke({"input": question})
            else:
                # ê²€ìƒ‰ ì²´ì¸ì´ ì—†ìœ¼ë©´ ì§ì ‘ ì²˜ë¦¬
                docs = self.retriever.get_relevant_documents(question)
                context = "\n\n".join([doc.page_content for doc in docs[:5]])

                # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë‹µë³€ ìƒì„±
                prompt_text = f"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n\nì»¨í…ìŠ¤íŠ¸: {context}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"

                try:
                    if hasattr(self.llm, "invoke"):
                        answer = self.llm.invoke(prompt_text)
                    elif hasattr(self.llm, "_call"):
                        answer = self.llm._call(prompt_text)
                    else:
                        answer = str(self.llm(prompt_text))

                    answer = answer if isinstance(answer, str) else str(answer)
                except Exception as llm_error:
                    logger.error(f"âŒ LLM call failed: {llm_error}")
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

                result = {"answer": answer, "context": docs}

            return {
                "answer": result.get("answer", ""),
                "source_documents": result.get("context", []),
            }

        except Exception as e:
            logger.error(f"âŒ Basic QA failed: {e}")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "source_documents": [],
            }

    # ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ (ë™ì¼í•˜ê²Œ ìœ ì§€)
    def _adapt_search_config(
        self, query_analysis: QueryAnalysisResult
    ) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ê²€ìƒ‰ ì„¤ì • ì¡°ì •"""
        config = {}

        # ë³µì¡ë„ì— ë”°ë¥¸ ë¬¸ì„œ ìˆ˜ ì¡°ì •
        if query_analysis.complexity.value == "simple":
            config["max_docs"] = min(5, self.config.max_docs_for_context)
        elif query_analysis.complexity.value == "complex":
            config["max_docs"] = self.config.max_docs_for_context
        elif query_analysis.complexity.value == "exploratory":
            config["max_docs"] = min(15, self.config.max_docs_for_context + 5)

        return config

    def _post_process_result(
        self,
        result: Dict[str, Any],
        question: str,
        query_analysis: Optional[QueryAnalysisResult],
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""

        answer = result.get("answer", "")
        source_docs = result.get("source_documents", [])

        # ë‹µë³€ ê¸¸ì´ ì œí•œ
        if len(answer) > self.config.max_answer_tokens * 4:
            answer = answer[: self.config.max_answer_tokens * 4] + "..."
            logger.debug("ğŸ“ Answer truncated due to length limit")

        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            "answer": answer,
            "question": question,
            "source_count": len(source_docs),
        }

        if self.config.return_source_documents:
            final_result["source_documents"] = source_docs[
                : self.config.max_docs_for_context
            ]

        if query_analysis:
            final_result["query_analysis"] = {
                "type": query_analysis.query_type.value,
                "complexity": query_analysis.complexity.value,
                "confidence": getattr(query_analysis, "confidence_score", 0.0),
            }

        return final_result

    def _fallback_answer(self, question: str, error_msg: str) -> Dict[str, Any]:
        """í´ë°± ë‹µë³€ ìƒì„±"""

        fallback_answer = (
            f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
            f"ë¬¸ì œ: {error_msg}\n\n"
            f"ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
        )

        return {
            "answer": fallback_answer,
            "question": question,
            "source_count": 0,
            "error": error_msg,
            "is_fallback": True,
        }

    @property
    def _chain_type(self) -> str:
        """LangChain Chain íƒ€ì… ì‹ë³„ì"""
        return "graphrag_qa_chain"


class QAChainBuilder:
    """GraphRAG QA ì²´ì¸ ë¹Œë” - íŒ©í† ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        unified_graph_path: str,
        vector_store_path: str,
        config_manager: Optional[object] = None,
    ):
        """
        Args:
            unified_graph_path: í†µí•© ê·¸ë˜í”„ ê²½ë¡œ
            vector_store_path: ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ
            config_manager: GraphRAG ì„¤ì • ê´€ë¦¬ì (ì„ íƒì )
        """

        if not _langchain_available:
            raise ImportError("LangChain is required for QA Chain Builder")

        self.unified_graph_path = Path(unified_graph_path)
        self.vector_store_path = Path(vector_store_path)
        self.config_manager = config_manager

        # ì»´í¬ë„ŒíŠ¸ë“¤ (ì§€ì—° ë¡œë”©)
        self._retriever = None
        self._llm = None
        self._memory_manager = None
        self._query_analyzer = None
        self._prompt_templates = None

        logger.info("âœ… QAChainBuilder initialized")

    def create_chain(
        self,
        chain_type: Union[ChainType, str] = ChainType.GRAPH_ENHANCED_QA,
        llm: Optional[BaseLanguageModel] = None,
        embedding_model: str = "auto",
        config: Optional[QAChainConfig] = None,
        **kwargs,
    ) -> GraphRAGQAChain:
        """QA ì²´ì¸ ìƒì„± ë©”ì¸ ë©”ì„œë“œ"""

        # ì²´ì¸ íƒ€ì… ë³€í™˜
        if isinstance(chain_type, str):
            try:
                chain_type = ChainType(chain_type)
            except ValueError:
                logger.warning(f"âš ï¸ Unknown chain type: {chain_type}, using default")
                chain_type = ChainType.GRAPH_ENHANCED_QA

        # ì„¤ì • ìƒì„±
        config = config or QAChainConfig(chain_type=chain_type)

        logger.info(f"ğŸ—ï¸ Building QA chain: {chain_type.value}")

        # 1. ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
        retriever = self._get_or_create_retriever(embedding_model, config)

        # 2. LLM ì´ˆê¸°í™”
        llm_model = llm or self._get_or_create_llm(config)

        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        prompt_template = self._get_or_create_prompt_template(config)

        # 4. ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
        memory = None
        if config.enable_memory:
            memory = self._get_or_create_memory(config)

        # 5. ì¿¼ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™” (ì„ íƒì )
        query_analyzer = self._get_or_create_query_analyzer()

        # 6. QA ì²´ì¸ ìƒì„±
        qa_chain = GraphRAGQAChain(
            retriever=retriever,
            llm=llm_model,
            prompt_template=prompt_template,
            memory=memory,
            query_analyzer=query_analyzer,
            config=config,
            **kwargs,
        )

        logger.info(f"âœ… QA chain created successfully: {chain_type.value}")
        return qa_chain

    def _get_or_create_retriever(
        self, embedding_model: str, config: QAChainConfig
    ) -> BaseRetriever:
        """ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ë˜ëŠ” ì¡°íšŒ"""

        if self._retriever is None:
            logger.info("ğŸ“¥ Creating GraphRAG retriever...")

            self._retriever = create_graphrag_retriever(
                unified_graph_path=str(self.unified_graph_path),
                vector_store_path=str(self.vector_store_path),
                embedding_model=embedding_model,
                max_docs=config.max_docs_for_context,
                min_relevance_score=config.min_relevance_score,
                enable_caching=True,
            )

            logger.info("âœ… GraphRAG retriever created")

        return self._retriever

    def _get_or_create_llm(self, config: QAChainConfig) -> BaseLanguageModel:
        """LLM ìƒì„± ë˜ëŠ” ì¡°íšŒ - YAML ì„¤ì • ì™„ì „ í˜¸í™˜"""

        if self._llm is None:
            logger.info("ğŸ¤– Creating LLM with YAML config compatibility...")

            if not self.config_manager:
                raise ValueError("config_manager is required for LLM creation")

            try:
                # YAML ì„¤ì •ìœ¼ë¡œë¶€í„° LLM ì–´ëŒ‘í„° ìƒì„±
                from .langchain_llm_adapter import create_llm_adapter, AdapterMode

                # ì„¤ì •ì— ë”°ë¥¸ ì–´ëŒ‘í„° ëª¨ë“œ ê²°ì •
                adapter_mode = (
                    AdapterMode.CACHED if config.enable_memory else AdapterMode.DIRECT
                )

                self._llm = create_llm_adapter(
                    config_manager=self.config_manager,
                    temperature=config.temperature,
                    max_tokens=config.max_answer_tokens,
                    mode=adapter_mode,
                    enable_caching=True,
                    max_retries=config.max_retries,
                )

                # LLM ì •ë³´ ë¡œê¹…
                model_info = self._llm.get_model_info()
                logger.info("âœ… LLM adapter created from YAML config")
                logger.info(f"   Provider: {model_info.get('model_path', 'unknown')}")
                logger.info(f"   Mode: {model_info.get('mode', 'unknown')}")
                logger.info(
                    f"   Temperature: {model_info.get('temperature', 'unknown')}"
                )

            except ImportError as e:
                logger.error(f"âŒ Failed to import LLM adapter: {e}")
                raise ImportError(
                    "langchain_llm_adapter is required. "
                    "Make sure langchain_llm_adapter.py is in the same directory."
                )
            except Exception as e:
                logger.error(f"âŒ Failed to create LLM adapter: {e}")
                # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                logger.info("ğŸ”„ Falling back to direct LocalLLMManager...")

                llm_config = self.config_manager.get_llm_config()
                if llm_config.get("provider") == "huggingface_local":
                    # ì§ì ‘ LocalLLMManager ì‚¬ìš© (LangChain í˜¸í™˜ì„± ì—†ìŒ)
                    from ..graphrag_pipeline import LocalLLMManager
                    from .langchain_llm_adapter import create_llm_adapter_from_manager

                    llm_manager = LocalLLMManager(llm_config)
                    self._llm = create_llm_adapter_from_manager(
                        llm_manager=llm_manager,
                        temperature=config.temperature,
                        max_tokens=config.max_answer_tokens,
                    )

                    logger.info("âš ï¸ Using fallback LLM adapter")
                else:
                    raise NotImplementedError(
                        f"Provider '{llm_config.get('provider')}' not implemented yet. "
                        f"Currently supported: huggingface_local"
                    )

        return self._llm

    def _get_or_create_prompt_template(
        self, config: QAChainConfig
    ) -> BasePromptTemplate:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ë˜ëŠ” ì¡°íšŒ - ê°„ë‹¨í•œ í•´ê²°ì±…"""

        if self._prompt_templates is None:
            logger.info("ğŸ“ Creating prompt templates...")

            try:
                # config ì—†ì´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
                self._prompt_templates = GraphRAGPromptTemplates()
                logger.info("âœ… Prompt templates created")

            except Exception as e:
                logger.warning(f"âš ï¸ GraphRAGPromptTemplates failed: {e}")

                # ì§ì ‘ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
                from langchain_core.prompts import PromptTemplate

                template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

    ì»¨í…ìŠ¤íŠ¸: {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€:"""

                return PromptTemplate(
                    template=template, input_variables=["context", "question"]
                )

        # í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œë„
        try:
            return self._prompt_templates.create_langchain_prompt()
        except:
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            from langchain_core.prompts import PromptTemplate

            template = """ì»¨í…ìŠ¤íŠ¸: {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€:"""

            return PromptTemplate(
                template=template, input_variables=["context", "question"]
            )

    def _get_or_create_memory(self, config: QAChainConfig) -> BaseMemory:
        """ë©”ëª¨ë¦¬ ìƒì„± ë˜ëŠ” ì¡°íšŒ"""

        if self._memory_manager is None:
            logger.info("ğŸ§  Creating memory manager...")

            # GraphRAG ë©”ëª¨ë¦¬ ê´€ë¦¬ìë¥¼ LangChain ë©”ëª¨ë¦¬ë¡œ ë³€í™˜
            memory_config = {
                "memory_type": config.memory_type,
                "max_token_limit": config.max_memory_tokens,
                "return_messages": True,
            }

            if config.memory_type == "summary_buffer":
                # LLMì´ í•„ìš”í•œ ê²½ìš° ì¶”í›„ ì„¤ì •
                self._memory_manager = ConversationBufferMemory(
                    memory_key="chat_history", return_messages=True, output_key="answer"
                )
            else:
                self._memory_manager = ConversationBufferMemory(
                    memory_key="chat_history", return_messages=True, output_key="answer"
                )

            logger.info("âœ… Memory manager created")

        return self._memory_manager

    def _get_or_create_query_analyzer(self) -> Optional[QueryAnalyzer]:
        """ì¿¼ë¦¬ ë¶„ì„ê¸° ìƒì„± ë˜ëŠ” ì¡°íšŒ"""

        if self._query_analyzer is None:
            try:
                logger.info("ğŸ“Š Creating query analyzer...")
                self._query_analyzer = QueryAnalyzer()
                logger.info("âœ… Query analyzer created")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to create query analyzer: {e}")
                self._query_analyzer = None

        return self._query_analyzer

    def create_conversational_chain(
        self,
        llm: Optional[BaseLanguageModel] = None,
        session_id: str = "default",
        **kwargs,
    ) -> GraphRAGQAChain:
        """ëŒ€í™”í˜• QA ì²´ì¸ ìƒì„± (í¸ì˜ ë©”ì„œë“œ)"""

        config = QAChainConfig(
            chain_type=ChainType.CONVERSATIONAL_QA,
            enable_memory=True,
            memory_type="summary_buffer",
            **kwargs,
        )

        return self.create_chain(
            chain_type=ChainType.CONVERSATIONAL_QA, llm=llm, config=config
        )

    def create_basic_chain(
        self, llm: Optional[BaseLanguageModel] = None, **kwargs
    ) -> GraphRAGQAChain:
        """ê¸°ë³¸ QA ì²´ì¸ ìƒì„± (í¸ì˜ ë©”ì„œë“œ)"""

        config = QAChainConfig(
            chain_type=ChainType.BASIC_QA, enable_memory=False, **kwargs
        )

        return self.create_chain(chain_type=ChainType.BASIC_QA, llm=llm, config=config)

    def get_chain_info(self) -> Dict[str, Any]:
        """ì²´ì¸ ë¹Œë” ì •ë³´ ë°˜í™˜ - í™•ì¥ëœ ë²„ì „"""

        info = {
            "unified_graph_path": str(self.unified_graph_path),
            "vector_store_path": str(self.vector_store_path),
            "config_manager_available": self.config_manager is not None,
            "components_loaded": {
                "retriever": self._retriever is not None,
                "llm": self._llm is not None,
                "memory": self._memory_manager is not None,
                "query_analyzer": self._query_analyzer is not None,
                "prompt_templates": self._prompt_templates is not None,
            },
            "available_chain_types": [ct.value for ct in ChainType],
            "langchain_available": _langchain_available,
        }

        # LLM ìƒì„¸ ì •ë³´ ì¶”ê°€
        if self._llm:
            try:
                llm_info = self._llm.get_model_info()
                info["llm_info"] = llm_info

                if hasattr(self._llm, "get_usage_stats"):
                    info["llm_usage_stats"] = self._llm.get_usage_stats()
            except Exception as e:
                logger.warning(f"âš ï¸ Could not get LLM info: {e}")

        # ì„¤ì • ê´€ë¦¬ì ì •ë³´ ì¶”ê°€
        if self.config_manager:
            try:
                info["yaml_config"] = {
                    "llm_provider": self.config_manager.config.llm.provider,
                    "vector_store_type": self.config_manager.config.vector_store.store_type,
                    "embedding_model": self.config_manager.config.embeddings.sentence_transformers.model_name,
                }
            except Exception as e:
                logger.warning(f"âš ï¸ Could not get config info: {e}")

        return info

    def integrate_with_pipeline(self, pipeline: "GraphRAGPipeline") -> GraphRAGQAChain:
        """GraphRAG Pipelineê³¼ í†µí•©í•˜ì—¬ QA ì²´ì¸ ìƒì„±"""

        logger.info("ğŸ”— Integrating QA Chain with GraphRAG Pipeline...")

        # Pipelineì˜ ì„¤ì • ê´€ë¦¬ì ì‚¬ìš©
        self.config_manager = pipeline.config_manager

        # Pipelineì˜ ì»´í¬ë„ŒíŠ¸ë“¤ ì¬ì‚¬ìš©
        if hasattr(pipeline, "vector_store") and pipeline.vector_store:
            logger.info("ğŸ“š Reusing pipeline's vector store...")
            # ë²¡í„° ì €ì¥ì†Œê°€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©

        if hasattr(pipeline, "query_analyzer") and pipeline.query_analyzer:
            logger.info("ğŸ” Reusing pipeline's query analyzer...")
            self._query_analyzer = pipeline.query_analyzer

        # ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ QA ì²´ì¸ ìƒì„±
        config = QAChainConfig(
            chain_type=ChainType.GRAPH_ENHANCED_QA,
            enable_memory=True,
            memory_type="summary_buffer",
            max_docs_for_context=10,
            temperature=0.1,
            max_retries=3,
            fallback_to_simple=True,
        )

        qa_chain = self.create_chain(config=config)

        logger.info("âœ… QA Chain integrated with Pipeline successfully")
        return qa_chain

    def create_optimized_chain_for_pipeline(
        self, config_manager: "GraphRAGConfigManager"
    ) -> GraphRAGQAChain:
        """Pipeline ì „ìš© ìµœì í™”ëœ ì²´ì¸ ìƒì„±"""

        self.config_manager = config_manager

        # YAML ì„¤ì •ì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        try:
            llm_config = config_manager.get_llm_config()
            vector_config = config_manager.get_vector_store_config()

            # ì„¤ì • ê¸°ë°˜ ìµœì í™”
            config = QAChainConfig(
                chain_type=ChainType.GRAPH_ENHANCED_QA,
                enable_memory=True,
                max_docs_for_context=min(15, vector_config.get("batch_size", 10)),
                temperature=llm_config.get("temperature", 0.1),
                max_answer_tokens=llm_config.get("max_new_tokens", 1000),
                max_retries=3,
                streaming=False,  # YAML ì„¤ì •ì—ì„œ do_sample: falseì´ë¯€ë¡œ
                fallback_to_simple=True,
            )

            logger.info("ğŸ¯ Creating optimized chain from YAML config...")
            logger.info(f"   LLM Provider: {llm_config.get('provider')}")
            logger.info(f"   Vector Store: {vector_config.get('store_type')}")
            logger.info(f"   Max Docs: {config.max_docs_for_context}")
            logger.info(f"   Temperature: {config.temperature}")

            return self.create_chain(config=config)

        except Exception as e:
            logger.error(f"âŒ Failed to create optimized chain: {e}")
            logger.info("ğŸ”„ Falling back to default configuration...")

            # í´ë°±: ê¸°ë³¸ ì„¤ì •
            return self.create_chain()

    def health_check(self) -> Dict[str, Any]:
        """QA ì²´ì¸ ë¹Œë” ìƒíƒœ í™•ì¸"""

        health_status = {
            "overall_status": "unknown",
            "components": {},
            "dependencies": {
                "langchain": _langchain_available,
                "unified_graph_exists": self.unified_graph_path.exists(),
                "vector_store_exists": self.vector_store_path.exists(),
            },
            "config_manager": self.config_manager is not None,
            "errors": [],
        }

        try:
            # ì»´í¬ë„ŒíŠ¸ë³„ ìƒíƒœ í™•ì¸
            if self._llm:
                try:
                    llm_health = self._llm.health_check()
                    health_status["components"]["llm"] = llm_health
                except Exception as e:
                    health_status["components"]["llm"] = {
                        "status": "error",
                        "error": str(e),
                    }
                    health_status["errors"].append(f"LLM health check failed: {e}")

            if self._retriever:
                health_status["components"]["retriever"] = {"status": "loaded"}

            if self._query_analyzer:
                health_status["components"]["query_analyzer"] = {"status": "loaded"}

            # ì „ì²´ ìƒíƒœ ê²°ì •
            error_count = len(health_status["errors"])
            if error_count == 0:
                health_status["overall_status"] = "healthy"
            elif error_count <= 2:
                health_status["overall_status"] = "degraded"
            else:
                health_status["overall_status"] = "unhealthy"

        except Exception as e:
            health_status["overall_status"] = "error"
            health_status["errors"].append(f"Health check failed: {e}")

        return health_status


# í¸ì˜ í•¨ìˆ˜ë“¤ - Pipeline í†µí•© ì§€ì›
def create_qa_chain(
    unified_graph_path: str,
    vector_store_path: str,
    chain_type: Union[ChainType, str] = ChainType.GRAPH_ENHANCED_QA,
    config_manager: Optional[object] = None,
    **kwargs,
) -> GraphRAGQAChain:
    """QA ì²´ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜ - YAML ì„¤ì • ì§€ì›"""

    builder = QAChainBuilder(
        unified_graph_path=unified_graph_path,
        vector_store_path=vector_store_path,
        config_manager=config_manager,
    )

    return builder.create_chain(chain_type=chain_type, **kwargs)


def create_conversational_qa_chain(
    unified_graph_path: str,
    vector_store_path: str,
    config_manager: Optional[object] = None,
    **kwargs,
) -> GraphRAGQAChain:
    """ëŒ€í™”í˜• QA ì²´ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""

    builder = QAChainBuilder(
        unified_graph_path=unified_graph_path,
        vector_store_path=vector_store_path,
        config_manager=config_manager,
    )

    return builder.create_conversational_chain(**kwargs)


def create_qa_chain_from_pipeline(pipeline: "GraphRAGPipeline") -> GraphRAGQAChain:
    """GraphRAG Pipelineìœ¼ë¡œë¶€í„° QA ì²´ì¸ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)"""

    if not hasattr(pipeline, "config_manager") or not pipeline.config_manager:
        raise ValueError("Pipeline must have a valid config_manager")

    config = pipeline.config_manager.config

    builder = QAChainBuilder(
        unified_graph_path=config.graph.unified_graph_path,
        vector_store_path=config.graph.vector_store_path,
        config_manager=pipeline.config_manager,
    )

    # Pipelineê³¼ í†µí•©
    qa_chain = builder.integrate_with_pipeline(pipeline)

    logger.info("ğŸ”— QA Chain created from GraphRAG Pipeline")
    return qa_chain


def create_optimized_qa_chain(
    config_manager: "GraphRAGConfigManager",
) -> GraphRAGQAChain:
    """YAML ì„¤ì • ìµœì í™”ëœ QA ì²´ì¸ ìƒì„±"""

    config = config_manager.config

    builder = QAChainBuilder(
        unified_graph_path=config.graph.unified_graph_path,
        vector_store_path=config.graph.vector_store_path,
        config_manager=config_manager,
    )

    return builder.create_optimized_chain_for_pipeline(config_manager)


def replace_pipeline_llm_with_qa_chain(
    pipeline: "GraphRAGPipeline",
) -> "GraphRAGPipeline":
    """Pipelineì˜ LLM í˜¸ì¶œì„ QA Chainìœ¼ë¡œ êµì²´ (ì„±ëŠ¥ ê°œì„ )"""

    logger.info("ğŸ”„ Replacing Pipeline LLM with optimized QA Chain...")

    # QA ì²´ì¸ ìƒì„±
    qa_chain = create_qa_chain_from_pipeline(pipeline)

    # Pipelineì˜ ask ë©”ì„œë“œë¥¼ QA ì²´ì¸ìœ¼ë¡œ êµì²´
    original_ask = pipeline.ask

    def optimized_ask(query: str, return_context: bool = False):
        """ìµœì í™”ëœ ask ë©”ì„œë“œ (QA Chain ì‚¬ìš©)"""

        try:
            logger.info(f"ğŸš€ Processing query with optimized QA Chain: {query[:50]}...")

            # QA ì²´ì¸ìœ¼ë¡œ ì²˜ë¦¬
            result = qa_chain._call({"question": query})

            if return_context:
                # QAResult í˜•íƒœë¡œ ë³€í™˜
                from ..graphrag_pipeline import QAResult

                return QAResult(
                    query=query,
                    answer=result.get("answer", ""),
                    subgraph_result=None,  # QA Chainì—ì„œëŠ” ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŒ
                    serialized_context=None,
                    query_analysis=result.get("query_analysis"),
                    processing_time=0.0,  # QA Chainì—ì„œ ì¸¡ì •ë¨
                    confidence_score=result.get("query_analysis", {}).get(
                        "confidence", 0.0
                    ),
                    source_nodes=[
                        doc.get("metadata", {}).get("node_id", "")
                        for doc in result.get("source_documents", [])
                    ],
                )
            else:
                return result.get("answer", "")

        except Exception as e:
            logger.error(f"âŒ QA Chain failed: {e}")
            logger.info("ğŸ”„ Falling back to original Pipeline method...")

            # í´ë°±: ì›ë˜ ë©”ì„œë“œ ì‚¬ìš©
            return original_ask(query, return_context)

    # ë©”ì„œë“œ êµì²´
    pipeline.ask = optimized_ask
    pipeline._qa_chain = qa_chain  # ì°¸ì¡° ë³´ê´€

    logger.info("âœ… Pipeline LLM replaced with QA Chain successfully")
    logger.info("ğŸ’¡ Use pipeline.ask() as usual - now with LangChain optimization!")

    return pipeline


# ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
def validate_qa_chain_integration(
    config_manager: "GraphRAGConfigManager",
) -> Dict[str, Any]:
    """QA Chain í†µí•© ê²€ì¦ - HuggingFace ID ì§€ì›"""

    validation_result = {
        "status": "unknown",
        "checks": {},
        "recommendations": [],
        "errors": [],
    }

    try:
        # 1. ì„¤ì • ê²€ì¦
        validation_result["checks"]["config_valid"] = config_manager is not None

        if config_manager:
            config = config_manager.config

            # 2. ê²½ë¡œ ê²€ì¦
            graph_path = Path(config.graph.unified_graph_path)
            vector_path = Path(config.graph.vector_store_path)

            validation_result["checks"]["graph_exists"] = graph_path.exists()
            validation_result["checks"]["vector_store_exists"] = vector_path.exists()

            if not graph_path.exists():
                validation_result["errors"].append(
                    f"Unified graph not found: {graph_path}"
                )
                validation_result["recommendations"].append(
                    "Run unified graph builder first"
                )

            if not vector_path.exists():
                validation_result["errors"].append(
                    f"Vector store not found: {vector_path}"
                )
                validation_result["recommendations"].append(
                    "Run embedding generation first"
                )

            # 3. LLM ì„¤ì • ê²€ì¦ (ìˆ˜ì •ëœ ë¶€ë¶„)
            try:
                llm_config = config_manager.get_llm_config()
                validation_result["checks"]["llm_config_valid"] = True
                validation_result["checks"]["llm_provider"] = llm_config.get("provider")

                if llm_config.get("provider") == "huggingface_local":
                    model_path = llm_config.get("model_path")

                    # âœ… HuggingFace ID vs ë¡œì»¬ ê²½ë¡œ êµ¬ë¶„
                    if model_path:
                        # HuggingFace ID íŒ¨í„´ (org/model-name)
                        if "/" in model_path and not model_path.startswith("/"):
                            validation_result["checks"][
                                "model_source"
                            ] = "huggingface_hub"
                            validation_result["checks"]["model_id"] = model_path
                            logger.info(f"âœ… Using HuggingFace model ID: {model_path}")
                        else:
                            # ë¡œì»¬ ê²½ë¡œ
                            validation_result["checks"]["model_source"] = "local_path"
                            if not Path(model_path).exists():
                                validation_result["errors"].append(
                                    f"Local model path not found: {model_path}"
                                )
                                validation_result["recommendations"].append(
                                    f"Download model to {model_path} or use HuggingFace ID"
                                )
                            else:
                                validation_result["checks"]["local_model_exists"] = True

            except Exception as e:
                validation_result["checks"]["llm_config_valid"] = False
                validation_result["errors"].append(f"LLM config error: {e}")

            # 4. ì˜ì¡´ì„± ê²€ì¦ (ë” ì •í™•í•œ ì²´í¬)
            try:
                import langchain_core
                import langchain

                validation_result["checks"]["langchain_available"] = True
                validation_result["checks"]["langchain_version"] = langchain.__version__
            except ImportError:
                validation_result["checks"]["langchain_available"] = False
                validation_result["errors"].append("LangChain not available")
                validation_result["recommendations"].append(
                    "Install LangChain: pip install langchain langchain-core"
                )

        # 5. ì „ì²´ ìƒíƒœ ê²°ì • (ìˆ˜ì •ëœ ë¡œì§)
        error_count = len(validation_result["errors"])

        # ì¤‘ìš”í•œ ì—ëŸ¬ì™€ ê²½ê³  êµ¬ë¶„
        critical_errors = []
        for error in validation_result["errors"]:
            if "not found" in error and "graph" in error:
                critical_errors.append(error)
            elif "LangChain not available" in error:
                critical_errors.append(error)

        if len(critical_errors) == 0:
            validation_result["status"] = "ready"
        elif len(critical_errors) <= 1:
            validation_result["status"] = "partial"
        else:
            validation_result["status"] = "not_ready"

        # ìƒíƒœë³„ ë©”ì‹œì§€ ì¶”ê°€
        if validation_result["status"] == "ready":
            validation_result["message"] = (
                "All components ready for QA Chain integration"
            )
        elif validation_result["status"] == "partial":
            validation_result["message"] = "QA Chain can work with some limitations"
        else:
            validation_result["message"] = "Critical components missing"

    except Exception as e:
        validation_result["status"] = "error"
        validation_result["errors"].append(f"Validation failed: {e}")

    return validation_result


def print_qa_chain_integration_guide():
    """QA Chain í†µí•© ê°€ì´ë“œ ì¶œë ¥"""

    guide = """
ğŸ”— GraphRAG QA Chain Integration Guide
=====================================

1. ê¸°ë³¸ ì‚¬ìš©ë²•:
   ```python
   from graphrag.langchain import create_qa_chain_from_pipeline
   
   # Pipelineì—ì„œ QA Chain ìƒì„±
   qa_chain = create_qa_chain_from_pipeline(pipeline)
   
   # ì§ˆë¬¸í•˜ê¸°
   result = qa_chain.invoke({"question": "your question"})
   print(result["answer"])
   ```

2. Pipeline ìµœì í™”:
   ```python
   from graphrag.langchain import replace_pipeline_llm_with_qa_chain
   
   # Pipelineì˜ ask() ë©”ì„œë“œë¥¼ QA Chainìœ¼ë¡œ êµì²´
   optimized_pipeline = replace_pipeline_llm_with_qa_chain(pipeline)
   
   # ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš© (ë‚´ë¶€ì ìœ¼ë¡œ LangChain ìµœì í™” ì ìš©)
   answer = optimized_pipeline.ask("your question")
   ```

3. ì„¤ì • ê²€ì¦:
   ```python
   from graphrag.langchain import validate_qa_chain_integration
   
   validation = validate_qa_chain_integration(config_manager)
   print(f"Status: {validation['status']}")
   ```

4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§:
   ```python
   # LLM ì‚¬ìš© í†µê³„
   stats = qa_chain._llm.get_usage_stats()
   print(f"Total calls: {stats['total_calls']}")
   print(f"Cache hit ratio: {stats['cache_hit_ratio']:.2%}")
   ```

âš¡ Performance Benefits:
- ğŸš€ LangChain ì²´ì¸ ìµœì í™”
- ğŸ’¾ ìë™ ì‘ë‹µ ìºì‹±
- ğŸ”„ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„
- ğŸ“Š ì‚¬ìš©ëŸ‰ í†µê³„ ì¶”ì 
- ğŸ§  ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
"""

    print(guide)


def main():
    """QA Chain Builder í…ŒìŠ¤íŠ¸"""

    if not _langchain_available:
        print("âŒ LangChain not available for testing")
        return

    print("ğŸ§ª Testing QA Chain Builder...")

    try:
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)
        from pathlib import Path

        base_dir = Path(__file__).parent.parent.parent.parent
        unified_graph_path = (
            base_dir
            / "data"
            / "processed"
            / "graphs"
            / "unified"
            / "unified_knowledge_graph.json"
        )
        vector_store_path = base_dir / "data" / "processed" / "vector_store"

        # QA ì²´ì¸ ë¹Œë” ìƒì„±
        builder = QAChainBuilder(
            unified_graph_path=str(unified_graph_path),
            vector_store_path=str(vector_store_path),
        )

        # ë¹Œë” ì •ë³´ ì¶œë ¥
        info = builder.get_chain_info()
        print(f"ğŸ“Š Builder info:")
        print(f"   Graph path: {info['unified_graph_path']}")
        print(f"   Vector store: {info['vector_store_path']}")
        print(f"   Available chain types: {info['available_chain_types']}")

        # ê¸°ë³¸ QA ì²´ì¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print(f"\nğŸ”§ Testing basic QA chain creation...")

        # ì‹¤ì œ LLM ì—†ì´ í…ŒìŠ¤íŠ¸ (ëª¨í‚¹)
        class MockLLM(BaseLanguageModel):
            def _generate(self, messages, stop=None, run_manager=None, **kwargs):
                return "í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤."

            def _llm_type(self):
                return "mock_llm"

        try:
            # ì²´ì¸ ì„¤ì •
            config = QAChainConfig(
                chain_type=ChainType.BASIC_QA,
                enable_memory=False,
                max_docs_for_context=5,
                verbose=True,
            )

            # ê¸°ë³¸ ì²´ì¸ ìƒì„± (ì‹¤ì œë¡œëŠ” ì»´í¬ë„ŒíŠ¸ë“¤ì´ í•„ìš”)
            print(f"âœ… QA chain configuration created")
            print(f"   Chain type: {config.chain_type.value}")
            print(f"   Memory enabled: {config.enable_memory}")
            print(f"   Max docs: {config.max_docs_for_context}")

        except Exception as e:
            print(f"âš ï¸ Chain creation test skipped: {e}")

        # ì‚¬ìš© ê°€ëŠ¥í•œ ì²´ì¸ íƒ€ì…ë“¤ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ“‹ Available chain types:")
        for chain_type in ChainType:
            print(f"   â€¢ {chain_type.value}: {chain_type.name}")

        print(f"\nâœ… QA Chain Builder test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
