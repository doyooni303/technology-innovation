"""
GraphRAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ëª¨ë“ˆ
Prompt Templates for GraphRAG System

ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì œê³µ
- ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±
- í•œêµ­ì–´/ì˜ì–´/í˜¼ìš© ì–¸ì–´ ì§€ì›
- ë³µì¡ë„ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
- LangChain PromptTemplate ì™„ì „ í˜¸í™˜
"""

import logging
from enum import Enum
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass
import warnings

# LangChain imports
try:
    from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
    from langchain_core.prompts.base import BasePromptTemplate
    from langchain_core.messages import SystemMessage, HumanMessage

    _langchain_available = True
except ImportError:
    _langchain_available = False
    warnings.warn(
        "LangChain not available. Install with: pip install langchain langchain-core"
    )

    # Placeholder classes
    class PromptTemplate:
        def __init__(self, *args, **kwargs):
            pass

    class ChatPromptTemplate:
        def __init__(self, *args, **kwargs):
            pass

    class BasePromptTemplate:
        def __init__(self, *args, **kwargs):
            pass


# GraphRAG imports
try:
    from ..query_analyzer import QueryType, QueryComplexity, QueryAnalysisResult
except ImportError as e:
    warnings.warn(f"GraphRAG QueryAnalyzer not available: {e}")

    # Placeholder enums
    class QueryType(Enum):
        CITATION_ANALYSIS = "citation_analysis"
        AUTHOR_ANALYSIS = "author_analysis"
        KEYWORD_ANALYSIS = "keyword_analysis"
        TREND_ANALYSIS = "trend_analysis"
        COLLABORATION_ANALYSIS = "collaboration_analysis"
        COMPREHENSIVE_ANALYSIS = "comprehensive_analysis"
        FACTUAL_LOOKUP = "factual_lookup"

    class QueryComplexity(Enum):
        SIMPLE = "simple"
        MEDIUM = "medium"
        COMPLEX = "complex"
        EXPLORATORY = "exploratory"


# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class PromptConfig:
    """í”„ë¡¬í”„íŠ¸ ì„¤ì • í´ë˜ìŠ¤"""

    language: str = "mixed"  # "ko", "en", "mixed"
    style: str = "academic"  # "academic", "conversational", "technical"
    include_metadata: bool = True
    include_confidence: bool = True
    max_context_length: int = 8000
    citation_style: str = "detailed"  # "minimal", "detailed", "academic"


class PromptStyle(Enum):
    """í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼"""

    ACADEMIC = "academic"
    CONVERSATIONAL = "conversational"
    TECHNICAL = "technical"
    CONCISE = "concise"


class GraphRAGPromptTemplates:
    """GraphRAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ì"""

    def __init__(self, config: Optional[PromptConfig] = None):
        """
        Args:
            config: í”„ë¡¬í”„íŠ¸ ì„¤ì •
        """
        self.config = config or PromptConfig()

        # ì–¸ì–´ë³„ ê¸°ë³¸ ì§€ì‹œë¬¸
        self._load_base_instructions()

        # ì¿¼ë¦¬ íƒ€ì…ë³„ í…œí”Œë¦¿
        self._load_query_type_templates()

        # ë³µì¡ë„ë³„ í…œí”Œë¦¿
        self._load_complexity_templates()

        logger.info("âœ… GraphRAGPromptTemplates initialized")
        logger.info(f"   ğŸŒ Language: {self.config.language}")
        logger.info(f"   ğŸ¨ Style: {self.config.style}")

    def _load_base_instructions(self) -> None:
        """ê¸°ë³¸ ì§€ì‹œë¬¸ ë¡œë“œ"""
        self.base_instructions = {
            "ko": {
                "system_role": "ë‹¹ì‹ ì€ ê³¼í•™ ë…¼ë¬¸ê³¼ ì—°êµ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                "task_description": "ì œê³µëœ ì§€ì‹ ê·¸ë˜í”„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
                "context_explanation": "ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë…¼ë¬¸, ì €ì, í‚¤ì›Œë“œ, ì €ë„ ì •ë³´ê°€ í¬í•¨ëœ ì§€ì‹ ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:",
                "answer_guidelines": [
                    "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”",
                    "êµ¬ì²´ì ì¸ ë…¼ë¬¸ ì œëª©, ì €ìëª…, ì—°ë„ ë“±ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”",
                    "ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ì•Œ ìˆ˜ ì—†ë‹¤ê³  ëª…ì‹œí•˜ì„¸ìš”",
                    "ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”",
                ],
                "citation_instruction": "ë‹µë³€ì—ëŠ” ê´€ë ¨ ë…¼ë¬¸ì´ë‚˜ ì €ìë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.",
            },
            "en": {
                "system_role": "You are an expert AI assistant specialized in analyzing scientific papers and research data.",
                "task_description": "Please provide accurate and helpful answers to user questions based on the provided knowledge graph information.",
                "context_explanation": "The following is knowledge graph context containing papers, authors, keywords, and journal information related to the question:",
                "answer_guidelines": [
                    "Base your answer on the provided context information",
                    "Include specific paper titles, author names, years, etc. in your response",
                    "If information is uncertain, explicitly state that it's unknown rather than guessing",
                    "Prioritize highly relevant information",
                ],
                "citation_instruction": "Please specifically mention relevant papers or authors in your answer.",
            },
            "mixed": {
                "system_role": "You are an expert AI assistant specialized in analyzing scientific papers and research data. ë‹¹ì‹ ì€ ê³¼í•™ ë…¼ë¬¸ê³¼ ì—°êµ¬ ë°ì´í„° ë¶„ì„ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                "task_description": "Please provide accurate and helpful answers based on the knowledge graph information. ì§€ì‹ ê·¸ë˜í”„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
                "context_explanation": "ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì§€ì‹ ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤ (The following is knowledge graph context related to the question):",
                "answer_guidelines": [
                    "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš” (Base your answer on the provided context)",
                    "êµ¬ì²´ì ì¸ ë…¼ë¬¸ ì œëª©, ì €ìëª…ì„ í¬í•¨í•˜ì„¸ìš” (Include specific paper titles and author names)",
                    "ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ëª…ì‹œí•˜ì„¸ìš” (Explicitly state uncertain information)",
                    "ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ìš°ì„ í•˜ì„¸ìš” (Prioritize highly relevant information)",
                ],
                "citation_instruction": "ê´€ë ¨ ë…¼ë¬¸ì´ë‚˜ ì €ìë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš” (Please mention relevant papers or authors specifically).",
            },
        }

    def _load_query_type_templates(self) -> None:
        """ì¿¼ë¦¬ íƒ€ì…ë³„ í…œí”Œë¦¿ ë¡œë“œ"""
        self.query_type_templates = {
            QueryType.CITATION_ANALYSIS: {
                "ko": {
                    "specific_instruction": "ì¸ìš© ê´€ê³„ì™€ ì˜í–¥ë ¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    "focus_areas": [
                        "í”¼ì¸ìš© ìˆ˜",
                        "ì¸ìš© íŒ¨í„´",
                        "ì˜í–¥ë ¥ ìˆëŠ” ë…¼ë¬¸",
                        "ì—°êµ¬ ì˜í–¥ë„",
                    ],
                    "output_format": "ì¸ìš© ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì œì‹œí•´ì£¼ì„¸ìš”.",
                },
                "en": {
                    "specific_instruction": "Focus on citation relationships and impact analysis.",
                    "focus_areas": [
                        "Citation counts",
                        "Citation patterns",
                        "Influential papers",
                        "Research impact",
                    ],
                    "output_format": "Present citation analysis results with specific metrics.",
                },
            },
            QueryType.AUTHOR_ANALYSIS: {
                "ko": {
                    "specific_instruction": "ì—°êµ¬ìì˜ ì—°êµ¬ ë¶„ì•¼, í˜‘ì—… ë„¤íŠ¸ì›Œí¬, ì—°êµ¬ ìƒì‚°ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    "focus_areas": [
                        "ì£¼ìš” ì—°êµ¬ ë¶„ì•¼",
                        "ê³µë™ ì—°êµ¬ì",
                        "ë…¼ë¬¸ ìˆ˜",
                        "ì—°êµ¬ í™œë™ ê¸°ê°„",
                    ],
                    "output_format": "ì—°êµ¬ìë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ìƒì„¸í•œ í”„ë¡œí•„ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
                },
                "en": {
                    "specific_instruction": "Analyze researchers' fields, collaboration networks, and productivity.",
                    "focus_areas": [
                        "Research areas",
                        "Collaborators",
                        "Publication count",
                        "Research period",
                    ],
                    "output_format": "Provide detailed profiles for each researcher.",
                },
            },
            QueryType.KEYWORD_ANALYSIS: {
                "ko": {
                    "specific_instruction": "í‚¤ì›Œë“œì™€ ì—°êµ¬ ì£¼ì œì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    "focus_areas": [
                        "í‚¤ì›Œë“œ ë¹ˆë„",
                        "ê´€ë ¨ ì£¼ì œ",
                        "ì—°êµ¬ íŠ¸ë Œë“œ",
                        "ì£¼ì œ ì—°ê´€ì„±",
                    ],
                    "output_format": "í‚¤ì›Œë“œë³„ ì‚¬ìš© ë¹ˆë„ì™€ ê´€ë ¨ ì—°êµ¬ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                },
                "en": {
                    "specific_instruction": "Analyze relationships between keywords and research topics.",
                    "focus_areas": [
                        "Keyword frequency",
                        "Related topics",
                        "Research trends",
                        "Topic associations",
                    ],
                    "output_format": "Organize keyword frequencies and related research.",
                },
            },
            QueryType.TREND_ANALYSIS: {
                "ko": {
                    "specific_instruction": "ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ì—°êµ¬ ë™í–¥ê³¼ ë³€í™”ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    "focus_areas": [
                        "ì—°ë„ë³„ ì—°êµ¬ëŸ‰",
                        "ì£¼ì œ ë³€í™”",
                        "ìƒˆë¡œìš´ íŠ¸ë Œë“œ",
                        "ê¸°ìˆ  ë°œì „",
                    ],
                    "output_format": "ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ íŠ¸ë Œë“œ ë³€í™”ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ì„¸ìš”.",
                },
                "en": {
                    "specific_instruction": "Analyze research trends and changes over time.",
                    "focus_areas": [
                        "Research volume by year",
                        "Topic evolution",
                        "Emerging trends",
                        "Technology development",
                    ],
                    "output_format": "Present trend changes clearly in chronological order.",
                },
            },
            QueryType.COLLABORATION_ANALYSIS: {
                "ko": {
                    "specific_instruction": "ì—°êµ¬ì ê°„ í˜‘ì—… ê´€ê³„ì™€ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    "focus_areas": [
                        "ê³µë™ ì—°êµ¬",
                        "í˜‘ì—… ë¹ˆë„",
                        "ì—°êµ¬ ë„¤íŠ¸ì›Œí¬",
                        "ê¸°ê´€ ê°„ í˜‘ë ¥",
                    ],
                    "output_format": "í˜‘ì—… ê´€ê³„ë¥¼ ë„¤íŠ¸ì›Œí¬ í˜•íƒœë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                },
                "en": {
                    "specific_instruction": "Analyze collaboration relationships and networks among researchers.",
                    "focus_areas": [
                        "Joint research",
                        "Collaboration frequency",
                        "Research networks",
                        "Institutional cooperation",
                    ],
                    "output_format": "Explain collaboration relationships in network format.",
                },
            },
            QueryType.COMPREHENSIVE_ANALYSIS: {
                "ko": {
                    "specific_instruction": "ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                    "focus_areas": [
                        "ì „ì²´ì  ê°œìš”",
                        "ì£¼ìš” ì—°êµ¬ì",
                        "í•µì‹¬ ë…¼ë¬¸",
                        "ì—°êµ¬ ë™í–¥",
                        "í–¥í›„ ë°©í–¥",
                    ],
                    "output_format": "ì„¹ì…˜ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.",
                },
                "en": {
                    "specific_instruction": "Provide comprehensive analysis from multiple perspectives.",
                    "focus_areas": [
                        "Overall overview",
                        "Key researchers",
                        "Core papers",
                        "Research trends",
                        "Future directions",
                    ],
                    "output_format": "Present analysis results systematically by sections.",
                },
            },
            QueryType.FACTUAL_LOOKUP: {
                "ko": {
                    "specific_instruction": "ìš”ì²­ëœ ì •ë³´ë¥¼ ì •í™•í•˜ê³  ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì œê³µí•´ì£¼ì„¸ìš”.",
                    "focus_areas": ["ì •í™•í•œ ì •ë³´", "êµ¬ì²´ì  ë°ì´í„°", "ëª…í™•í•œ ë‹µë³€"],
                    "output_format": "ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
                },
                "en": {
                    "specific_instruction": "Provide the requested information accurately and concisely.",
                    "focus_areas": [
                        "Accurate information",
                        "Specific data",
                        "Clear answers",
                    ],
                    "output_format": "Provide direct answers to the question.",
                },
            },
        }

    def _load_complexity_templates(self) -> None:
        """ë³µì¡ë„ë³„ í…œí”Œë¦¿ ë¡œë“œ"""
        self.complexity_templates = {
            QueryComplexity.SIMPLE: {
                "ko": {
                    "approach": "ê°„ë‹¨ëª…ë£Œí•œ ë‹µë³€",
                    "detail_level": "í•µì‹¬ ì •ë³´ë§Œ í¬í•¨",
                    "structure": "ë‹¨ë‹µí˜• ë˜ëŠ” ì§§ì€ ì„¤ëª…",
                },
                "en": {
                    "approach": "Simple and clear answer",
                    "detail_level": "Include only key information",
                    "structure": "Short answer or brief explanation",
                },
            },
            QueryComplexity.MEDIUM: {
                "ko": {
                    "approach": "ì ì ˆí•œ ìˆ˜ì¤€ì˜ ìƒì„¸ ë¶„ì„",
                    "detail_level": "ì£¼ìš” ë‚´ìš©ê³¼ ë°°ê²½ ì •ë³´ í¬í•¨",
                    "structure": "êµ¬ì¡°í™”ëœ ì„¤ëª… (2-3ê°œ ì„¹ì…˜)",
                },
                "en": {
                    "approach": "Moderately detailed analysis",
                    "detail_level": "Include main content and background",
                    "structure": "Structured explanation (2-3 sections)",
                },
            },
            QueryComplexity.COMPLEX: {
                "ko": {
                    "approach": "ì‹¬ì¸µì ì´ê³  ì¢…í•©ì ì¸ ë¶„ì„",
                    "detail_level": "ë‹¤ì–‘í•œ ê´€ì ê³¼ ì„¸ë¶€ ì •ë³´ í¬í•¨",
                    "structure": "ì²´ê³„ì ì¸ ë‹¤ì¤‘ ì„¹ì…˜ êµ¬ì„±",
                },
                "en": {
                    "approach": "In-depth and comprehensive analysis",
                    "detail_level": "Include multiple perspectives and details",
                    "structure": "Systematic multi-section organization",
                },
            },
            QueryComplexity.EXPLORATORY: {
                "ko": {
                    "approach": "íƒìƒ‰ì ì´ê³  ì°½ì˜ì ì¸ ë¶„ì„",
                    "detail_level": "ìˆ¨ê²¨ì§„ íŒ¨í„´ê³¼ ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ ë°œêµ´",
                    "structure": "ë°œê²¬ì  ì ‘ê·¼ë²•ìœ¼ë¡œ ë‹¤ê°ë„ ë¶„ì„",
                },
                "en": {
                    "approach": "Exploratory and creative analysis",
                    "detail_level": "Discover hidden patterns and new insights",
                    "structure": "Multi-angle analysis with discovery approach",
                },
            },
        }

    def get_base_prompt(
        self, query_analysis: Optional[QueryAnalysisResult] = None
    ) -> str:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        # ì–¸ì–´ ê²°ì •
        language = self._determine_language(query_analysis)
        base = self.base_instructions[language]

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = [
            base["system_role"],
            "",
            base["task_description"],
            "",
            base["context_explanation"],
            "{context}",
            "",
        ]

        # ë‹µë³€ ê°€ì´ë“œë¼ì¸ ì¶”ê°€
        if self.config.include_metadata:
            prompt_parts.append(
                "ë‹µë³€ ê°€ì´ë“œë¼ì¸:"
                if language in ["ko", "mixed"]
                else "Answer Guidelines:"
            )
            for guideline in base["answer_guidelines"]:
                prompt_parts.append(f"- {guideline}")
            prompt_parts.append("")

        # ì¸ìš© ì§€ì‹œë¬¸ ì¶”ê°€
        if self.config.citation_style != "minimal":
            prompt_parts.append(base["citation_instruction"])
            prompt_parts.append("")

        prompt_parts.extend(
            [
                (
                    "ì§ˆë¬¸: {question}"
                    if language in ["ko", "mixed"]
                    else "Question: {question}"
                ),
                "",
                "ë‹µë³€:" if language in ["ko", "mixed"] else "Answer:",
            ]
        )

        return "\n".join(prompt_parts)

    def get_query_specific_prompt(
        self, query_analysis: QueryAnalysisResult, include_base: bool = True
    ) -> str:
        """ì¿¼ë¦¬ íƒ€ì…ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        language = self._determine_language(query_analysis)
        query_type = query_analysis.query_type
        complexity = query_analysis.complexity

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        if include_base:
            prompt_parts = [self.get_base_prompt(query_analysis)]
        else:
            prompt_parts = []

        # ì¿¼ë¦¬ íƒ€ì…ë³„ íŠ¹í™” ì§€ì‹œë¬¸
        if query_type in self.query_type_templates:
            type_template = self.query_type_templates[query_type].get(
                language, self.query_type_templates[query_type].get("en", {})
            )

            if type_template:
                if language in ["ko", "mixed"]:
                    prompt_parts.append("\níŠ¹ë³„ ì§€ì‹œì‚¬í•­:")
                else:
                    prompt_parts.append("\nSpecific Instructions:")

                prompt_parts.append(
                    f"- {type_template.get('specific_instruction', '')}"
                )

                if "focus_areas" in type_template:
                    focus_label = (
                        "ì¤‘ì  ë¶„ì„ ì˜ì—­:"
                        if language in ["ko", "mixed"]
                        else "Focus Areas:"
                    )
                    prompt_parts.append(
                        f"- {focus_label} {', '.join(type_template['focus_areas'])}"
                    )

                if "output_format" in type_template:
                    prompt_parts.append(f"- {type_template['output_format']}")

        # ë³µì¡ë„ë³„ ì¡°ì •
        if complexity in self.complexity_templates:
            complexity_template = self.complexity_templates[complexity].get(
                language, self.complexity_templates[complexity].get("en", {})
            )

            if complexity_template:
                if language in ["ko", "mixed"]:
                    prompt_parts.append("\në‹µë³€ í˜•ì‹:")
                else:
                    prompt_parts.append("\nResponse Format:")

                for key, value in complexity_template.items():
                    prompt_parts.append(f"- {key}: {value}")

        # ì‹ ë¢°ë„ ì •ë³´ ì¶”ê°€
        if self.config.include_confidence and hasattr(
            query_analysis, "confidence_score"
        ):
            confidence_text = (
                f"\nì°¸ê³ : ì´ ë¶„ì„ì˜ ì‹ ë¢°ë„ëŠ” {query_analysis.confidence_score:.2f}ì…ë‹ˆë‹¤."
                if language in ["ko", "mixed"]
                else f"\nNote: The confidence score for this analysis is {query_analysis.confidence_score:.2f}."
            )
            prompt_parts.append(confidence_text)

        return "\n".join(prompt_parts)

    def create_langchain_prompt(
        self,
        query_analysis: Optional[QueryAnalysisResult] = None,
        prompt_type: str = "base",  # "base", "query_specific", "chat"
    ) -> BasePromptTemplate:
        """LangChain í˜¸í™˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""

        if not _langchain_available:
            raise ImportError("LangChain is required for creating prompt templates")

        if prompt_type == "chat":
            return self._create_chat_prompt_template(query_analysis)
        elif prompt_type == "query_specific" and query_analysis:
            prompt_text = self.get_query_specific_prompt(query_analysis)
        else:
            prompt_text = self.get_base_prompt(query_analysis)

        # PromptTemplate ìƒì„±
        return PromptTemplate(
            template=prompt_text,
            input_variables=["context", "question"],
            template_format="f-string",
        )

    def _create_chat_prompt_template(
        self, query_analysis: Optional[QueryAnalysisResult] = None
    ) -> ChatPromptTemplate:
        """ì±„íŒ…ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""

        language = self._determine_language(query_analysis)
        base = self.base_instructions[language]

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€
        system_content = f"{base['system_role']}\n\n{base['task_description']}"

        if query_analysis:
            # ì¿¼ë¦¬ë³„ íŠ¹í™” ì§€ì‹œë¬¸ ì¶”ê°€
            query_prompt = self.get_query_specific_prompt(
                query_analysis, include_base=False
            )
            if query_prompt.strip():
                system_content += f"\n\n{query_prompt}"

        system_message = SystemMessage(content=system_content)

        # íœ´ë¨¼ ë©”ì‹œì§€ í…œí”Œë¦¿
        human_content = f"{base['context_explanation']}\n{{context}}\n\n"
        human_content += (
            "ì§ˆë¬¸: {question}"
            if language in ["ko", "mixed"]
            else "Question: {question}"
        )

        human_message = HumanMessage(content=human_content)

        return ChatPromptTemplate.from_messages([system_message, human_message])

    def _determine_language(
        self, query_analysis: Optional[QueryAnalysisResult] = None
    ) -> str:
        """ì‚¬ìš©í•  ì–¸ì–´ ê²°ì •"""
        if query_analysis and hasattr(query_analysis, "language"):
            detected_lang = query_analysis.language
            if detected_lang in self.base_instructions:
                return detected_lang

        # ì„¤ì •ëœ ì–¸ì–´ ì‚¬ìš©
        if self.config.language in self.base_instructions:
            return self.config.language

        return "mixed"  # ê¸°ë³¸ê°’

    def update_config(self, **kwargs) -> None:
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                logger.info(f"ğŸ“ Updated config.{key} = {value}")

    def get_available_templates(self) -> Dict[str, List[str]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿ ëª©ë¡ ë°˜í™˜"""
        return {
            "query_types": [qt.value for qt in QueryType],
            "complexities": [qc.value for qc in QueryComplexity],
            "languages": list(self.base_instructions.keys()),
            "styles": [style.value for style in PromptStyle],
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_base_prompt(
    language: str = "mixed", style: str = "academic", **kwargs
) -> PromptTemplate:
    """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    config = PromptConfig(language=language, style=style, **kwargs)
    template_manager = GraphRAGPromptTemplates(config)
    return template_manager.create_langchain_prompt()


def create_query_prompt(
    query_analysis: QueryAnalysisResult, language: str = "auto", **kwargs
) -> PromptTemplate:
    """ì¿¼ë¦¬ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    if language == "auto":
        language = getattr(query_analysis, "language", "mixed")

    config = PromptConfig(language=language, **kwargs)
    template_manager = GraphRAGPromptTemplates(config)
    return template_manager.create_langchain_prompt(
        query_analysis=query_analysis, prompt_type="query_specific"
    )


def create_chat_prompt(
    query_analysis: Optional[QueryAnalysisResult] = None,
    language: str = "mixed",
    **kwargs,
) -> ChatPromptTemplate:
    """ì±„íŒ…ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    config = PromptConfig(language=language, **kwargs)
    template_manager = GraphRAGPromptTemplates(config)
    return template_manager.create_langchain_prompt(
        query_analysis=query_analysis, prompt_type="chat"
    )


def main():
    """GraphRAGPromptTemplates í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Testing GraphRAGPromptTemplates...")

    # ì„¤ì • ìƒì„±
    config = PromptConfig(
        language="mixed",
        style="academic",
        include_metadata=True,
        include_confidence=True,
    )

    # í…œí”Œë¦¿ ê´€ë¦¬ì ìƒì„±
    template_manager = GraphRAGPromptTemplates(config)

    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
    print("ğŸ“ Base Prompt:")
    base_prompt = template_manager.get_base_prompt()
    print(base_prompt[:300] + "...")

    # ë”ë¯¸ ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼
    class DummyQueryAnalysis:
        def __init__(self):
            self.language = "mixed"
            self.query_type = QueryType.KEYWORD_ANALYSIS
            self.complexity = QueryComplexity.MEDIUM
            self.confidence_score = 0.85

    dummy_analysis = DummyQueryAnalysis()

    # ì¿¼ë¦¬ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“Š Query-Specific Prompt ({dummy_analysis.query_type.value}):")
    specific_prompt = template_manager.get_query_specific_prompt(dummy_analysis)
    print(specific_prompt[:400] + "...")

    # LangChain í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    if _langchain_available:
        print(f"\nğŸ”— LangChain PromptTemplate:")
        lc_prompt = template_manager.create_langchain_prompt(dummy_analysis)
        print(f"Input variables: {lc_prompt.input_variables}")

        print(f"\nğŸ’¬ ChatPromptTemplate:")
        chat_prompt = template_manager.create_langchain_prompt(
            dummy_analysis, prompt_type="chat"
        )
        print(f"Messages: {len(chat_prompt.messages)}")

    # ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿ ëª©ë¡
    print(f"\nğŸ“‹ Available Templates:")
    available = template_manager.get_available_templates()
    for category, items in available.items():
        print(f"   {category}: {items}")

    print(f"\nâœ… GraphRAGPromptTemplates test completed!")


if __name__ == "__main__":
    main()
