"""
GraphRAG ë©”ëª¨ë¦¬ ê´€ë¦¬ ëª¨ë“ˆ
Memory Manager for GraphRAG System

ëŒ€í™”í˜• GraphRAG ì‹œìŠ¤í…œì˜ ë©”ëª¨ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ë° ì••ì¶•
- GraphRAG íŠ¹í™” ì»¨í…ìŠ¤íŠ¸ ìºì‹±
- ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬
- ê´€ë ¨ì„± ê¸°ë°˜ íˆìŠ¤í† ë¦¬ í•„í„°ë§
- í† í° ì œí•œ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ ì••ì¶•
"""

import json
import pickle
import hashlib
import logging
import warnings
from abc import ABC, abstractmethod
from enum import Enum
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from dataclasses import dataclass, field
from collections import defaultdict, deque
import numpy as np
from datetime import datetime, timedelta

# LangChain imports
try:
    from langchain_core.memory import BaseMemory
    from langchain.memory import (
        ConversationBufferMemory,
        ConversationBufferWindowMemory,
        ConversationSummaryMemory,
        ConversationSummaryBufferMemory,
    )
    from langchain_core.messages import (
        BaseMessage,
        HumanMessage,
        AIMessage,
        SystemMessage,
    )
    from langchain_core.language_models import BaseLanguageModel

    _langchain_available = True
except ImportError:
    _langchain_available = False
    warnings.warn("LangChain not available. Install with: pip install langchain")

    # Placeholder classes
    class BaseMemory:
        def __init__(self, *args, **kwargs):
            pass

    class BaseMessage:
        pass

    class BaseLanguageModel:
        pass


# GraphRAG imports
try:
    from ..query_analyzer import QueryAnalysisResult, QueryType, QueryComplexity
    from ..embeddings.subgraph_extractor import SubgraphResult
    from ..embeddings.context_serializer import SerializedContext
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class MemoryType(Enum):
    """ë©”ëª¨ë¦¬ íƒ€ì…"""

    BUFFER = "buffer"  # ë‹¨ìˆœ ë²„í¼ (ìµœê·¼ Nê°œ)
    WINDOW = "window"  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
    SUMMARY = "summary"  # ìš”ì•½ ê¸°ë°˜
    SUMMARY_BUFFER = "summary_buffer"  # ìš”ì•½ + ë²„í¼ í•˜ì´ë¸Œë¦¬ë“œ
    GRAPHRAG_ENHANCED = "graphrag_enhanced"  # GraphRAG íŠ¹í™”


class CompressionStrategy(Enum):
    """ì••ì¶• ì „ëµ"""

    FIFO = "fifo"  # First In First Out
    RELEVANCE = "relevance"  # ê´€ë ¨ì„± ê¸°ë°˜
    FREQUENCY = "frequency"  # ì–¸ê¸‰ ë¹ˆë„ ê¸°ë°˜
    SEMANTIC = "semantic"  # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜
    HYBRID = "hybrid"  # ë³µí•© ì „ëµ


@dataclass
class MemoryConfig:
    """ë©”ëª¨ë¦¬ ì„¤ì •"""

    # ê¸°ë³¸ ì„¤ì •
    memory_type: MemoryType = MemoryType.GRAPHRAG_ENHANCED
    memory_key: str = "chat_history"
    return_messages: bool = True

    # ìš©ëŸ‰ ì œí•œ
    max_token_limit: int = 4000
    max_interactions: int = 20
    buffer_size: int = 10

    # ì••ì¶• ì„¤ì •
    compression_strategy: CompressionStrategy = CompressionStrategy.HYBRID
    compression_ratio: float = 0.3  # ì••ì¶•ì‹œ ìœ ì§€í•  ë¹„ìœ¨

    # GraphRAG íŠ¹í™” ì„¤ì •
    cache_subgraph_results: bool = True
    cache_ttl_hours: int = 24
    max_cached_contexts: int = 50

    # ê´€ë ¨ì„± í•„í„°ë§
    relevance_threshold: float = 0.3
    enable_semantic_filtering: bool = True

    # ìš”ì•½ ì„¤ì •
    enable_summarization: bool = True
    summary_max_tokens: int = 500
    summary_overlap_tokens: int = 100


@dataclass
class ConversationTurn:
    """ëŒ€í™” í„´ ì •ë³´"""

    timestamp: datetime
    human_message: str
    ai_message: str
    query_analysis: Optional[QueryAnalysisResult] = None
    subgraph_result: Optional[SubgraphResult] = None
    serialized_context: Optional[SerializedContext] = None
    relevance_score: float = 1.0
    token_count: int = 0
    session_id: str = "default"


class GraphRAGMemoryManager:
    """GraphRAG ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €"""

    def __init__(
        self,
        config: Optional[MemoryConfig] = None,
        llm: Optional[BaseLanguageModel] = None,
        persist_directory: Optional[str] = None,
    ):
        """
        Args:
            config: ë©”ëª¨ë¦¬ ì„¤ì •
            llm: ìš”ì•½ìš© LLM (ì„ íƒì )
            persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        if not _langchain_available:
            raise ImportError("LangChain is required for MemoryManager")

        self.config = config or MemoryConfig()
        self.llm = llm
        self.persist_directory = Path(persist_directory) if persist_directory else None

        # ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
        self.session_memories: Dict[str, BaseMemory] = {}
        self.conversation_histories: Dict[str, List[ConversationTurn]] = defaultdict(
            list
        )

        # GraphRAG íŠ¹í™” ìºì‹œ
        self.subgraph_cache: Dict[str, SubgraphResult] = {}
        self.context_cache: Dict[str, SerializedContext] = {}
        self.cache_timestamps: Dict[str, datetime] = {}

        # í† í° ì¹´ìš´í„° (ëŒ€ëµì )
        self.token_counter = self._create_token_counter()

        # ì˜êµ¬ ì €ì¥ì†Œ ì„¤ì •
        if self.persist_directory:
            self.persist_directory.mkdir(parents=True, exist_ok=True)
            self._load_persisted_data()

        logger.info("âœ… GraphRAGMemoryManager initialized")
        logger.info(f"   ğŸ§  Memory type: {self.config.memory_type.value}")
        logger.info(f"   ğŸ’¾ Persist: {'Yes' if self.persist_directory else 'No'}")

    def _create_token_counter(self) -> Callable[[str], int]:
        """í† í° ì¹´ìš´í„° í•¨ìˆ˜ ìƒì„±"""
        try:
            import tiktoken

            encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 ê¸°ë³¸ ì¸ì½”ë”©
            return lambda text: len(encoding.encode(text))
        except ImportError:
            # tiktokenì´ ì—†ìœ¼ë©´ ë‹¨ìˆœ ì¶”ì •
            logger.warning("âš ï¸ tiktoken not available, using simple token estimation")
            return lambda text: len(text.split()) * 1.3  # ë‹¨ì–´ìˆ˜ Ã— 1.3 (ëŒ€ëµì )

    def get_or_create_memory(self, session_id: str = "default") -> BaseMemory:
        """ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ì¡°íšŒ ë˜ëŠ” ìƒì„±"""

        if session_id not in self.session_memories:
            logger.info(f"ğŸ§  Creating new memory for session: {session_id}")
            self.session_memories[session_id] = self._create_memory_instance()

        return self.session_memories[session_id]

    def _create_memory_instance(self) -> BaseMemory:
        """ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""

        memory_type = self.config.memory_type

        if memory_type == MemoryType.BUFFER:
            return ConversationBufferMemory(
                memory_key=self.config.memory_key,
                return_messages=self.config.return_messages,
            )

        elif memory_type == MemoryType.WINDOW:
            return ConversationBufferWindowMemory(
                k=self.config.buffer_size,
                memory_key=self.config.memory_key,
                return_messages=self.config.return_messages,
            )

        elif memory_type == MemoryType.SUMMARY:
            if not self.llm:
                logger.warning(
                    "âš ï¸ LLM required for summary memory, falling back to buffer"
                )
                return self._create_fallback_memory()

            return ConversationSummaryMemory(
                llm=self.llm,
                memory_key=self.config.memory_key,
                return_messages=self.config.return_messages,
                max_token_limit=self.config.summary_max_tokens,
            )

        elif memory_type == MemoryType.SUMMARY_BUFFER:
            if not self.llm:
                logger.warning(
                    "âš ï¸ LLM required for summary buffer memory, falling back to window"
                )
                return ConversationBufferWindowMemory(
                    k=self.config.buffer_size,
                    memory_key=self.config.memory_key,
                    return_messages=self.config.return_messages,
                )

            return ConversationSummaryBufferMemory(
                llm=self.llm,
                memory_key=self.config.memory_key,
                return_messages=self.config.return_messages,
                max_token_limit=self.config.max_token_limit,
            )

        elif memory_type == MemoryType.GRAPHRAG_ENHANCED:
            return self._create_graphrag_enhanced_memory()

        else:
            logger.warning(f"âš ï¸ Unknown memory type: {memory_type}, using buffer")
            return self._create_fallback_memory()

    def _create_fallback_memory(self) -> BaseMemory:
        """í´ë°± ë©”ëª¨ë¦¬ ìƒì„±"""
        return ConversationBufferWindowMemory(
            k=5,  # ìµœê·¼ 5ê°œë§Œ
            memory_key=self.config.memory_key,
            return_messages=self.config.return_messages,
        )

    def _create_graphrag_enhanced_memory(self) -> BaseMemory:
        """GraphRAG íŠ¹í™” ë©”ëª¨ë¦¬ ìƒì„±"""
        # ê¸°ë³¸ì ìœ¼ë¡œëŠ” SummaryBufferë¥¼ ë² ì´ìŠ¤ë¡œ í•˜ë˜, ì¶”ê°€ ê¸°ëŠ¥ êµ¬í˜„
        if self.llm:
            base_memory = ConversationSummaryBufferMemory(
                llm=self.llm,
                memory_key=self.config.memory_key,
                return_messages=self.config.return_messages,
                max_token_limit=self.config.max_token_limit,
            )
        else:
            base_memory = ConversationBufferWindowMemory(
                k=self.config.buffer_size,
                memory_key=self.config.memory_key,
                return_messages=self.config.return_messages,
            )

        return base_memory

    def add_conversation_turn(
        self,
        human_message: str,
        ai_message: str,
        session_id: str = "default",
        query_analysis: Optional[QueryAnalysisResult] = None,
        subgraph_result: Optional[SubgraphResult] = None,
        serialized_context: Optional[SerializedContext] = None,
    ) -> None:
        """ëŒ€í™” í„´ ì¶”ê°€ (GraphRAG íŠ¹í™” ì •ë³´ í¬í•¨)"""

        # ê¸°ë³¸ ë©”ëª¨ë¦¬ì— ì¶”ê°€
        memory = self.get_or_create_memory(session_id)
        memory.save_context({"input": human_message}, {"output": ai_message})

        # GraphRAG íŠ¹í™” ì •ë³´ ì €ì¥
        turn = ConversationTurn(
            timestamp=datetime.now(),
            human_message=human_message,
            ai_message=ai_message,
            query_analysis=query_analysis,
            subgraph_result=subgraph_result,
            serialized_context=serialized_context,
            token_count=self.token_counter(human_message + ai_message),
            session_id=session_id,
        )

        self.conversation_histories[session_id].append(turn)

        # ìºì‹œ ì €ì¥
        if self.config.cache_subgraph_results and subgraph_result:
            self._cache_subgraph_result(human_message, subgraph_result)

        if self.config.cache_subgraph_results and serialized_context:
            self._cache_serialized_context(human_message, serialized_context)

        # ë©”ëª¨ë¦¬ ì••ì¶• ê²€ì‚¬
        self._check_and_compress_memory(session_id)

        logger.info(f"ğŸ’­ Added conversation turn to session: {session_id}")

    def _cache_subgraph_result(self, query: str, result: SubgraphResult) -> None:
        """ì„œë¸Œê·¸ë˜í”„ ê²°ê³¼ ìºì‹±"""
        cache_key = self._generate_cache_key(query)
        self.subgraph_cache[cache_key] = result
        self.cache_timestamps[cache_key] = datetime.now()

        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.subgraph_cache) > self.config.max_cached_contexts:
            self._clean_old_cache()

    def _cache_serialized_context(self, query: str, context: SerializedContext) -> None:
        """ì§ë ¬í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìºì‹±"""
        cache_key = self._generate_cache_key(query)
        self.context_cache[cache_key] = context
        self.cache_timestamps[cache_key] = datetime.now()

    def _generate_cache_key(self, query: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì¿¼ë¦¬ ì •ê·œí™” í›„ í•´ì‹œ
        normalized_query = query.lower().strip()
        return hashlib.md5(normalized_query.encode()).hexdigest()

    def _clean_old_cache(self) -> None:
        """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
        now = datetime.now()
        ttl_threshold = now - timedelta(hours=self.config.cache_ttl_hours)

        expired_keys = [
            key
            for key, timestamp in self.cache_timestamps.items()
            if timestamp < ttl_threshold
        ]

        for key in expired_keys:
            self.subgraph_cache.pop(key, None)
            self.context_cache.pop(key, None)
            self.cache_timestamps.pop(key, None)

        # í¬ê¸° ì œí•œ
        if len(self.subgraph_cache) > self.config.max_cached_contexts:
            # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì œê±°
            sorted_keys = sorted(self.cache_timestamps.items(), key=lambda x: x[1])

            excess_count = len(self.subgraph_cache) - self.config.max_cached_contexts
            for key, _ in sorted_keys[:excess_count]:
                self.subgraph_cache.pop(key, None)
                self.context_cache.pop(key, None)
                self.cache_timestamps.pop(key, None)

        if expired_keys:
            logger.info(f"ğŸ—‘ï¸ Cleaned {len(expired_keys)} expired cache entries")

    def get_cached_result(
        self, query: str
    ) -> Optional[Tuple[SubgraphResult, SerializedContext]]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        cache_key = self._generate_cache_key(query)

        # TTL ê²€ì‚¬
        if cache_key in self.cache_timestamps:
            cache_time = self.cache_timestamps[cache_key]
            now = datetime.now()

            if now - cache_time > timedelta(hours=self.config.cache_ttl_hours):
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                self.subgraph_cache.pop(cache_key, None)
                self.context_cache.pop(cache_key, None)
                self.cache_timestamps.pop(cache_key, None)
                return None

        subgraph_result = self.subgraph_cache.get(cache_key)
        context_result = self.context_cache.get(cache_key)

        if subgraph_result and context_result:
            logger.info(f"âœ… Cache hit for query: {query[:50]}...")
            return subgraph_result, context_result

        return None

    def _check_and_compress_memory(self, session_id: str) -> None:
        """ë©”ëª¨ë¦¬ ì••ì¶• ê²€ì‚¬ ë° ì‹¤í–‰"""

        history = self.conversation_histories[session_id]

        # í† í° ìˆ˜ ê³„ì‚°
        total_tokens = sum(turn.token_count for turn in history)

        # ì••ì¶• í•„ìš”ì„± í™•ì¸
        if (
            total_tokens > self.config.max_token_limit
            or len(history) > self.config.max_interactions
        ):

            logger.info(f"ğŸ—œï¸ Compressing memory for session {session_id}")
            self._compress_memory(session_id)

    def _compress_memory(self, session_id: str) -> None:
        """ë©”ëª¨ë¦¬ ì••ì¶• ì‹¤í–‰"""

        history = self.conversation_histories[session_id]
        strategy = self.config.compression_strategy

        if strategy == CompressionStrategy.FIFO:
            compressed_history = self._compress_fifo(history)
        elif strategy == CompressionStrategy.RELEVANCE:
            compressed_history = self._compress_by_relevance(history)
        elif strategy == CompressionStrategy.FREQUENCY:
            compressed_history = self._compress_by_frequency(history)
        elif strategy == CompressionStrategy.SEMANTIC:
            compressed_history = self._compress_by_semantic_similarity(history)
        elif strategy == CompressionStrategy.HYBRID:
            compressed_history = self._compress_hybrid(history)
        else:
            compressed_history = self._compress_fifo(history)  # í´ë°±

        # ì••ì¶•ëœ íˆìŠ¤í† ë¦¬ë¡œ êµì²´
        self.conversation_histories[session_id] = compressed_history

        # ê¸°ë³¸ ë©”ëª¨ë¦¬ë„ ì¬êµ¬ì„±
        self._rebuild_base_memory(session_id, compressed_history)

        logger.info(f"âœ… Compressed {len(history)} â†’ {len(compressed_history)} turns")

    def _compress_fifo(self, history: List[ConversationTurn]) -> List[ConversationTurn]:
        """FIFO ì••ì¶• (ìµœê·¼ ê²ƒë§Œ ìœ ì§€)"""
        target_size = int(len(history) * self.config.compression_ratio)
        target_size = max(target_size, 3)  # ìµœì†Œ 3ê°œëŠ” ìœ ì§€
        return history[-target_size:]

    def _compress_by_relevance(
        self, history: List[ConversationTurn]
    ) -> List[ConversationTurn]:
        """ê´€ë ¨ì„± ê¸°ë°˜ ì••ì¶•"""
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        scored_history = []
        for turn in history:
            score = turn.relevance_score

            # ìµœê·¼ì„± ë³´ë„ˆìŠ¤
            age_hours = (datetime.now() - turn.timestamp).total_seconds() / 3600
            recency_bonus = max(0, 1.0 - age_hours / 24)  # 24ì‹œê°„ë‚´ ë³´ë„ˆìŠ¤

            total_score = score + recency_bonus * 0.3
            scored_history.append((total_score, turn))

        # ìƒìœ„ Nê°œ ì„ íƒ
        scored_history.sort(key=lambda x: x[0], reverse=True)
        target_size = int(len(history) * self.config.compression_ratio)
        target_size = max(target_size, 3)

        selected_turns = [turn for _, turn in scored_history[:target_size]]

        # ì‹œê°„ìˆœ ì •ë ¬
        selected_turns.sort(key=lambda x: x.timestamp)
        return selected_turns

    def _compress_by_frequency(
        self, history: List[ConversationTurn]
    ) -> List[ConversationTurn]:
        """ë¹ˆë„ ê¸°ë°˜ ì••ì¶• (ìì£¼ ì–¸ê¸‰ë˜ëŠ” ì£¼ì œ ìš°ì„ )"""
        # ê°„ë‹¨íˆ í‚¤ì›Œë“œ ë¹ˆë„ë¡œ ê³„ì‚°
        keyword_freq = defaultdict(int)

        for turn in history:
            if turn.query_analysis and turn.query_analysis.keywords:
                for keyword in turn.query_analysis.keywords:
                    keyword_freq[keyword.lower()] += 1

        # ê° í„´ì˜ ë¹ˆë„ ì ìˆ˜ ê³„ì‚°
        scored_turns = []
        for turn in history:
            score = 0
            if turn.query_analysis and turn.query_analysis.keywords:
                for keyword in turn.query_analysis.keywords:
                    score += keyword_freq[keyword.lower()]

            scored_turns.append((score, turn))

        # ìƒìœ„ Nê°œ ì„ íƒ
        scored_turns.sort(key=lambda x: x[0], reverse=True)
        target_size = int(len(history) * self.config.compression_ratio)
        target_size = max(target_size, 3)

        selected_turns = [turn for _, turn in scored_turns[:target_size]]
        selected_turns.sort(key=lambda x: x.timestamp)
        return selected_turns

    def _compress_by_semantic_similarity(
        self, history: List[ConversationTurn]
    ) -> List[ConversationTurn]:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì••ì¶• (ìœ ì‚¬í•œ ëŒ€í™”ëŠ” ëŒ€í‘œì ì¸ ê²ƒë§Œ)"""
        # ê°„ë‹¨í•œ êµ¬í˜„: ìµœê·¼ ê²ƒë“¤ ìœ„ì£¼ë¡œ ì„ íƒ
        return self._compress_fifo(history)

    def _compress_hybrid(
        self, history: List[ConversationTurn]
    ) -> List[ConversationTurn]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• (ì—¬ëŸ¬ ì „ëµ ì¡°í•©)"""
        # 1ë‹¨ê³„: ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§
        relevant_turns = [
            turn
            for turn in history
            if turn.relevance_score >= self.config.relevance_threshold
        ]

        if not relevant_turns:
            relevant_turns = history  # í´ë°±

        # 2ë‹¨ê³„: ìµœê·¼ì„± ê³ ë ¤
        if len(relevant_turns) > self.config.buffer_size:
            # ìµœê·¼ ì ˆë°˜ + ê´€ë ¨ì„± ë†’ì€ ë‚˜ë¨¸ì§€
            recent_count = len(relevant_turns) // 2
            recent_turns = relevant_turns[-recent_count:]

            older_turns = relevant_turns[:-recent_count]
            older_turns.sort(key=lambda x: x.relevance_score, reverse=True)

            target_older = self.config.buffer_size - recent_count
            selected_older = older_turns[:target_older] if target_older > 0 else []

            final_turns = selected_older + recent_turns
            final_turns.sort(key=lambda x: x.timestamp)
            return final_turns

        return relevant_turns

    def _rebuild_base_memory(
        self, session_id: str, compressed_history: List[ConversationTurn]
    ) -> None:
        """ì••ì¶•ëœ íˆìŠ¤í† ë¦¬ë¡œ ê¸°ë³¸ ë©”ëª¨ë¦¬ ì¬êµ¬ì„±"""

        # ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        new_memory = self._create_memory_instance()

        # ì••ì¶•ëœ íˆìŠ¤í† ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì¬êµ¬ì„±
        for turn in compressed_history:
            new_memory.save_context(
                {"input": turn.human_message}, {"output": turn.ai_message}
            )

        # ê¸°ì¡´ ë©”ëª¨ë¦¬ êµì²´
        self.session_memories[session_id] = new_memory

    def get_conversation_summary(self, session_id: str = "default") -> str:
        """ëŒ€í™” ìš”ì•½ ìƒì„±"""

        if session_id not in self.conversation_histories:
            return "No conversation history found."

        history = self.conversation_histories[session_id]

        if not history:
            return "No conversations yet."

        # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
        total_turns = len(history)
        recent_topics = []

        # ìµœê·¼ 5ê°œ ëŒ€í™”ì˜ ì£¼ìš” í‚¤ì›Œë“œ
        for turn in history[-5:]:
            if turn.query_analysis and turn.query_analysis.keywords:
                recent_topics.extend(turn.query_analysis.keywords[:3])

        topic_counts = defaultdict(int)
        for topic in recent_topics:
            topic_counts[topic.lower()] += 1

        top_topics = [
            topic
            for topic, _ in sorted(
                topic_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]
        ]

        summary = f"ì´ {total_turns}ê°œ ëŒ€í™” ì§„í–‰ë¨."
        if top_topics:
            summary += f" ì£¼ìš” ê´€ì‹¬ ì£¼ì œ: {', '.join(top_topics)}"

        return summary

    def clear_session(self, session_id: str) -> None:
        """ì„¸ì…˜ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        self.session_memories.pop(session_id, None)
        self.conversation_histories.pop(session_id, None)
        logger.info(f"ğŸ—‘ï¸ Cleared session: {session_id}")

    def get_memory_stats(self, session_id: str = "default") -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ í†µê³„ ì •ë³´"""

        stats = {
            "session_id": session_id,
            "total_turns": 0,
            "total_tokens": 0,
            "cache_hits": len(self.subgraph_cache),
            "memory_type": self.config.memory_type.value,
        }

        if session_id in self.conversation_histories:
            history = self.conversation_histories[session_id]
            stats["total_turns"] = len(history)
            stats["total_tokens"] = sum(turn.token_count for turn in history)

            if history:
                stats["oldest_conversation"] = history[0].timestamp.isoformat()
                stats["newest_conversation"] = history[-1].timestamp.isoformat()

        return stats

    def _load_persisted_data(self) -> None:
        """ì˜êµ¬ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        if not self.persist_directory:
            return

        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ
            history_file = self.persist_directory / "conversation_histories.pkl"
            if history_file.exists():
                with open(history_file, "rb") as f:
                    self.conversation_histories = pickle.load(f)
                logger.info(f"ğŸ“‚ Loaded conversation histories")

            # ìºì‹œ ë¡œë“œ
            cache_file = self.persist_directory / "cache_data.pkl"
            if cache_file.exists():
                with open(cache_file, "rb") as f:
                    cache_data = pickle.load(f)
                    self.subgraph_cache = cache_data.get("subgraph_cache", {})
                    self.context_cache = cache_data.get("context_cache", {})
                    self.cache_timestamps = cache_data.get("cache_timestamps", {})
                logger.info(f"ğŸ“‚ Loaded cache data")

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load persisted data: {e}")

    def save_to_disk(self) -> None:
        """ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        if not self.persist_directory:
            return

        try:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
            history_file = self.persist_directory / "conversation_histories.pkl"
            with open(history_file, "wb") as f:
                pickle.dump(dict(self.conversation_histories), f)

            # ìºì‹œ ì €ì¥
            cache_file = self.persist_directory / "cache_data.pkl"
            cache_data = {
                "subgraph_cache": self.subgraph_cache,
                "context_cache": self.context_cache,
                "cache_timestamps": self.cache_timestamps,
            }
            with open(cache_file, "wb") as f:
                pickle.dump(cache_data, f)

            logger.info(f"ğŸ’¾ Saved memory data to disk")

        except Exception as e:
            logger.error(f"âŒ Failed to save memory data: {e}")


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_memory_manager(
    memory_type: str = "graphrag_enhanced",
    max_token_limit: int = 4000,
    llm: Optional[BaseLanguageModel] = None,
    persist_directory: Optional[str] = None,
    **kwargs,
) -> GraphRAGMemoryManager:
    """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„± í¸ì˜ í•¨ìˆ˜"""

    config = MemoryConfig(
        memory_type=MemoryType(memory_type), max_token_limit=max_token_limit, **kwargs
    )

    return GraphRAGMemoryManager(
        config=config, llm=llm, persist_directory=persist_directory
    )


def main():
    """GraphRAGMemoryManager í…ŒìŠ¤íŠ¸"""

    if not _langchain_available:
        print("âŒ LangChain not available for testing")
        return

    print("ğŸ§ª Testing GraphRAGMemoryManager...")

    try:
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ìƒì„±
        memory_manager = create_memory_manager(
            memory_type="buffer", max_token_limit=1000, max_interactions=5
        )

        # í…ŒìŠ¤íŠ¸ ëŒ€í™” ì¶”ê°€
        test_conversations = [
            (
                "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ ê¸°ë²•ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "SoC ì˜ˆì¸¡ì—ëŠ” ì¹¼ë§Œ í•„í„°, LSTM ë“±ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.",
            ),
            (
                "ê¹€ì² ìˆ˜ êµìˆ˜ì˜ ì—°êµ¬ ë¶„ì•¼ëŠ”?",
                "ê¹€ì² ìˆ˜ êµìˆ˜ëŠ” ë°°í„°ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì—°êµ¬í•©ë‹ˆë‹¤.",
            ),
            (
                "ì „ê¸°ì°¨ ì¶©ì „ ê¸°ìˆ  ë™í–¥ì€?",
                "ë¬´ì„  ì¶©ì „, ê³ ì† ì¶©ì „ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            ),
        ]

        session_id = "test_session"

        for human_msg, ai_msg in test_conversations:
            memory_manager.add_conversation_turn(
                human_message=human_msg, ai_message=ai_msg, session_id=session_id
            )
            print(f"ğŸ’­ Added: {human_msg[:30]}...")

        # ë©”ëª¨ë¦¬ ì¡°íšŒ
        memory = memory_manager.get_or_create_memory(session_id)
        print(f"ğŸ§  Memory type: {type(memory).__name__}")

        # í†µê³„ í™•ì¸
        stats = memory_manager.get_memory_stats(session_id)
        print(f"ğŸ“Š Memory stats: {stats}")

        # ëŒ€í™” ìš”ì•½
        summary = memory_manager.get_conversation_summary(session_id)
        print(f"ğŸ“ Summary: {summary}")

        # ìºì‹œ í…ŒìŠ¤íŠ¸
        cached_result = memory_manager.get_cached_result("ë°°í„°ë¦¬ SoC")
        print(f"ğŸ’¾ Cache result: {'Found' if cached_result else 'Not found'}")

        print(f"\nâœ… GraphRAGMemoryManager test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
