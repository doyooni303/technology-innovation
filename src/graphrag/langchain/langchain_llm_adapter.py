"""
GraphRAG LangChain LLM ì–´ëŒ‘í„°
LangChain LLM Adapter for GraphRAG System

GraphRAGì˜ LocalLLMManagerë¥¼ LangChain BaseLanguageModelë¡œ ë³€í™˜
- YAML ì„¤ì • ì™„ì „ í˜¸í™˜
- ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
"""

import asyncio
import logging
import warnings
from typing import Any, Dict, List, Optional, Union, Iterator, AsyncIterator, Mapping
from dataclasses import dataclass
from enum import Enum
import time

# LangChain imports
try:
    from langchain_core.language_models.llms import LLM
    from langchain_core.language_models.base import BaseLanguageModel
    from langchain_core.outputs import Generation, LLMResult, GenerationChunk
    from langchain_core.callbacks.manager import (
        CallbackManagerForLLMRun,
        AsyncCallbackManagerForLLMRun,
    )
    from langchain_core.pydantic_v1 import Field, root_validator

    _langchain_available = True
except ImportError:
    _langchain_available = False
    warnings.warn("LangChain not available for LLM adapter")

# GraphRAG imports
try:
    from ..graphrag_pipeline import LocalLLMManager
    from ..config_manager import GraphRAGConfigManager
except ImportError as e:
    warnings.warn(f"GraphRAG pipeline components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class AdapterMode(Enum):
    """ì–´ëŒ‘í„° ëª¨ë“œ"""

    DIRECT = "direct"  # ì§ì ‘ í˜¸ì¶œ
    BATCHED = "batched"  # ë°°ì¹˜ ì²˜ë¦¬
    STREAMING = "streaming"  # ìŠ¤íŠ¸ë¦¬ë°
    CACHED = "cached"  # ìºì‹± í™œìš©


@dataclass
class LLMUsageStats:
    """LLM ì‚¬ìš© í†µê³„"""

    total_calls: int = 0
    total_tokens: int = 0
    total_time: float = 0.0
    average_time: float = 0.0
    cache_hits: int = 0
    failed_calls: int = 0

    def update_call(self, tokens: int, call_time: float, from_cache: bool = False):
        """í˜¸ì¶œ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_calls += 1
        if not from_cache:
            self.total_tokens += tokens
        self.total_time += call_time
        self.average_time = self.total_time / self.total_calls
        if from_cache:
            self.cache_hits += 1

    def update_failure(self):
        """ì‹¤íŒ¨ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.failed_calls += 1


class GraphRAGLLMAdapter(LLM):
    """ìˆ˜ì •ëœ GraphRAG LLM Adapter - FieldInfo ì˜¤ë¥˜ í•´ê²°"""

    # LangChain Pydantic í•„ë“œë“¤
    llm_manager: Any = Field(description="GraphRAG LocalLLMManager instance")
    temperature: float = Field(default=0.1, description="Generation temperature")
    max_tokens: int = Field(default=1000, description="Maximum tokens to generate")

    # ì–´ëŒ‘í„° ì„¤ì •
    mode: AdapterMode = Field(default=AdapterMode.DIRECT, description="Adapter mode")
    enable_caching: bool = Field(default=True, description="Enable response caching")
    cache_ttl: int = Field(default=3600, description="Cache TTL in seconds")
    max_retries: int = Field(default=3, description="Maximum retry attempts")
    retry_delay: float = Field(default=1.0, description="Delay between retries")

    # ë‚´ë¶€ ìƒíƒœ (exclude=Trueë¡œ ì§ë ¬í™”ì—ì„œ ì œì™¸)
    _cache: Dict[str, Any] = Field(default_factory=dict, exclude=True)
    _cache_timestamps: Dict[str, float] = Field(default_factory=dict, exclude=True)
    _stats: LLMUsageStats = Field(default_factory=LLMUsageStats, exclude=True)
    _is_loaded: bool = Field(default=False, exclude=True)

    class Config:
        """Pydantic ì„¤ì •"""

        arbitrary_types_allowed = True

    @root_validator(pre=True)
    def validate_llm_manager(cls, values):
        """LLM ë§¤ë‹ˆì € ê²€ì¦"""
        llm_manager = values.get("llm_manager")
        if llm_manager is None:
            raise ValueError("llm_manager is required")
        return values

    def __init__(self, **kwargs):
        """ì–´ëŒ‘í„° ì´ˆê¸°í™”"""
        super().__init__(**kwargs)

        if not _langchain_available:
            raise ImportError("LangChain is required for LLM adapter")

        # âœ… Pydantic Field ê°’ë“¤ì„ ì•ˆì „í•˜ê²Œ ê²€ì¦
        self._validate_field_values()

        # í†µê³„ ì´ˆê¸°í™”
        self._stats = LLMUsageStats()
        self._cache = {}
        self._cache_timestamps = {}
        self._is_loaded = False

        logger.info("âœ… GraphRAGLLMAdapter initialized")
        logger.info(f"   ğŸŒ¡ï¸ Temperature: {self._get_safe_temperature()}")
        logger.info(f"   ğŸ“ Max tokens: {self._get_safe_max_tokens()}")
        logger.info(
            f"   ğŸ”§ Mode: {self.mode.value if hasattr(self.mode, 'value') else self.mode}"
        )
        logger.info(f"   ğŸ’¾ Caching: {self.enable_caching}")

    def _validate_field_values(self):
        """Pydantic Field ê°’ë“¤ì„ ê²€ì¦í•˜ê³  ìˆ˜ì •"""

        # temperature ê²€ì¦
        if not isinstance(getattr(self, "temperature", None), (int, float)):
            logger.warning("âš ï¸ Invalid temperature field, setting to 0.1")
            object.__setattr__(self, "temperature", 0.1)

        # max_tokens ê²€ì¦
        if not isinstance(getattr(self, "max_tokens", None), (int, float)):
            logger.warning("âš ï¸ Invalid max_tokens field, setting to 1000")
            object.__setattr__(self, "max_tokens", 1000)

        # mode ê²€ì¦
        if not hasattr(getattr(self, "mode", None), "value"):
            logger.warning("âš ï¸ Invalid mode field, setting to DIRECT")
            from enum import Enum

            if hasattr(self, "mode") and isinstance(self.mode, str):
                # ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜ ì‹œë„
                try:
                    object.__setattr__(self, "mode", AdapterMode(self.mode))
                except (ValueError, AttributeError):
                    object.__setattr__(self, "mode", AdapterMode.DIRECT)
            else:
                object.__setattr__(self, "mode", AdapterMode.DIRECT)

    def _get_safe_temperature(self) -> float:
        """ì•ˆì „í•œ temperature ê°’ ë°˜í™˜"""
        temp = getattr(self, "temperature", 0.1)
        return float(temp) if isinstance(temp, (int, float)) else 0.1

    def _get_safe_max_tokens(self) -> int:
        """ì•ˆì „í•œ max_tokens ê°’ ë°˜í™˜"""
        tokens = getattr(self, "max_tokens", 1000)
        return int(tokens) if isinstance(tokens, (int, float)) else 1000

    @property
    def _llm_type(self) -> str:
        """LangChain LLM íƒ€ì… ì‹ë³„ì"""
        return "graphrag_llm"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """LangChainì´ ì‚¬ìš©í•˜ëŠ” ì‹ë³„ íŒŒë¼ë¯¸í„° - ì•ˆì „í•œ ê°’ ì¶”ì¶œ"""
        return {
            "temperature": self._get_safe_temperature(),
            "max_tokens": self._get_safe_max_tokens(),
            "mode": self.mode.value if hasattr(self.mode, "value") else str(self.mode),
            "model_path": getattr(
                getattr(self.llm_manager, "config", None), "model_path", "unknown"
            ),
        }

    def _ensure_loaded(self) -> None:
        """LLM ë¡œë“œ í™•ì¸ ë° ì§€ì—° ë¡œë”©"""
        if not self._is_loaded and not self.llm_manager.is_loaded:
            logger.info("ğŸ“¥ Loading LLM model (lazy loading)...")
            try:
                self.llm_manager.load_model()
                self._is_loaded = True
                logger.info("âœ… LLM model loaded successfully")
            except Exception as e:
                logger.error(f"âŒ Failed to load LLM model: {e}")
                raise

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """ë™ê¸° LLM í˜¸ì¶œ (LangChain ì¸í„°í˜ì´ìŠ¤)"""

        start_time = time.time()

        try:
            # 1. ìºì‹œ í™•ì¸
            if self.enable_caching:
                cached_response = self._get_from_cache(prompt)
                if cached_response is not None:
                    call_time = time.time() - start_time
                    self._stats.update_call(
                        tokens=len(cached_response.split()),
                        call_time=call_time,
                        from_cache=True,
                    )
                    logger.debug(f"ğŸ’¾ Cache hit for prompt: {prompt[:50]}...")
                    return cached_response

            # 2. LLM ë¡œë“œ í™•ì¸
            self._ensure_loaded()

            # 3. íŒŒë¼ë¯¸í„° ì¤€ë¹„
            generation_kwargs = self._prepare_generation_kwargs(stop, **kwargs)

            # 4. ëª¨ë“œë³„ ì²˜ë¦¬
            if self.mode == AdapterMode.BATCHED:
                response = self._call_batched([prompt], **generation_kwargs)[0]
            elif self.mode == AdapterMode.STREAMING:
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ë™ê¸°ë¡œ ë³€í™˜
                response = self._call_streaming_sync(prompt, **generation_kwargs)
            else:  # DIRECT ë˜ëŠ” CACHED
                response = self._call_direct(prompt, **generation_kwargs)

            # 5. ì‘ë‹µ í›„ì²˜ë¦¬
            response = self._post_process_response(response)

            # 6. ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache(prompt, response)

            # 7. í†µê³„ ì—…ë°ì´íŠ¸
            call_time = time.time() - start_time
            self._stats.update_call(tokens=len(response.split()), call_time=call_time)

            # 8. ì½œë°± í˜¸ì¶œ (LangChain)
            if run_manager:
                run_manager.on_llm_end(
                    LLMResult(
                        generations=[[Generation(text=response)]],
                        llm_output={"model_name": self._llm_type},
                    )
                )

            logger.debug(f"âœ… LLM call completed in {call_time:.2f}s")
            return response

        except Exception as e:
            self._stats.update_failure()
            logger.error(f"âŒ LLM call failed: {e}")

            # ì¬ì‹œë„ ë¡œì§
            if self.max_retries > 0:
                return self._retry_call(prompt, stop, run_manager, **kwargs)
            else:
                raise

    def _call_direct(self, prompt: str, **kwargs) -> str:
        """ì§ì ‘ LLM í˜¸ì¶œ - ì™„ì „íˆ ì•ˆì „í•œ íŒŒë¼ë¯¸í„° ì²˜ë¦¬"""
        try:
            # âœ… FieldInfo ê°ì²´ ì‚¬ìš© ì™„ì „ ë°©ì§€
            safe_max_length = 500  # ê³ ì •ê°’ ì‚¬ìš©

            # kwargsì—ì„œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            if "max_length" in kwargs and isinstance(
                kwargs["max_length"], (int, float)
            ):
                safe_max_length = int(kwargs["max_length"])

            logger.debug(f"ğŸ”§ Using safe max_length: {safe_max_length}")

            # âœ… í”„ë¡¬í”„íŠ¸ ì „ì²˜ë¦¬ ì¶”ê°€
            processed_prompt = self._enhance_prompt_quality(prompt)

            # LocalLLMManager.generate() í˜¸ì¶œ - íŒŒë¼ë¯¸í„° ìµœì†Œí™”
            response = self.llm_manager.generate(
                prompt=processed_prompt, max_length=safe_max_length
            )

            if not isinstance(response, str):
                logger.warning(f"âš ï¸ LLM returned non-string: {type(response)}")
                response = str(response)

            # âœ… ì‘ë‹µ í’ˆì§ˆ ê°œì„ 
            enhanced_response = self._enhance_response_quality(response)
            return enhanced_response

        except Exception as e:
            logger.error(f"âŒ Direct LLM call failed: {e}")

            # êµ¬ì²´ì ì¸ ì—ëŸ¬ ì²˜ë¦¬
            if "unexpected keyword argument" in str(e):
                raise RuntimeError(f"íŒŒë¼ë¯¸í„° ì „ë‹¬ ì˜¤ë¥˜ (FieldInfo ê´€ë ¨): {e}")
            else:
                raise

    # ğŸ†• í”„ë¡¬í”„íŠ¸ ì „ì²˜ë¦¬ ë©”ì„œë“œ ì¶”ê°€
    def _preprocess_prompt(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ì „ì²˜ë¦¬ - ë” ë‚˜ì€ ì‘ë‹µì„ ìœ„í•œ êµ¬ì¡°í™”"""

        # ê¸°ë³¸ ì •ë¦¬
        prompt = prompt.strip()

        # ğŸ†• ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (í•œêµ­ì–´ ì‘ë‹µ ê°œì„ )
        if not prompt.startswith("<|system|>") and len(prompt) > 20:
            system_prompt = """ë‹¹ì‹ ì€ ë°°í„°ë¦¬ ë° ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
    ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ê¸°ìˆ ì  ë‚´ìš©ì€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

            structured_prompt = f"""<|system|>
    {system_prompt}

    <|user|>
    {prompt}

    <|assistant|>
    """
            return structured_prompt

        return prompt

    def _call_batched(self, prompts: List[str], **kwargs) -> List[str]:
        """ë°°ì¹˜ LLM í˜¸ì¶œ"""
        responses = []

        for prompt in prompts:
            try:
                response = self._call_direct(prompt, **kwargs)
                responses.append(response)
            except Exception as e:
                logger.warning(f"âš ï¸ Batch item failed: {e}")
                responses.append(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

        return responses

    def _call_streaming_sync(self, prompt: str, **kwargs) -> str:
        """ìŠ¤íŠ¸ë¦¬ë°ì„ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬"""
        # í˜„ì¬ LocalLLMManagerëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
        # ì¼ë°˜ í˜¸ì¶œë¡œ í´ë°±
        logger.debug("ğŸ“¡ Streaming not supported, falling back to direct call")
        return self._call_direct(prompt, **kwargs)

    def _retry_call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """ì¬ì‹œë„ ë¡œì§ - FieldInfo ì˜¤ë¥˜ ì™„ì „ í•´ê²°"""

        last_error = None

        for attempt in range(self.max_retries):
            try:
                logger.info(f"ğŸ”„ Retry attempt {attempt + 1}/{self.max_retries}")

                # âœ… FieldInfo ì˜¤ë¥˜ ë°©ì§€ - ì•ˆì „í•œ ê°’ ì¶”ì¶œ
                if attempt > 0:
                    # ê¸°ë³¸ ì§€ì—°ë§Œ ì‚¬ìš© (FieldInfo ì—°ì‚° ì œê±°)
                    base_delay = 1.0  # self.retry_delay ì§ì ‘ ì‚¬ìš© ì•ˆí•¨
                    delay = base_delay * (2 ** (attempt - 1))
                    time.sleep(min(delay, 10.0))

                # âœ… ì¬ì‹œë„ ì‹œ ì•ˆì „í•œ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©
                retry_kwargs = {
                    "max_length": 500,  # ê³ ì •ê°’ ì‚¬ìš© (FieldInfo ì—°ì‚° ë°©ì§€)
                }

                # FieldInfo ê°ì²´ì™€ì˜ ì—°ì‚°ì„ ì™„ì „íˆ í”¼í•¨
                logger.debug(f"ğŸ”§ Retry {attempt + 1} with safe params: {retry_kwargs}")

                response = self._call_direct(prompt, **retry_kwargs)

                # âœ… ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
                if (
                    response
                    and len(response.strip()) > 10
                    and not self._is_meaningless_response(response)
                ):
                    logger.info(f"âœ… Retry successful on attempt {attempt + 1}")
                    return response
                else:
                    logger.warning(
                        f"âš ï¸ Retry {attempt + 1} produced poor quality response"
                    )
                    continue

            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ Retry attempt {attempt + 1} failed: {e}")

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ê°œì„ ëœ í´ë°± ì‘ë‹µ
        logger.error(f"âŒ All retry attempts failed. Last error: {last_error}")
        return self._generate_fallback_response(prompt, str(last_error))

    def _generate_fallback_response(self, prompt: str, error_msg: str) -> str:
        """ê°œì„ ëœ í´ë°± ì‘ë‹µ ìƒì„±"""
        # âœ… ì—ëŸ¬ íƒ€ì…ë³„ ë§ì¶¤ ë©”ì‹œì§€
        if "FieldInfo" in error_msg:
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì„¤ì •ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆì–´ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜, ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ ë°”ê¿”ì„œ ì‹œë„í•´ë³´ì„¸ìš”."
            )
        elif "memory" in error_msg.lower():
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                "ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ì´ê³  ê°„ë‹¨í•˜ê²Œ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )
        elif "timeout" in error_msg.lower():
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                "ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )
        else:
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜, ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•´ ì£¼ì„¸ìš”."
            )

    def _prepare_generation_kwargs(
        self, stop: Optional[List[str]], **kwargs
    ) -> Dict[str, Any]:
        """ìƒì„± íŒŒë¼ë¯¸í„° ì¤€ë¹„ - FieldInfo ì™„ì „ ì œê±°"""

        # âœ… FieldInfo ê°ì²´ ì‚¬ìš© ì™„ì „ ë°©ì§€ - ê³ ì •ê°’ ì‚¬ìš©
        safe_max_tokens = 500  # ê¸°ë³¸ê°’ ê³ ì •

        # kwargsì—ì„œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        if "max_tokens" in kwargs and isinstance(kwargs["max_tokens"], (int, float)):
            safe_max_tokens = int(kwargs["max_tokens"])
        elif "max_length" in kwargs and isinstance(kwargs["max_length"], (int, float)):
            safe_max_tokens = int(kwargs["max_length"])

        # âœ… LocalLLMManagerê°€ ì‹¤ì œë¡œ ë°›ëŠ” íŒŒë¼ë¯¸í„°ë§Œ
        generation_kwargs = {
            "max_length": safe_max_tokens,
            # âŒ temperature, top_p ë“±ì€ ì™„ì „ ì œê±° (transformers ê²½ê³  ì œê±°)
        }

        logger.debug(f"ğŸ”§ Safe generation kwargs: {generation_kwargs}")

        # stop í† í° ê²½ê³ 
        if stop:
            logger.debug(f"âš ï¸ Stop tokens not supported by LocalLLMManager: {stop}")

        return generation_kwargs

    def _enhance_prompt_quality(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ê°œì„ """

        # ê¸°ë³¸ ì •ë¦¬
        prompt = prompt.strip()

        # âœ… í•œêµ­ì–´ ë‹µë³€ì„ ìœ„í•œ ëª…í™•í•œ ì§€ì‹œ ì¶”ê°€
        if not prompt.startswith("<|system|>") and len(prompt) > 20:
            enhanced_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
    ê¸°ìˆ ì  ë‚´ìš©ì€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

    ì§ˆë¬¸: {prompt}

    ë‹µë³€:"""
            return enhanced_prompt

        return prompt

    def _enhance_response_quality(self, response: str) -> str:
        """ì‘ë‹µ í’ˆì§ˆ ê°œì„ """

        if not isinstance(response, str):
            response = str(response)

        # ê¸°ë³¸ ì •ë¦¬
        response = response.strip()

        # âœ… ë°˜ë³µ ë¬¸ì ì œê±° (ê¸°ì¡´ ë¬¸ì œ í•´ê²°)
        import re

        # ê°™ì€ ë¬¸ìê°€ 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ë©´ ì œê±°
        response = re.sub(r"(.)\1{3,}", r"\1", response)

        # ê°™ì€ ë‹¨ì–´ê°€ 2ë²ˆ ì´ìƒ ë°˜ë³µë˜ë©´ ì œê±°
        response = re.sub(r"\b(\w+)(\s+\1)+\b", r"\1", response)

        # ì˜ë¯¸ì—†ëŠ” ê¸°í˜¸ ë°˜ë³µ ì œê±°
        response = re.sub(r"[!]{3,}", "!", response)
        response = re.sub(r"[?]{3,}", "?", response)
        response = re.sub(r"[.]{3,}", "...", response)

        # âœ… ë¹ˆ ì‘ë‹µì´ë‚˜ ë„ˆë¬´ ì§§ì€ ì‘ë‹µ ì²˜ë¦¬
        if not response or len(response.strip()) < 15:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•´ ì£¼ì‹œê² ì–´ìš”?"

        # ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ ì‘ë‹µ ë°©ì§€)
        if len(response) > 1500:
            response = response[:1500] + "..."

        return response

    def _post_process_response(self, response: str) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬ - í’ˆì§ˆ ê²€ì¦ ê°•í™”"""
        if not isinstance(response, str):
            response = str(response)

        # ê¸°ë³¸ ì •ë¦¬
        response = response.strip()

        # ğŸ†• ë°˜ë³µ ë¬¸ì ì œê±° (ê¸°ì¡´ ë¬¸ì œ í•´ê²°)
        import re

        # ê°™ì€ ë¬¸ìê°€ 5ë²ˆ ì´ìƒ ë°˜ë³µë˜ë©´ ì œê±°
        response = re.sub(r"(.)\1{5,}", r"\1", response)

        # ê°™ì€ ë‹¨ì–´ê°€ 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ë©´ ì œê±°
        response = re.sub(r"\b(\w+)(\s+\1){2,}\b", r"\1", response)

        # ğŸ†• ì˜ë¯¸ì—†ëŠ” ì‘ë‹µ ê°ì§€ ë° ì²˜ë¦¬
        if self._is_meaningless_response(response):
            logger.warning("âš ï¸ Detected meaningless response, generating fallback")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•´ ì£¼ì‹œê² ì–´ìš”?"

        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
        if not response or len(response.strip()) < 5:
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

        # ê¸¸ì´ ì œí•œ
        max_chars = self._get_safe_max_tokens() * 4
        if len(response) > max_chars:
            response = response[:max_chars] + "..."
            logger.debug(f"ğŸ“ Response truncated to {max_chars} characters")

        return response

    def _is_meaningless_response(self, response: str) -> bool:
        """ì˜ë¯¸ì—†ëŠ” ì‘ë‹µ ê°ì§€ - ê°œì„ ëœ ë²„ì „"""
        if not response or len(response.strip()) < 15:
            return True

        # âœ… ë°˜ë³µ ë¬¸ì íŒ¨í„´ ê°ì§€
        import re

        # ê°™ì€ ë¬¸ìê°€ 50% ì´ìƒì„ ì°¨ì§€í•˜ëŠ” ê²½ìš°
        char_counts = {}
        for char in response.replace(" ", ""):  # ê³µë°± ì œì™¸
            if char.isalnum():  # ì•ŒíŒŒë²³ê³¼ ìˆ«ìë§Œ
                char_counts[char] = char_counts.get(char, 0) + 1

        if char_counts:
            max_char_count = max(char_counts.values())
            total_chars = sum(char_counts.values())
            if max_char_count > total_chars * 0.5:
                return True

        # âœ… ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ìˆ˜ í™•ì¸
        words = re.findall(r"\w+", response)
        korean_words = re.findall(r"[ê°€-í£]+", response)
        english_words = re.findall(r"[a-zA-Z]+", response)

        # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ê°€ ë„ˆë¬´ ì ì€ ê²½ìš°
        if len(words) < 5 and len(korean_words) < 2 and len(english_words) < 3:
            return True

        # âœ… íŠ¹ì • ë¬´ì˜ë¯¸ íŒ¨í„´ ê°ì§€
        meaningless_patterns = [
            r'^[!@#$%^&*()_+\-=\[\]{};:"\\|,.<>\/?]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r"^[0-9\s]+$",  # ìˆ«ìì™€ ê³µë°±ë§Œ
            r"^(.)\1{10,}",  # ê°™ì€ ë¬¸ì 10ë²ˆ ì´ìƒ ë°˜ë³µ
        ]

        for pattern in meaningless_patterns:
            if re.search(pattern, response.strip()):
                return True

        return False

    def _get_cache_key(self, prompt: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„± - FieldInfo ì‚¬ìš© ë°©ì§€"""
        import hashlib

        # âœ… FieldInfo ê°ì²´ ì‚¬ìš© ë°©ì§€ - ê³ ì •ê°’ ì‚¬ìš©
        safe_temp = 0.1
        safe_max_tokens = 500

        # ì „ì²˜ë¦¬ëœ í”„ë¡¬í”„íŠ¸ë¡œ í‚¤ ìƒì„±
        processed_prompt = self._enhance_prompt_quality(prompt)

        key_data = f"{processed_prompt}_{safe_temp}_{safe_max_tokens}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _get_from_cache(self, prompt: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ"""
        if not self.enable_caching:
            return None

        cache_key = self._get_cache_key(prompt)

        if cache_key in self._cache:
            # TTL í™•ì¸
            cache_time = self._cache_timestamps.get(cache_key, 0)
            current_time = time.time()

            if current_time - cache_time < self.cache_ttl:
                return self._cache[cache_key]
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self._cache[cache_key]
                del self._cache_timestamps[cache_key]

        return None

    def _save_to_cache(self, prompt: str, response: str) -> None:
        """ìºì‹œì— ì‘ë‹µ ì €ì¥ - í’ˆì§ˆ ê²€ì¦ ì¶”ê°€"""
        if not self.enable_caching:
            return

        # âœ… ì˜ë¯¸ì—†ëŠ” ì‘ë‹µì€ ìºì‹œí•˜ì§€ ì•ŠìŒ
        if self._is_meaningless_response(response):
            logger.debug("ğŸš« Not caching meaningless response")
            return

        # âœ… í´ë°± ì‘ë‹µë„ ìºì‹œí•˜ì§€ ì•ŠìŒ
        if "ì£„ì†¡í•©ë‹ˆë‹¤" in response and "ì˜¤ë¥˜" in response:
            logger.debug("ğŸš« Not caching error response")
            return

        cache_key = self._get_cache_key(prompt)

        self._cache[cache_key] = response
        self._cache_timestamps[cache_key] = time.time()

        # ìºì‹œ í¬ê¸° ê´€ë¦¬
        if len(self._cache) > 50:  # ë” ì‘ì€ ìºì‹œ í¬ê¸°ë¡œ ê´€ë¦¬
            # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ 10ê°œ ì œê±°
            sorted_items = sorted(self._cache_timestamps.items(), key=lambda x: x[1])
            for key, _ in sorted_items[:10]:
                if key in self._cache:
                    del self._cache[key]
                if key in self._cache_timestamps:
                    del self._cache_timestamps[key]

            logger.debug(f"ğŸ—‘ï¸ Cache cleaned, now {len(self._cache)} items")

    # ========================================================================
    # ë¹„ë™ê¸° ë©”ì„œë“œë“¤ (LangChain í˜¸í™˜ì„±)
    # ========================================================================

    async def _acall(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """ë¹„ë™ê¸° LLM í˜¸ì¶œ"""

        # í˜„ì¬ëŠ” ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        loop = asyncio.get_event_loop()

        # ë™ê¸° ì½œë°± ë§¤ë‹ˆì €ë¡œ ë³€í™˜
        sync_run_manager = None

        return await loop.run_in_executor(
            None, self._call, prompt, stop, sync_run_manager, **kwargs
        )

    # ========================================================================
    # ìŠ¤íŠ¸ë¦¬ë° ì§€ì› (í–¥í›„ í™•ì¥ìš©)
    # ========================================================================

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (í˜„ì¬ëŠ” ë¯¸ì§€ì›)"""

        # í˜„ì¬ LocalLLMManagerëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
        # ì¼ë°˜ ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ë°˜í™˜
        logger.debug("ğŸ“¡ Streaming not fully supported, simulating...")

        response = self._call(prompt, stop, run_manager, **kwargs)

        # ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        chunk_size = 50
        for i in range(0, len(response), chunk_size):
            chunk = response[i : i + chunk_size]
            yield GenerationChunk(text=chunk)

    async def _astream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> AsyncIterator[GenerationChunk]:
        """ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""

        # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        loop = asyncio.get_event_loop()

        for chunk in self._stream(prompt, stop, None, **kwargs):
            yield chunk
            await asyncio.sleep(0)  # ì´ë²¤íŠ¸ ë£¨í”„ ì–‘ë³´

    # ========================================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ========================================================================

    def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ ë°˜í™˜ - ë” ìƒì„¸í•œ ì •ë³´"""

        cache_efficiency = (
            self._stats.cache_hits / max(1, self._stats.total_calls)
        ) * 100

        success_rate = (
            (self._stats.total_calls - self._stats.failed_calls)
            / max(1, self._stats.total_calls)
        ) * 100

        return {
            "total_calls": self._stats.total_calls,
            "total_tokens": self._stats.total_tokens,
            "total_time": round(self._stats.total_time, 2),
            "average_time": round(self._stats.average_time, 2),
            "cache_hits": self._stats.cache_hits,
            "failed_calls": self._stats.failed_calls,
            "cache_efficiency": f"{cache_efficiency:.1f}%",
            "success_rate": f"{success_rate:.1f}%",
            # ğŸ†• ì¶”ê°€ ë©”íŠ¸ë¦­
            "cache_size": len(self._cache),
            "avg_tokens_per_call": (
                self._stats.total_tokens / max(1, self._stats.total_calls)
            ),
            "calls_per_minute": (
                (self._stats.total_calls / max(1, self._stats.total_time / 60))
                if self._stats.total_time > 0
                else 0
            ),
        }

    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self._cache_timestamps.clear()
        logger.info("ğŸ—‘ï¸ LLM adapter cache cleared")

    def update_config(self, **kwargs) -> None:
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
                logger.info(f"ğŸ“ Updated {key} = {value}")

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ - ì•ˆì „í•œ ê°’ ì¶”ì¶œ"""
        return {
            "adapter_type": self._llm_type,
            "mode": self.mode.value if hasattr(self.mode, "value") else str(self.mode),
            "temperature": self._get_safe_temperature(),
            "max_tokens": self._get_safe_max_tokens(),
            "caching_enabled": getattr(self, "enable_caching", True),
            "is_loaded": getattr(self, "_is_loaded", False),
            "llm_manager_loaded": (
                self.llm_manager.is_loaded if self.llm_manager else False
            ),
            "model_path": (
                getattr(
                    getattr(self.llm_manager, "config", None), "model_path", "unknown"
                )
                if self.llm_manager
                else "unknown"
            ),
        }

    def health_check(self) -> Dict[str, Any]:
        """ê°œì„ ëœ í—¬ìŠ¤ì²´í¬"""
        try:
            start_time = time.time()

            # âœ… ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ í…ŒìŠ¤íŠ¸
            test_response = self._call_direct("í…ŒìŠ¤íŠ¸", max_length=50)

            response_time = time.time() - start_time
            is_healthy = (
                test_response
                and len(test_response.strip()) > 5
                and not self._is_meaningless_response(test_response)
                and response_time < 60.0  # 1ë¶„ ì´ë‚´
            )

            return {
                "status": "healthy" if is_healthy else "degraded",
                "response_time": round(response_time, 2),
                "test_response_quality": (
                    "good"
                    if not self._is_meaningless_response(test_response)
                    else "poor"
                ),
                "cache_size": len(self._cache),
                "total_calls": self._stats.total_calls,
                "success_rate": f"{((self._stats.total_calls - self._stats.failed_calls) / max(1, self._stats.total_calls)) * 100:.1f}%",
                "recommendations": self._get_performance_recommendations(
                    is_healthy, response_time
                ),
            }

        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "recommendations": ["ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."],
            }

    def _get_health_recommendations(
        self, is_healthy: bool, response_time: float
    ) -> List[str]:
        """í—¬ìŠ¤ì²´í¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­"""
        recommendations = []

        if not is_healthy:
            recommendations.append(
                "ëª¨ë¸ ì‘ë‹µ í’ˆì§ˆì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•´ë³´ì„¸ìš”."
            )

        if response_time > 20:
            recommendations.append(
                "ì‘ë‹µ ì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ë‚˜ ëª¨ë¸ í¬ê¸°ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
            )

        if len(self._cache) == 0:
            recommendations.append(
                "ìºì‹œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìì£¼ ì‚¬ìš©í•˜ëŠ” ì¿¼ë¦¬ë¡œ ìºì‹œë¥¼ ì›Œë°ì—…í•˜ì„¸ìš”."
            )

        cache_hit_rate = self._stats.cache_hits / max(1, self._stats.total_calls)
        if cache_hit_rate < 0.1:
            recommendations.append(
                "ìºì‹œ íš¨ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ìºì‹œ TTL ì„¤ì •ì„ í™•ì¸í•´ë³´ì„¸ìš”."
            )

        if not recommendations:
            recommendations.append("ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")

        return recommendations

    def optimize_for_performance(self) -> None:
        """ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰"""

        logger.info("âš¡ Optimizing LLM adapter for performance...")

        # ìºì‹œ ìµœì í™”
        if len(self._cache) > 30:
            # í’ˆì§ˆì´ ë‚®ì€ ìºì‹œ í•­ëª© ì œê±°
            poor_quality_keys = []
            for key, response in self._cache.items():
                if self._is_meaningless_response(response):
                    poor_quality_keys.append(key)

            for key in poor_quality_keys:
                if key in self._cache:
                    del self._cache[key]
                if key in self._cache_timestamps:
                    del self._cache_timestamps[key]

            logger.info(f"ğŸ—‘ï¸ Removed {len(poor_quality_keys)} poor quality cache items")

        # í†µê³„ ë¦¬ì…‹ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if self._stats.total_calls > 100:
            self._stats = LLMUsageStats()
            logger.info("ğŸ“Š Reset statistics for memory optimization")

        logger.info("âœ… Performance optimization completed")


# ============================================================================
# íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================


def create_llm_adapter(
    config_manager: "GraphRAGConfigManager",
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    mode: Union[AdapterMode, str] = AdapterMode.DIRECT,
    **kwargs,
) -> GraphRAGLLMAdapter:
    """LLM ì–´ëŒ‘í„° íŒ©í† ë¦¬ í•¨ìˆ˜ (YAML ì„¤ì • í˜¸í™˜) - ì•ˆì „í•œ ê°’ ì „ë‹¬"""

    # LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    llm_config = config_manager.get_llm_config()

    # LocalLLMManager ìƒì„±
    from ..graphrag_pipeline import LocalLLMManager

    llm_manager = LocalLLMManager(llm_config)

    # ëª¨ë“œ ë³€í™˜
    if isinstance(mode, str):
        try:
            mode = AdapterMode(mode)
        except ValueError:
            logger.warning(f"âš ï¸ Unknown mode: {mode}, using DIRECT")
            mode = AdapterMode.DIRECT

    # âœ… ì•ˆì „í•œ ê°’ ì¶”ì¶œ ë° ê²€ì¦
    safe_temperature = (
        temperature if temperature is not None else llm_config.get("temperature", 0.1)
    )
    safe_max_tokens = (
        max_tokens if max_tokens is not None else llm_config.get("max_new_tokens", 1000)
    )

    # ê°’ íƒ€ì… ê²€ì¦
    if not isinstance(safe_temperature, (int, float)):
        safe_temperature = 0.1
        logger.warning("âš ï¸ Invalid temperature in config, using 0.1")

    if not isinstance(safe_max_tokens, (int, float)):
        safe_max_tokens = 1000
        logger.warning("âš ï¸ Invalid max_tokens in config, using 1000")

    # ì–´ëŒ‘í„° ì„¤ì •
    adapter_config = {
        "llm_manager": llm_manager,
        "temperature": float(safe_temperature),
        "max_tokens": int(safe_max_tokens),
        "mode": mode,
        **kwargs,
    }

    adapter = GraphRAGLLMAdapter(**adapter_config)

    logger.info(f"âœ… LLM adapter created from YAML config")
    logger.info(f"   Provider: {llm_config.get('provider')}")
    logger.info(f"   Temperature: {safe_temperature}")
    logger.info(f"   Max tokens: {safe_max_tokens}")
    logger.info(f"   Mode: {mode.value}")

    return adapter


def create_llm_adapter_from_manager(
    llm_manager: LocalLLMManager,
    temperature: float = 0.1,
    max_tokens: int = 1000,
    mode: Union[AdapterMode, str] = AdapterMode.DIRECT,
    **kwargs,
) -> GraphRAGLLMAdapter:
    """ê¸°ì¡´ LLM ë§¤ë‹ˆì €ë¡œë¶€í„° ì–´ëŒ‘í„° ìƒì„±"""

    if isinstance(mode, str):
        mode = AdapterMode(mode)

    return GraphRAGLLMAdapter(
        llm_manager=llm_manager,
        temperature=temperature,
        max_tokens=max_tokens,
        mode=mode,
        **kwargs,
    )


def main():
    """LLM ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸"""

    if not _langchain_available:
        print("âŒ LangChain not available for testing")
        return

    print("ğŸ§ª Testing GraphRAG LLM Adapter...")

    try:
        # Mock LLM Manager ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        class MockLLMManager:
            def __init__(self):
                self.config = type(
                    "Config",
                    (),
                    {
                        "model_path": "test-model",
                        "temperature": 0.1,
                        "max_new_tokens": 100,
                    },
                )()
                self.is_loaded = False

            def load_model(self):
                self.is_loaded = True
                print("ğŸ“¥ Mock LLM loaded")

            def generate(self, prompt, max_length=100):
                return f"Mock response to: {prompt[:30]}..."

        # ì–´ëŒ‘í„° ìƒì„±
        mock_manager = MockLLMManager()
        adapter = GraphRAGLLMAdapter(
            llm_manager=mock_manager,
            temperature=0.1,
            max_tokens=100,
            enable_caching=True,
        )

        print(f"âœ… Adapter created")

        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = adapter.get_model_info()
        print(f"ğŸ“Š Model info:")
        for key, value in model_info.items():
            print(f"   {key}: {value}")

        # í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤."
        print(f"\nğŸ” Testing prompt: {test_prompt}")

        try:
            response = adapter._call(test_prompt)
            print(f"âœ… Response: {response}")

            # ì‚¬ìš© í†µê³„ í™•ì¸
            stats = adapter.get_usage_stats()
            print(f"ğŸ“ˆ Usage stats:")
            for key, value in stats.items():
                print(f"   {key}: {value}")

        except Exception as e:
            print(f"âš ï¸ Call test skipped: {e}")

        # ìƒíƒœ í™•ì¸
        health = adapter.health_check()
        print(f"\nğŸ¥ Health check:")
        for key, value in health.items():
            print(f"   {key}: {value}")

        print(f"\nâœ… GraphRAG LLM Adapter test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
