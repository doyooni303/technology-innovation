"""
GraphRAG LangChain LLM ì–´ëŒ‘í„°
LangChain LLM Adapter for GraphRAG System

GraphRAGì˜ LocalLLMManagerë¥¼ LangChain BaseLanguageModelë¡œ ë³€í™˜
- YAML ì„¤ì • ì™„ì „ í˜¸í™˜
- ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
"""

import asyncio
import logging
import warnings
from typing import Any, Dict, List, Optional, Union, Iterator, AsyncIterator, Mapping
from dataclasses import dataclass
from enum import Enum
import time

# LangChain imports
try:
    from langchain_core.language_models.llms import LLM
    from langchain_core.language_models.base import BaseLanguageModel
    from langchain_core.outputs import Generation, LLMResult, GenerationChunk
    from langchain_core.callbacks.manager import (
        CallbackManagerForLLMRun,
        AsyncCallbackManagerForLLMRun,
    )
    from langchain_core.pydantic_v1 import Field, root_validator

    _langchain_available = True
except ImportError:
    _langchain_available = False
    warnings.warn("LangChain not available for LLM adapter")

# GraphRAG imports
try:
    from ..graphrag_pipeline import LocalLLMManager
    from ..config_manager import GraphRAGConfigManager
except ImportError as e:
    warnings.warn(f"GraphRAG pipeline components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class AdapterMode(Enum):
    """ì–´ëŒ‘í„° ëª¨ë“œ"""

    DIRECT = "direct"  # ì§ì ‘ í˜¸ì¶œ
    BATCHED = "batched"  # ë°°ì¹˜ ì²˜ë¦¬
    STREAMING = "streaming"  # ìŠ¤íŠ¸ë¦¬ë°
    CACHED = "cached"  # ìºì‹± í™œìš©


@dataclass
class LLMUsageStats:
    """LLM ì‚¬ìš© í†µê³„"""

    total_calls: int = 0
    total_tokens: int = 0
    total_time: float = 0.0
    average_time: float = 0.0
    cache_hits: int = 0
    failed_calls: int = 0

    def update_call(self, tokens: int, call_time: float, from_cache: bool = False):
        """í˜¸ì¶œ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_calls += 1
        if not from_cache:
            self.total_tokens += tokens
        self.total_time += call_time
        self.average_time = self.total_time / self.total_calls
        if from_cache:
            self.cache_hits += 1

    def update_failure(self):
        """ì‹¤íŒ¨ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.failed_calls += 1


class GraphRAGLLMAdapter(LLM):
    """ìˆ˜ì •ëœ GraphRAG LLM Adapter - FieldInfo ì˜¤ë¥˜ í•´ê²°"""

    # LangChain Pydantic í•„ë“œë“¤
    llm_manager: Any = Field(description="GraphRAG LocalLLMManager instance")
    temperature: float = Field(default=0.1, description="Generation temperature")
    max_tokens: int = Field(default=1000, description="Maximum tokens to generate")

    # ì–´ëŒ‘í„° ì„¤ì •
    mode: AdapterMode = Field(default=AdapterMode.DIRECT, description="Adapter mode")
    enable_caching: bool = Field(default=True, description="Enable response caching")
    cache_ttl: int = Field(default=3600, description="Cache TTL in seconds")
    max_retries: int = Field(default=3, description="Maximum retry attempts")
    retry_delay: float = Field(default=1.0, description="Delay between retries")

    # ë‚´ë¶€ ìƒíƒœ (exclude=Trueë¡œ ì§ë ¬í™”ì—ì„œ ì œì™¸)
    _cache: Dict[str, Any] = Field(default_factory=dict, exclude=True)
    _cache_timestamps: Dict[str, float] = Field(default_factory=dict, exclude=True)
    _stats: LLMUsageStats = Field(default_factory=LLMUsageStats, exclude=True)
    _is_loaded: bool = Field(default=False, exclude=True)

    class Config:
        """Pydantic ì„¤ì •"""

        arbitrary_types_allowed = True

    @root_validator(pre=True)
    def validate_llm_manager(cls, values):
        """LLM ë§¤ë‹ˆì € ê²€ì¦"""
        llm_manager = values.get("llm_manager")
        if llm_manager is None:
            raise ValueError("llm_manager is required")
        return values

    def __init__(self, **kwargs):
        """ì–´ëŒ‘í„° ì´ˆê¸°í™”"""
        super().__init__(**kwargs)

        if not _langchain_available:
            raise ImportError("LangChain is required for LLM adapter")

        # âœ… Pydantic Field ê°’ë“¤ì„ ì•ˆì „í•˜ê²Œ ê²€ì¦
        self._validate_field_values()

        # í†µê³„ ì´ˆê¸°í™”
        self._stats = LLMUsageStats()
        self._cache = {}
        self._cache_timestamps = {}
        self._is_loaded = False

        logger.info("âœ… GraphRAGLLMAdapter initialized")
        logger.info(f"   ğŸŒ¡ï¸ Temperature: {self._get_safe_temperature()}")
        logger.info(f"   ğŸ“ Max tokens: {self._get_safe_max_tokens()}")
        logger.info(
            f"   ğŸ”§ Mode: {self.mode.value if hasattr(self.mode, 'value') else self.mode}"
        )
        logger.info(f"   ğŸ’¾ Caching: {self.enable_caching}")

    def _validate_field_values(self):
        """Pydantic Field ê°’ë“¤ì„ ê²€ì¦í•˜ê³  ìˆ˜ì •"""

        # temperature ê²€ì¦
        if not isinstance(getattr(self, "temperature", None), (int, float)):
            logger.warning("âš ï¸ Invalid temperature field, setting to 0.1")
            object.__setattr__(self, "temperature", 0.1)

        # max_tokens ê²€ì¦
        if not isinstance(getattr(self, "max_tokens", None), (int, float)):
            logger.warning("âš ï¸ Invalid max_tokens field, setting to 1000")
            object.__setattr__(self, "max_tokens", 1000)

        # mode ê²€ì¦
        if not hasattr(getattr(self, "mode", None), "value"):
            logger.warning("âš ï¸ Invalid mode field, setting to DIRECT")
            from enum import Enum

            if hasattr(self, "mode") and isinstance(self.mode, str):
                # ë¬¸ìì—´ì„ Enumìœ¼ë¡œ ë³€í™˜ ì‹œë„
                try:
                    object.__setattr__(self, "mode", AdapterMode(self.mode))
                except (ValueError, AttributeError):
                    object.__setattr__(self, "mode", AdapterMode.DIRECT)
            else:
                object.__setattr__(self, "mode", AdapterMode.DIRECT)

    def _get_safe_temperature(self) -> float:
        """ì•ˆì „í•œ temperature ê°’ ë°˜í™˜"""
        temp = getattr(self, "temperature", 0.1)
        return float(temp) if isinstance(temp, (int, float)) else 0.1

    def _get_safe_max_tokens(self) -> int:
        """ì•ˆì „í•œ max_tokens ê°’ ë°˜í™˜"""
        tokens = getattr(self, "max_tokens", 1000)
        return int(tokens) if isinstance(tokens, (int, float)) else 1000

    @property
    def _llm_type(self) -> str:
        """LangChain LLM íƒ€ì… ì‹ë³„ì"""
        return "graphrag_llm"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """LangChainì´ ì‚¬ìš©í•˜ëŠ” ì‹ë³„ íŒŒë¼ë¯¸í„° - ì•ˆì „í•œ ê°’ ì¶”ì¶œ"""
        return {
            "temperature": self._get_safe_temperature(),
            "max_tokens": self._get_safe_max_tokens(),
            "mode": self.mode.value if hasattr(self.mode, "value") else str(self.mode),
            "model_path": getattr(
                getattr(self.llm_manager, "config", None), "model_path", "unknown"
            ),
        }

    def _ensure_loaded(self) -> None:
        """LLM ë¡œë“œ í™•ì¸ ë° ì§€ì—° ë¡œë”©"""
        if not self._is_loaded and not self.llm_manager.is_loaded:
            logger.info("ğŸ“¥ Loading LLM model (lazy loading)...")
            try:
                self.llm_manager.load_model()
                self._is_loaded = True
                logger.info("âœ… LLM model loaded successfully")
            except Exception as e:
                logger.error(f"âŒ Failed to load LLM model: {e}")
                raise

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """ë™ê¸° LLM í˜¸ì¶œ (LangChain ì¸í„°í˜ì´ìŠ¤)"""

        start_time = time.time()

        try:
            # 1. ìºì‹œ í™•ì¸
            if self.enable_caching:
                cached_response = self._get_from_cache(prompt)
                if cached_response is not None:
                    call_time = time.time() - start_time
                    self._stats.update_call(
                        tokens=len(cached_response.split()),
                        call_time=call_time,
                        from_cache=True,
                    )
                    logger.debug(f"ğŸ’¾ Cache hit for prompt: {prompt[:50]}...")
                    return cached_response

            # 2. LLM ë¡œë“œ í™•ì¸
            self._ensure_loaded()

            # 3. íŒŒë¼ë¯¸í„° ì¤€ë¹„
            generation_kwargs = self._prepare_generation_kwargs(stop, **kwargs)

            # 4. ëª¨ë“œë³„ ì²˜ë¦¬
            if self.mode == AdapterMode.BATCHED:
                response = self._call_batched([prompt], **generation_kwargs)[0]
            elif self.mode == AdapterMode.STREAMING:
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ë™ê¸°ë¡œ ë³€í™˜
                response = self._call_streaming_sync(prompt, **generation_kwargs)
            else:  # DIRECT ë˜ëŠ” CACHED
                response = self._call_direct(prompt, **generation_kwargs)

            # 5. ì‘ë‹µ í›„ì²˜ë¦¬
            response = self._post_process_response(response)

            # 6. ìºì‹œ ì €ì¥
            if self.enable_caching:
                self._save_to_cache(prompt, response)

            # 7. í†µê³„ ì—…ë°ì´íŠ¸
            call_time = time.time() - start_time
            self._stats.update_call(tokens=len(response.split()), call_time=call_time)

            # 8. ì½œë°± í˜¸ì¶œ (LangChain)
            if run_manager:
                run_manager.on_llm_end(
                    LLMResult(
                        generations=[[Generation(text=response)]],
                        llm_output={"model_name": self._llm_type},
                    )
                )

            logger.debug(f"âœ… LLM call completed in {call_time:.2f}s")
            return response

        except Exception as e:
            self._stats.update_failure()
            logger.error(f"âŒ LLM call failed: {e}")

            # ì¬ì‹œë„ ë¡œì§
            if self.max_retries > 0:
                return self._retry_call(prompt, stop, run_manager, **kwargs)
            else:
                raise

    def _call_direct(self, prompt: str, **kwargs) -> str:
        """ì§ì ‘ LLM í˜¸ì¶œ"""
        try:
            response = self.llm_manager.generate(
                prompt=prompt, max_length=kwargs.get("max_length", self.max_tokens)
            )

            if not isinstance(response, str):
                logger.warning(f"âš ï¸ LLM returned non-string: {type(response)}")
                response = str(response)

            return response

        except Exception as e:
            logger.error(f"âŒ Direct LLM call failed: {e}")
            raise

    def _call_batched(self, prompts: List[str], **kwargs) -> List[str]:
        """ë°°ì¹˜ LLM í˜¸ì¶œ"""
        responses = []

        for prompt in prompts:
            try:
                response = self._call_direct(prompt, **kwargs)
                responses.append(response)
            except Exception as e:
                logger.warning(f"âš ï¸ Batch item failed: {e}")
                responses.append(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

        return responses

    def _call_streaming_sync(self, prompt: str, **kwargs) -> str:
        """ìŠ¤íŠ¸ë¦¬ë°ì„ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬"""
        # í˜„ì¬ LocalLLMManagerëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
        # ì¼ë°˜ í˜¸ì¶œë¡œ í´ë°±
        logger.debug("ğŸ“¡ Streaming not supported, falling back to direct call")
        return self._call_direct(prompt, **kwargs)

    def _retry_call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """ì¬ì‹œë„ ë¡œì§"""

        last_error = None

        for attempt in range(self.max_retries):
            try:
                logger.info(f"ğŸ”„ Retry attempt {attempt + 1}/{self.max_retries}")

                # ì¬ì‹œë„ ì§€ì—°
                if attempt > 0:
                    time.sleep(self.retry_delay * attempt)

                # ê°„ë‹¨í•œ ì§ì ‘ í˜¸ì¶œë¡œ ì¬ì‹œë„
                response = self._call_direct(prompt, max_length=self.max_tokens)

                logger.info(f"âœ… Retry successful on attempt {attempt + 1}")
                return response

            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ Retry attempt {attempt + 1} failed: {e}")

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ
        logger.error(f"âŒ All retry attempts failed. Last error: {last_error}")
        return self._generate_fallback_response(prompt, str(last_error))

    def _generate_fallback_response(self, prompt: str, error_msg: str) -> str:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        return (
            f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            f"ì˜¤ë¥˜: {error_msg}\n\n"
            f"ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

    def _prepare_generation_kwargs(
        self, stop: Optional[List[str]], **kwargs
    ) -> Dict[str, Any]:
        """ìƒì„± íŒŒë¼ë¯¸í„° ì¤€ë¹„ - FieldInfo ì˜¤ë¥˜ ìˆ˜ì •"""

        # âœ… Pydantic Field ê°’ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        max_tokens_value = getattr(self, "max_tokens", 1000)
        temperature_value = getattr(self, "temperature", 0.1)

        # FieldInfo ê°ì²´ì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not isinstance(max_tokens_value, (int, float)):
            max_tokens_value = 1000
            logger.warning("âš ï¸ max_tokens is not a numeric value, using default 1000")

        if not isinstance(temperature_value, (int, float)):
            temperature_value = 0.1
            logger.warning("âš ï¸ temperature is not a numeric value, using default 0.1")

        generation_kwargs = {
            "max_length": kwargs.get("max_tokens", max_tokens_value),
            "temperature": kwargs.get("temperature", temperature_value),
        }

        # stop í† í°ì€ í˜„ì¬ LocalLLMManagerì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
        if stop:
            logger.debug(f"âš ï¸ Stop tokens not supported: {stop}")

        logger.debug(f"ğŸ”§ Generation kwargs: {generation_kwargs}")
        return generation_kwargs

    def _post_process_response(self, response: str) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        if not isinstance(response, str):
            response = str(response)

        # ê¸°ë³¸ ì •ë¦¬
        response = response.strip()

        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
        if not response:
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

        # ê¸¸ì´ ì œí•œ
        max_chars = self.max_tokens * 4  # ëŒ€ëµì  ì¶”ì •
        if len(response) > max_chars:
            response = response[:max_chars] + "..."
            logger.debug(f"ğŸ“ Response truncated to {max_chars} characters")

        return response

    def _get_cache_key(self, prompt: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib

        key_data = f"{prompt}_{self.temperature}_{self.max_tokens}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _get_from_cache(self, prompt: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ"""
        if not self.enable_caching:
            return None

        cache_key = self._get_cache_key(prompt)

        if cache_key in self._cache:
            # TTL í™•ì¸
            cache_time = self._cache_timestamps.get(cache_key, 0)
            current_time = time.time()

            if current_time - cache_time < self.cache_ttl:
                return self._cache[cache_key]
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self._cache[cache_key]
                del self._cache_timestamps[cache_key]

        return None

    def _save_to_cache(self, prompt: str, response: str) -> None:
        """ìºì‹œì— ì‘ë‹µ ì €ì¥"""
        if not self.enable_caching:
            return

        cache_key = self._get_cache_key(prompt)

        self._cache[cache_key] = response
        self._cache_timestamps[cache_key] = time.time()

        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self._cache) > 100:
            # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì œê±°
            oldest_key = min(self._cache_timestamps, key=self._cache_timestamps.get)
            del self._cache[oldest_key]
            del self._cache_timestamps[oldest_key]

    # ========================================================================
    # ë¹„ë™ê¸° ë©”ì„œë“œë“¤ (LangChain í˜¸í™˜ì„±)
    # ========================================================================

    async def _acall(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """ë¹„ë™ê¸° LLM í˜¸ì¶œ"""

        # í˜„ì¬ëŠ” ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        loop = asyncio.get_event_loop()

        # ë™ê¸° ì½œë°± ë§¤ë‹ˆì €ë¡œ ë³€í™˜
        sync_run_manager = None

        return await loop.run_in_executor(
            None, self._call, prompt, stop, sync_run_manager, **kwargs
        )

    # ========================================================================
    # ìŠ¤íŠ¸ë¦¬ë° ì§€ì› (í–¥í›„ í™•ì¥ìš©)
    # ========================================================================

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (í˜„ì¬ëŠ” ë¯¸ì§€ì›)"""

        # í˜„ì¬ LocalLLMManagerëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
        # ì¼ë°˜ ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ë°˜í™˜
        logger.debug("ğŸ“¡ Streaming not fully supported, simulating...")

        response = self._call(prompt, stop, run_manager, **kwargs)

        # ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
        chunk_size = 50
        for i in range(0, len(response), chunk_size):
            chunk = response[i : i + chunk_size]
            yield GenerationChunk(text=chunk)

    async def _astream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> AsyncIterator[GenerationChunk]:
        """ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""

        # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        loop = asyncio.get_event_loop()

        for chunk in self._stream(prompt, stop, None, **kwargs):
            yield chunk
            await asyncio.sleep(0)  # ì´ë²¤íŠ¸ ë£¨í”„ ì–‘ë³´

    # ========================================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # ========================================================================

    def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ ë°˜í™˜"""
        return {
            "total_calls": self._stats.total_calls,
            "total_tokens": self._stats.total_tokens,
            "total_time": round(self._stats.total_time, 2),
            "average_time": round(self._stats.average_time, 2),
            "cache_hits": self._stats.cache_hits,
            "failed_calls": self._stats.failed_calls,
            "cache_hit_ratio": (
                self._stats.cache_hits / max(1, self._stats.total_calls)
            ),
            "success_rate": (
                (self._stats.total_calls - self._stats.failed_calls)
                / max(1, self._stats.total_calls)
            ),
        }

    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self._cache_timestamps.clear()
        logger.info("ğŸ—‘ï¸ LLM adapter cache cleared")

    def update_config(self, **kwargs) -> None:
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
                logger.info(f"ğŸ“ Updated {key} = {value}")

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ - ì•ˆì „í•œ ê°’ ì¶”ì¶œ"""
        return {
            "adapter_type": self._llm_type,
            "mode": self.mode.value if hasattr(self.mode, "value") else str(self.mode),
            "temperature": self._get_safe_temperature(),
            "max_tokens": self._get_safe_max_tokens(),
            "caching_enabled": getattr(self, "enable_caching", True),
            "is_loaded": getattr(self, "_is_loaded", False),
            "llm_manager_loaded": (
                self.llm_manager.is_loaded if self.llm_manager else False
            ),
            "model_path": (
                getattr(
                    getattr(self.llm_manager, "config", None), "model_path", "unknown"
                )
                if self.llm_manager
                else "unknown"
            ),
        }

    def health_check(self) -> Dict[str, Any]:
        """ì–´ëŒ‘í„° ìƒíƒœ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
            test_response = self._call("Test", max_tokens=10)
            is_healthy = len(test_response) > 0

            return {
                "status": "healthy" if is_healthy else "unhealthy",
                "model_loaded": self._is_loaded,
                "llm_manager_available": self.llm_manager is not None,
                "test_response_length": len(test_response),
                "last_check": time.time(),
            }

        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "model_loaded": self._is_loaded,
                "llm_manager_available": self.llm_manager is not None,
                "last_check": time.time(),
            }


# ============================================================================
# íŒ©í† ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================


def create_llm_adapter(
    config_manager: "GraphRAGConfigManager",
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    mode: Union[AdapterMode, str] = AdapterMode.DIRECT,
    **kwargs,
) -> GraphRAGLLMAdapter:
    """LLM ì–´ëŒ‘í„° íŒ©í† ë¦¬ í•¨ìˆ˜ (YAML ì„¤ì • í˜¸í™˜) - ì•ˆì „í•œ ê°’ ì „ë‹¬"""

    # LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    llm_config = config_manager.get_llm_config()

    # LocalLLMManager ìƒì„±
    from ..graphrag_pipeline import LocalLLMManager

    llm_manager = LocalLLMManager(llm_config)

    # ëª¨ë“œ ë³€í™˜
    if isinstance(mode, str):
        try:
            mode = AdapterMode(mode)
        except ValueError:
            logger.warning(f"âš ï¸ Unknown mode: {mode}, using DIRECT")
            mode = AdapterMode.DIRECT

    # âœ… ì•ˆì „í•œ ê°’ ì¶”ì¶œ ë° ê²€ì¦
    safe_temperature = (
        temperature if temperature is not None else llm_config.get("temperature", 0.1)
    )
    safe_max_tokens = (
        max_tokens if max_tokens is not None else llm_config.get("max_new_tokens", 1000)
    )

    # ê°’ íƒ€ì… ê²€ì¦
    if not isinstance(safe_temperature, (int, float)):
        safe_temperature = 0.1
        logger.warning("âš ï¸ Invalid temperature in config, using 0.1")

    if not isinstance(safe_max_tokens, (int, float)):
        safe_max_tokens = 1000
        logger.warning("âš ï¸ Invalid max_tokens in config, using 1000")

    # ì–´ëŒ‘í„° ì„¤ì •
    adapter_config = {
        "llm_manager": llm_manager,
        "temperature": float(safe_temperature),
        "max_tokens": int(safe_max_tokens),
        "mode": mode,
        **kwargs,
    }

    adapter = GraphRAGLLMAdapter(**adapter_config)

    logger.info(f"âœ… LLM adapter created from YAML config")
    logger.info(f"   Provider: {llm_config.get('provider')}")
    logger.info(f"   Temperature: {safe_temperature}")
    logger.info(f"   Max tokens: {safe_max_tokens}")
    logger.info(f"   Mode: {mode.value}")

    return adapter


def create_llm_adapter_from_manager(
    llm_manager: LocalLLMManager,
    temperature: float = 0.1,
    max_tokens: int = 1000,
    mode: Union[AdapterMode, str] = AdapterMode.DIRECT,
    **kwargs,
) -> GraphRAGLLMAdapter:
    """ê¸°ì¡´ LLM ë§¤ë‹ˆì €ë¡œë¶€í„° ì–´ëŒ‘í„° ìƒì„±"""

    if isinstance(mode, str):
        mode = AdapterMode(mode)

    return GraphRAGLLMAdapter(
        llm_manager=llm_manager,
        temperature=temperature,
        max_tokens=max_tokens,
        mode=mode,
        **kwargs,
    )


def main():
    """LLM ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸"""

    if not _langchain_available:
        print("âŒ LangChain not available for testing")
        return

    print("ğŸ§ª Testing GraphRAG LLM Adapter...")

    try:
        # Mock LLM Manager ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        class MockLLMManager:
            def __init__(self):
                self.config = type(
                    "Config",
                    (),
                    {
                        "model_path": "test-model",
                        "temperature": 0.1,
                        "max_new_tokens": 100,
                    },
                )()
                self.is_loaded = False

            def load_model(self):
                self.is_loaded = True
                print("ğŸ“¥ Mock LLM loaded")

            def generate(self, prompt, max_length=100):
                return f"Mock response to: {prompt[:30]}..."

        # ì–´ëŒ‘í„° ìƒì„±
        mock_manager = MockLLMManager()
        adapter = GraphRAGLLMAdapter(
            llm_manager=mock_manager,
            temperature=0.1,
            max_tokens=100,
            enable_caching=True,
        )

        print(f"âœ… Adapter created")

        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = adapter.get_model_info()
        print(f"ğŸ“Š Model info:")
        for key, value in model_info.items():
            print(f"   {key}: {value}")

        # í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤."
        print(f"\nğŸ” Testing prompt: {test_prompt}")

        try:
            response = adapter._call(test_prompt)
            print(f"âœ… Response: {response}")

            # ì‚¬ìš© í†µê³„ í™•ì¸
            stats = adapter.get_usage_stats()
            print(f"ğŸ“ˆ Usage stats:")
            for key, value in stats.items():
                print(f"   {key}: {value}")

        except Exception as e:
            print(f"âš ï¸ Call test skipped: {e}")

        # ìƒíƒœ í™•ì¸
        health = adapter.health_check()
        print(f"\nğŸ¥ Health check:")
        for key, value in health.items():
            print(f"   {key}: {value}")

        print(f"\nâœ… GraphRAG LLM Adapter test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
