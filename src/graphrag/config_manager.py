"""
GraphRAG ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ - ì™„ì „ í†µí•© ë²„ì „
1ë‹¨ê³„ (í™•ì¥ëœ dataclass) + 2ë‹¨ê³„ (ê°œì„ ëœ íŒŒì‹±) í†µí•©
"""

import os
import json
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field, asdict
from enum import Enum

# ì„¤ì • íŒŒì¼ ë¡œë”©
try:
    from dotenv import load_dotenv

    _dotenv_available = True
except ImportError:
    _dotenv_available = False

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class ConfigSource(Enum):
    """ì„¤ì • ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„"""

    DEFAULT = 1
    CONFIG_FILE = 2
    ENVIRONMENT = 3


# ============================================================================
# 1ë‹¨ê³„: YAML êµ¬ì¡°ì— ë§ì¶˜ í™•ì¥ëœ dataclassë“¤
# ============================================================================


# LLM ê´€ë ¨ ì„¤ì •ë“¤
@dataclass
class HuggingFaceLocalConfig:
    """HuggingFace ë¡œì»¬ ëª¨ë¸ ì„¤ì •"""

    model_path: str = "/DATA/MODELS/models--meta-llama--Llama-3.1-8B-Instruct"
    max_new_tokens: int = 2048
    do_sample: bool = True
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.1
    device_map: str = "auto"
    torch_dtype: str = "bfloat16"
    trust_remote_code: bool = True
    batch_size: int = 32


@dataclass
class OpenAIConfig:
    """OpenAI API ì„¤ì •"""

    api_key: str = "${OPENAI_API_KEY}"
    model_name: str = "gpt-4o"
    timeout: int = 60


@dataclass
class AnthropicConfig:
    """Anthropic API ì„¤ì •"""

    api_key: str = "${ANTHROPIC_API_KEY}"
    model_name: str = "claude-3-5-sonnet"
    timeout: int = 60


@dataclass
class HuggingFaceAPIConfig:
    """HuggingFace API ì„¤ì •"""

    model_name: str = "microsoft/DialoGPT-large"
    api_key: str = os.getenv("HUGGINGFACE_API_KEY", "${HUGGINGFACE_API_KEY}")


@dataclass
class LLMConfig:
    """LLM í†µí•© ì„¤ì •"""

    provider: str = "huggingface_local"
    temperature: float = 0.1

    # ê° í”„ë¡œë°”ì´ë”ë³„ ì¤‘ì²© ì„¤ì •
    huggingface_local: HuggingFaceLocalConfig = field(
        default_factory=HuggingFaceLocalConfig
    )
    openai: OpenAIConfig = field(default_factory=OpenAIConfig)
    anthropic: AnthropicConfig = field(default_factory=AnthropicConfig)
    huggingface_api: HuggingFaceAPIConfig = field(default_factory=HuggingFaceAPIConfig)


# ì„ë² ë”© ê´€ë ¨ ì„¤ì •ë“¤
@dataclass
class SentenceTransformersConfig:
    """SentenceTransformers ì„¤ì •"""

    model_name: str = "paraphrase-multilingual-mpnet-base-v2"
    device: str = "auto"
    batch_size: int = 32
    cache_dir: str = "./cache/embeddings"


@dataclass
class OpenAIEmbeddingConfig:
    """OpenAI ì„ë² ë”© ì„¤ì •"""

    model_name: str = "text-embedding-ada-002"
    api_key: str = "${OPENAI_API_KEY}"
    batch_size: int = 16
    max_length: int = 8192


@dataclass
class HuggingFaceAPIEmbeddingConfig:
    """HuggingFace API ì„ë² ë”© ì„¤ì •"""

    model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
    api_key: str = "${HUGGINGFACE_API_KEY}"
    batch_size: int = 32
    max_length: int = 512


@dataclass
class EmbeddingsConfig:
    """ì„ë² ë”© ì„¤ì • (í™•ì¥ëœ ë²„ì „)"""

    model_type: str = "sentence-transformers"
    save_directory: str = "./data/processed/vector_store/embeddings"

    # ê° íƒ€ì…ë³„ ì„¤ì •
    sentence_transformers: SentenceTransformersConfig = field(
        default_factory=SentenceTransformersConfig
    )
    openai: OpenAIEmbeddingConfig = field(default_factory=OpenAIEmbeddingConfig)
    huggingface_api: HuggingFaceAPIEmbeddingConfig = field(
        default_factory=HuggingFaceAPIEmbeddingConfig
    )


# ë²¡í„° ì €ì¥ì†Œ ê´€ë ¨ ì„¤ì •ë“¤
@dataclass
class FAISSConfig:
    """FAISS ë²¡í„° ì €ì¥ì†Œ ì„¤ì •"""

    persist_directory: str = "./data/processed/vector_store/faiss"
    index_type: str = "flat"
    distance_metric: str = "cosine"
    use_gpu: bool = False
    gpu_id: int = 0
    gpu_memory_fraction: float = 0.5


@dataclass
class ChromaDBConfig:
    """ChromaDB ë²¡í„° ì €ì¥ì†Œ ì„¤ì •"""

    persist_directory: str = "./data/processed/vector_store/chromadb"
    collection_name: str = "graphrag_embeddings"
    distance_metric: str = "cosine"


@dataclass
class SimpleStoreConfig:
    """Simple ë²¡í„° ì €ì¥ì†Œ ì„¤ì •"""

    persist_directory: str = "./data/processed/vector_store/simple"


@dataclass
class VectorStoreConfig:
    """ë²¡í„° ì €ì¥ì†Œ í†µí•© ì„¤ì •"""

    store_type: str = "faiss"
    batch_size: int = 128
    persist_directory: str = "./data/processed/vector_store"

    # ê° ì €ì¥ì†Œë³„ ì¤‘ì²© ì„¤ì •
    faiss: FAISSConfig = field(default_factory=FAISSConfig)
    chromadb: ChromaDBConfig = field(default_factory=ChromaDBConfig)
    simple: SimpleStoreConfig = field(default_factory=SimpleStoreConfig)


# ê·¸ë˜í”„ ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •ë“¤
@dataclass
class NodeEmbeddingsConfig:
    """ë…¸ë“œ ì„ë² ë”© ì„¤ì •"""

    max_text_length: int = 512
    batch_size: int = 32
    cache_embeddings: bool = True
    cache_dir: str = "./cache/embeddings"
    output_directory: str = "./data/processed/vector_store/embeddings"


@dataclass
class SubgraphExtractionConfig:
    """ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ ì„¤ì •"""

    max_nodes: int = 300
    max_edges: int = 800
    max_hops: int = 3
    initial_top_k: int = 25
    similarity_threshold: float = 0.5
    expansion_factor: float = 2.5


@dataclass
class ContextSerializationConfig:
    """ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™” ì„¤ì •"""

    max_tokens: int = 8000
    format_style: str = "structured"
    language: str = "mixed"
    include_statistics: bool = True
    include_relationships: bool = True


@dataclass
class GraphProcessingConfig:
    """ê·¸ë˜í”„ ì²˜ë¦¬ í†µí•© ì„¤ì •"""

    node_embeddings: NodeEmbeddingsConfig = field(default_factory=NodeEmbeddingsConfig)
    subgraph_extraction: SubgraphExtractionConfig = field(
        default_factory=SubgraphExtractionConfig
    )
    context_serialization: ContextSerializationConfig = field(
        default_factory=ContextSerializationConfig
    )


# í•˜ë“œì›¨ì–´ ë° ì„±ëŠ¥ ì„¤ì •ë“¤
@dataclass
class HardwareConfig:
    """í•˜ë“œì›¨ì–´ ìµœì í™” ì„¤ì •"""

    use_gpu: bool = True
    gpu_memory_fraction: float = 0.7
    mixed_precision: bool = True
    cpu_threads: int = 8
    enable_gradient_checkpointing: bool = True
    enable_cpu_offload: bool = False


@dataclass
class PerformanceConfig:
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì •"""

    enable_parallel: bool = True
    max_workers: int = 4
    enable_caching: bool = True
    cache_size_limit: str = "8GB"
    batch_processing: bool = True
    memory_limit: str = "16GB"
    enable_flash_attention: bool = True
    enable_model_parallelism: bool = True


# ì¿¼ë¦¬ ë¶„ì„ ì„¤ì •ë“¤
@dataclass
class ComplexityThresholds:
    """ë³µì¡ë„ ì„ê³„ê°’ ì„¤ì •"""

    simple_max: float = 0.3
    medium_max: float = 0.6
    complex_max: float = 0.8


@dataclass
class LanguageDetectionConfig:
    """ì–¸ì–´ ê°ì§€ ì„¤ì •"""

    default_language: str = "ko"
    supported_languages: List[str] = field(default_factory=lambda: ["ko", "en"])


@dataclass
class QueryTimeouts:
    """ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ ì„¤ì •"""

    simple: int = 20
    medium: int = 45
    complex: int = 120
    exploratory: int = 240


@dataclass
class QueryAnalysisConfig:
    """ì¿¼ë¦¬ ë¶„ì„ ì„¤ì •"""

    complexity_thresholds: ComplexityThresholds = field(
        default_factory=ComplexityThresholds
    )
    language_detection: LanguageDetectionConfig = field(
        default_factory=LanguageDetectionConfig
    )
    timeouts: QueryTimeouts = field(default_factory=QueryTimeouts)


# ê²½ë¡œ ê´€ë¦¬ ì„¤ì •ë“¤
@dataclass
class VectorStorePathsConfig:
    """ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œë“¤"""

    embeddings: str = "./data/processed/vector_store/embeddings"
    faiss: str = "./data/processed/vector_store/faiss"
    chromadb: str = "./data/processed/vector_store/chromadb"
    simple: str = "./data/processed/vector_store/simple"


@dataclass
class PathsConfig:
    """ê²½ë¡œ ì„¤ì • í†µí•©"""

    data_dir: str = "./data"
    processed_dir: str = "./data/processed"
    unified_graph: str = "./data/processed/graphs/unified/unified_knowledge_graph.json"
    individual_graphs_dir: str = "./data/processed/graphs"
    vector_store_root: str = "./data/processed/vector_store"
    cache_dir: str = "./cache"
    embeddings_cache: str = "./cache/embeddings"
    query_cache: str = "./cache/queries"
    logs_dir: str = "./logs"
    models_dir: str = "/DATA/MODELS"

    # ë²¡í„° ì €ì¥ì†Œ í•˜ìœ„ êµ¬ì¡°
    vector_store: VectorStorePathsConfig = field(default_factory=VectorStorePathsConfig)


# ë¡œê¹… ì„¤ì •ë“¤
@dataclass
class FileLoggingConfig:
    """íŒŒì¼ ë¡œê¹… ì„¤ì •"""

    enabled: bool = True
    log_file: str = "./logs/graphrag.log"
    max_size: str = "50MB"
    backup_count: int = 5


@dataclass
class ConsoleLoggingConfig:
    """ì½˜ì†” ë¡œê¹… ì„¤ì •"""

    enabled: bool = True
    colored: bool = True


@dataclass
class LoggingConfig:
    """ë¡œê¹… í†µí•© ì„¤ì •"""

    level: str = "INFO"
    format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file_logging: FileLoggingConfig = field(default_factory=FileLoggingConfig)
    console_logging: ConsoleLoggingConfig = field(default_factory=ConsoleLoggingConfig)


# ê°œë°œ ë° ì„œë²„ ì„¤ì •ë“¤
@dataclass
class DevelopmentConfig:
    """ê°œë°œ ì„¤ì •"""

    debug_mode: bool = False
    test_mode: bool = False
    sample_data_only: bool = False
    max_test_nodes: int = 200
    enable_profiling: bool = True


@dataclass
class ServerConfig:
    """ì„œë²„ í™˜ê²½ ì„¤ì •"""

    preload_models: bool = True
    model_cache_size: int = 2
    auto_cleanup: bool = True
    cleanup_interval: int = 3600
    restrict_model_access: bool = True


# ê¸°ì¡´ ë‹¨ìˆœ ì„¤ì •ë“¤ (í˜¸í™˜ì„± ìœ ì§€)
@dataclass
class GraphConfig:
    """ê·¸ë˜í”„ ì„¤ì • (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""

    unified_graph_path: str = (
        "./data/processed/graphs/unified/unified_knowledge_graph.json"
    )
    vector_store_path: str = "./data/processed/vector_store"
    graphs_directory: str = "./data/processed/graphs"
    cache_enabled: bool = True
    cache_ttl_hours: int = 24


@dataclass
class QAConfig:
    """QA ì²´ì¸ ì„¤ì •"""

    chain_type: str = "retrieval_qa"
    max_docs: int = 10
    min_relevance_score: float = 0.3
    return_source_documents: bool = True
    enable_memory: bool = False
    memory_type: str = "buffer"
    max_memory_tokens: int = 4000


@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • (ê¸°ë³¸)"""

    log_level: str = "INFO"
    verbose: bool = False
    parallel_processing: bool = True
    max_workers: int = 4
    temp_directory: str = "./tmp"
    enable_monitoring: bool = False


# ë©”ì¸ ì„¤ì • í´ë˜ìŠ¤
@dataclass
class GraphRAGConfig:
    """GraphRAG ì „ì²´ ì„¤ì • - YAML êµ¬ì¡°ì— ì™„ì „ ë§¤ì¹­"""

    # ë©”ì¸ ì„¤ì • ì„¹ì…˜ë“¤
    llm: LLMConfig = field(default_factory=LLMConfig)
    embeddings: EmbeddingsConfig = field(default_factory=EmbeddingsConfig)
    vector_store: VectorStoreConfig = field(default_factory=VectorStoreConfig)
    graph_processing: GraphProcessingConfig = field(
        default_factory=GraphProcessingConfig
    )
    hardware: HardwareConfig = field(default_factory=HardwareConfig)
    performance: PerformanceConfig = field(default_factory=PerformanceConfig)
    query_analysis: QueryAnalysisConfig = field(default_factory=QueryAnalysisConfig)
    paths: PathsConfig = field(default_factory=PathsConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)
    development: DevelopmentConfig = field(default_factory=DevelopmentConfig)
    server: ServerConfig = field(default_factory=ServerConfig)

    # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ìš©
    graph: GraphConfig = field(default_factory=GraphConfig)
    qa: QAConfig = field(default_factory=QAConfig)
    system: SystemConfig = field(default_factory=SystemConfig)

    # ë©”íƒ€ë°ì´í„°
    version: str = "1.0.0"
    config_source: ConfigSource = ConfigSource.DEFAULT
    last_updated: Optional[str] = None

    @property
    def embedding(self):
        """ì„ë² ë”© ì„¤ì • í˜¸í™˜ì„± ì†ì„± (embedding -> embeddings)"""

        # í˜¸í™˜ì„±ì„ ìœ„í•œ ê°€ì§œ ê°ì²´ ìƒì„±
        class EmbeddingCompat:
            def __init__(self, embeddings_config):
                self.model_name = embeddings_config.sentence_transformers.model_name
                self.device = embeddings_config.sentence_transformers.device
                self.batch_size = embeddings_config.sentence_transformers.batch_size
                self.save_directory = embeddings_config.save_directory
                self.cache_dir = embeddings_config.sentence_transformers.cache_dir

        return EmbeddingCompat(self.embeddings)


# ============================================================================
# 2ë‹¨ê³„: ê°œì„ ëœ GraphRAGConfigManager í´ë˜ìŠ¤
# ============================================================================


class GraphRAGConfigManager:
    """ë‹¨ìˆœí™”ëœ GraphRAG ì„¤ì • ê´€ë¦¬ì - ê°œì„ ëœ YAML íŒŒì‹±"""

    def __init__(
        self,
        config_file: Optional[str] = None,
        env_file: Optional[str] = None,
        auto_load: bool = True,
    ):
        self.config_file = Path(config_file) if config_file else None
        self.env_file = Path(env_file) if env_file else None

        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œì‘
        self.config = GraphRAGConfig()

        if auto_load:
            self.load_all()

        logger.info("âœ… GraphRAGConfigManager initialized (complete version)")

    def load_all(self) -> None:
        """ëª¨ë“  ì„¤ì • ì†ŒìŠ¤ ë¡œë”© (ê°œì„ ëœ ìˆœì„œ)"""
        logger.info("ğŸ”§ Loading configuration with improved parsing...")

        # 1. í•µì‹¬ í™˜ê²½ë³€ìˆ˜ ë¡œë”© (4ê°œë§Œ)
        self._load_core_environment_variables()

        # 2. YAML ì„¤ì • íŒŒì¼ ë¡œë”© (ë©”ì¸ ì„¤ì •)
        if self.config_file and self.config_file.exists():
            self._load_yaml_config_file()
        elif Path("graphrag_config.yaml").exists():
            self._load_yaml_config_file(Path("graphrag_config.yaml"))

        # 3. ì„¤ì • ê²€ì¦ ë° ë””ë ‰í† ë¦¬ ìƒì„±
        self._validate_and_setup()

        logger.info("âœ… Configuration loaded with improved parsing")

    def _load_core_environment_variables(self) -> None:
        """í•µì‹¬ í™˜ê²½ë³€ìˆ˜ 4ê°œë§Œ ë¡œë”© (ë‹¨ìˆœí™”ë¨)"""
        logger.info("ğŸŒ Loading core environment variables (4 only)...")

        # .env íŒŒì¼ ë¡œë”© (ìˆìœ¼ë©´)
        if _dotenv_available:
            env_path = self.env_file or Path(".env")
            if env_path.exists():
                load_dotenv(env_path, override=False)
                logger.info(f"ğŸ“‚ Loaded .env file: {env_path}")
        else:
            logger.warning("âš ï¸ python-dotenv not available")

        # 1. GPU ì„¤ì •
        cuda_devices = os.getenv("CUDA_VISIBLE_DEVICES")
        if cuda_devices is not None:
            os.environ["CUDA_VISIBLE_DEVICES"] = cuda_devices
            logger.info(f"ğŸ”§ CUDA devices: {cuda_devices}")

        # 2. HuggingFace API í‚¤
        hf_key = os.getenv("HUGGINGFACE_API_KEY")
        if hf_key:
            self.config.llm.huggingface_api.api_key = hf_key
            logger.info("ğŸ”‘ HuggingFace API key loaded")

        # 3. ë¡œê·¸ ë ˆë²¨
        log_level = os.getenv("GRAPHRAG_LOG_LEVEL")
        if log_level:
            self.config.system.log_level = log_level.upper()
            self.config.logging.level = log_level.upper()
            logging.getLogger().setLevel(
                getattr(logging, log_level.upper(), logging.INFO)
            )
            logger.info(f"ğŸ“ Log level: {log_level}")

        # 4. Verbose ëª¨ë“œ
        verbose = os.getenv("GRAPHRAG_VERBOSE")
        if verbose:
            verbose_bool = verbose.lower() in ("true", "1", "yes")
            self.config.system.verbose = verbose_bool
            logger.info(f"ğŸ” Verbose mode: {verbose_bool}")

    def _load_yaml_config_file(self, config_path: Optional[Path] = None) -> None:
        """YAML ì„¤ì • íŒŒì¼ ë¡œë”©"""
        config_path = config_path or self.config_file
        logger.info(f"ğŸ“‚ Loading YAML config: {config_path}")

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                yaml_data = yaml.safe_load(f)

            if not yaml_data:
                logger.warning("âš ï¸ Empty YAML file")
                return

            # YAML â†’ dataclass ë§¤í•‘
            self._apply_yaml_to_dataclass(yaml_data)
            logger.info("âœ… YAML config applied successfully")

        except Exception as e:
            logger.error(f"âŒ Failed to load YAML config: {e}")
            raise

    def _apply_yaml_to_dataclass(self, yaml_data: Dict[str, Any]) -> None:
        """YAML ë°ì´í„°ë¥¼ dataclassì— ì ìš©"""

        # ê° ì„¹ì…˜ë³„ë¡œ ì²˜ë¦¬
        section_handlers = {
            "llm": self._apply_llm_config,
            "embeddings": self._apply_embeddings_config,
            "vector_store": self._apply_vector_store_config,
            "graph_processing": self._apply_graph_processing_config,
            "hardware": self._apply_hardware_config,
            "performance": self._apply_performance_config,
            "query_analysis": self._apply_query_analysis_config,
            "paths": self._apply_paths_config,
            "logging": self._apply_logging_config,
            "development": self._apply_development_config,
            "server": self._apply_server_config,
            # ê¸°ì¡´ í˜¸í™˜ì„± ì„¹ì…˜ë“¤
            "graph": self._apply_graph_config,
            "qa": self._apply_qa_config,
            "system": self._apply_system_config,
        }

        for section_name, handler in section_handlers.items():
            if section_name in yaml_data:
                try:
                    handler(yaml_data[section_name])
                    logger.debug(f"âœ… Applied {section_name} section")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to apply {section_name}: {e}")

    def _apply_llm_config(self, llm_data: Dict[str, Any]) -> None:
        """LLM ì„¤ì • ì ìš©"""
        # ìµœìƒìœ„ ì„¤ì •
        for key, value in llm_data.items():
            if hasattr(self.config.llm, key) and not isinstance(value, dict):
                setattr(self.config.llm, key, value)

        # ì¤‘ì²© ì„¤ì •ë“¤
        provider_configs = {
            "huggingface_local": self.config.llm.huggingface_local,
            "openai": self.config.llm.openai,
            "anthropic": self.config.llm.anthropic,
            "huggingface_api": self.config.llm.huggingface_api,
        }

        for provider, config_obj in provider_configs.items():
            if provider in llm_data and isinstance(llm_data[provider], dict):
                self._apply_nested_config(llm_data[provider], config_obj)

    def _apply_embeddings_config(self, embeddings_data: Dict[str, Any]) -> None:
        """ì„ë² ë”© ì„¤ì • ì ìš©"""
        for key, value in embeddings_data.items():
            if hasattr(self.config.embeddings, key) and not isinstance(value, dict):
                setattr(self.config.embeddings, key, value)

        if "sentence_transformers" in embeddings_data:
            self._apply_nested_config(
                embeddings_data["sentence_transformers"],
                self.config.embeddings.sentence_transformers,
            )

    def _apply_vector_store_config(self, vector_data: Dict[str, Any]) -> None:
        """ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ì ìš©"""
        for key, value in vector_data.items():
            if hasattr(self.config.vector_store, key) and not isinstance(value, dict):
                setattr(self.config.vector_store, key, value)

        store_configs = {
            "faiss": self.config.vector_store.faiss,
            "chromadb": self.config.vector_store.chromadb,
            "simple": self.config.vector_store.simple,
        }

        for store_type, config_obj in store_configs.items():
            if store_type in vector_data:
                self._apply_nested_config(vector_data[store_type], config_obj)

    def _apply_graph_processing_config(self, graph_proc_data: Dict[str, Any]) -> None:
        """ê·¸ë˜í”„ ì²˜ë¦¬ ì„¤ì • ì ìš©"""
        processing_configs = {
            "node_embeddings": self.config.graph_processing.node_embeddings,
            "subgraph_extraction": self.config.graph_processing.subgraph_extraction,
            "context_serialization": self.config.graph_processing.context_serialization,
        }

        for proc_type, config_obj in processing_configs.items():
            if proc_type in graph_proc_data:
                self._apply_nested_config(graph_proc_data[proc_type], config_obj)

    def _apply_paths_config(self, paths_data: Dict[str, Any]) -> None:
        """ê²½ë¡œ ì„¤ì • ì ìš©"""
        for key, value in paths_data.items():
            if hasattr(self.config.paths, key) and not isinstance(value, dict):
                setattr(self.config.paths, key, value)

        if "vector_store" in paths_data and isinstance(
            paths_data["vector_store"], dict
        ):
            self._apply_nested_config(
                paths_data["vector_store"], self.config.paths.vector_store
            )

    def _apply_hardware_config(self, hardware_data: Dict[str, Any]) -> None:
        """í•˜ë“œì›¨ì–´ ì„¤ì • ì ìš©"""
        self._apply_nested_config(hardware_data, self.config.hardware)

    def _apply_performance_config(self, perf_data: Dict[str, Any]) -> None:
        """ì„±ëŠ¥ ì„¤ì • ì ìš©"""
        self._apply_nested_config(perf_data, self.config.performance)

    def _apply_query_analysis_config(self, query_data: Dict[str, Any]) -> None:
        """ì¿¼ë¦¬ ë¶„ì„ ì„¤ì • ì ìš©"""
        nested_configs = {
            "complexity_thresholds": self.config.query_analysis.complexity_thresholds,
            "language_detection": self.config.query_analysis.language_detection,
            "timeouts": self.config.query_analysis.timeouts,
        }

        for nested_key, config_obj in nested_configs.items():
            if nested_key in query_data:
                self._apply_nested_config(query_data[nested_key], config_obj)

    def _apply_logging_config(self, logging_data: Dict[str, Any]) -> None:
        """ë¡œê¹… ì„¤ì • ì ìš© (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )"""
        for key, value in logging_data.items():
            if key == "level" and os.getenv("GRAPHRAG_LOG_LEVEL"):
                continue  # í™˜ê²½ë³€ìˆ˜ ìš°ì„ 
            if hasattr(self.config.logging, key) and not isinstance(value, dict):
                setattr(self.config.logging, key, value)

        if "file_logging" in logging_data:
            self._apply_nested_config(
                logging_data["file_logging"], self.config.logging.file_logging
            )
        if "console_logging" in logging_data:
            self._apply_nested_config(
                logging_data["console_logging"], self.config.logging.console_logging
            )

    def _apply_development_config(self, dev_data: Dict[str, Any]) -> None:
        """ê°œë°œ ì„¤ì • ì ìš©"""
        self._apply_nested_config(dev_data, self.config.development)

    def _apply_server_config(self, server_data: Dict[str, Any]) -> None:
        """ì„œë²„ ì„¤ì • ì ìš©"""
        self._apply_nested_config(server_data, self.config.server)

    # ê¸°ì¡´ í˜¸í™˜ì„± ë©”ì„œë“œë“¤
    def _apply_graph_config(self, graph_data: Dict[str, Any]) -> None:
        """ê¸°ì¡´ ê·¸ë˜í”„ ì„¤ì • ì ìš©"""
        self._apply_nested_config(graph_data, self.config.graph)

    def _apply_qa_config(self, qa_data: Dict[str, Any]) -> None:
        """QA ì„¤ì • ì ìš©"""
        self._apply_nested_config(qa_data, self.config.qa)

    def _apply_system_config(self, system_data: Dict[str, Any]) -> None:
        """ì‹œìŠ¤í…œ ì„¤ì • ì ìš© (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )"""
        for key, value in system_data.items():
            if key == "log_level" and os.getenv("GRAPHRAG_LOG_LEVEL"):
                continue
            if key == "verbose" and os.getenv("GRAPHRAG_VERBOSE"):
                continue
            if hasattr(self.config.system, key):
                setattr(self.config.system, key, value)

    def _apply_nested_config(
        self, yaml_section: Dict[str, Any], config_obj: Any
    ) -> None:
        """ì¤‘ì²©ëœ ì„¤ì •ì„ dataclass ê°ì²´ì— ì ìš©"""
        for key, value in yaml_section.items():
            if hasattr(config_obj, key):
                if isinstance(value, list):
                    setattr(config_obj, key, value)
                elif not isinstance(value, dict):
                    setattr(config_obj, key, value)

    def _validate_and_setup(self) -> None:
        """ì„¤ì • ê²€ì¦ ë° ë””ë ‰í† ë¦¬ ìƒì„±"""
        logger.info("ğŸ” Validating configuration...")

        errors = []
        warnings = []

        # LLM ì„¤ì • ê²€ì¦
        if self.config.llm.provider == "huggingface_local":
            model_path = self.config.llm.huggingface_local.model_path
            if not model_path:
                errors.append("Local model path is required")
            elif not Path(model_path).exists():
                warnings.append(f"Local model path not found: {model_path}")

        # ì˜¨ë„ ë²”ìœ„ ê²€ì¦
        if not (0 <= self.config.llm.temperature <= 1):
            errors.append("LLM temperature must be between 0 and 1")

        if errors:
            error_msg = "Configuration validation failed:\n" + "\n".join(
                f"- {err}" for err in errors
            )
            raise ValueError(error_msg)

        for warning in warnings:
            logger.warning(f"âš ï¸ {warning}")

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        logger.info("âœ… Configuration validated and directories created")

    def _create_directories(self) -> None:
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ ìƒì„±"""
        directories = [
            self.config.paths.data_dir,
            self.config.paths.processed_dir,
            self.config.paths.vector_store_root,
            self.config.paths.vector_store.embeddings,
            self.config.paths.vector_store.faiss,
            self.config.paths.vector_store.chromadb,
            self.config.paths.vector_store.simple,
            self.config.paths.cache_dir,
            self.config.paths.embeddings_cache,
            self.config.paths.query_cache,
            self.config.paths.logs_dir,
            self.config.system.temp_directory,
        ]

        created_count = 0
        for directory in directories:
            if directory:
                Path(directory).mkdir(exist_ok=True)
                created_count += 1

        logger.info(f"ğŸ“ Created/verified {created_count} directories")

    # ========================================================================
    # Pipeline í˜¸í™˜ ë©”ì„œë“œë“¤
    # ========================================================================

    def get_llm_config(self) -> Dict[str, Any]:
        """LLM ì„¤ì • ë°˜í™˜ (pipeline í˜¸í™˜)"""
        llm_config = {
            "provider": self.config.llm.provider,
            "temperature": self.config.llm.temperature,
        }

        # í”„ë¡œë°”ì´ë”ë³„ ì„¤ì • ì¶”ê°€
        if self.config.llm.provider == "huggingface_api":
            llm_config.update(
                {
                    "model_name": getattr(
                        self.config.llm,
                        "model_name",
                        "meta-llama/Meta-Llama-3.1-8B-Instruct",
                    ),
                    "api_key": os.getenv("HUGGINGFACE_API_KEY"),
                    "timeout": getattr(self.config.llm, "timeout", 30),
                    "top_p": getattr(self.config.llm, "top_p", 0.9),
                }
            )
        elif self.config.llm.provider == "huggingface_local":
            hf_config = asdict(self.config.llm.huggingface_local)
            llm_config.update(hf_config)
        elif self.config.llm.provider == "openai":
            openai_config = asdict(self.config.llm.openai)
            llm_config.update(openai_config)

        return llm_config

    def get_vector_store_config(self) -> Dict[str, Any]:
        """ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ë°˜í™˜"""
        base_config = {
            "store_type": self.config.vector_store.store_type,
            "batch_size": self.config.vector_store.batch_size,
            "persist_directory": self.config.vector_store.persist_directory,
        }

        # ì €ì¥ì†Œ íƒ€ì…ë³„ ì„¤ì • ì¶”ê°€
        if self.config.vector_store.store_type == "faiss":
            faiss_config = asdict(self.config.vector_store.faiss)
            base_config.update(faiss_config)
        elif self.config.vector_store.store_type == "chromadb":
            chroma_config = asdict(self.config.vector_store.chromadb)
            base_config.update(chroma_config)

        return base_config

    def get_embeddings_config(self) -> Dict[str, Any]:
        """ì„ë² ë”© ì„¤ì • ë°˜í™˜ (ëª¨ë“  model_type ì§€ì›)"""

        # ê¸°ë³¸ ì„¤ì •
        base_config = {
            "model_type": self.config.embeddings.model_type,
            "save_directory": self.config.embeddings.save_directory,
        }

        # model_typeì— ë”°ë¼ ì•ˆì „í•˜ê²Œ ì„¤ì • ì¶”ê°€
        if self.config.embeddings.model_type == "sentence-transformers":
            # SentenceTransformers ì„¤ì •
            st_config = self.config.embeddings.sentence_transformers
            base_config.update(
                {
                    "model_name": st_config.model_name,
                    "device": st_config.device,
                    "batch_size": st_config.batch_size,
                    "cache_dir": st_config.cache_dir,
                    "max_length": 512,  # ê¸°ë³¸ê°’
                    "sentence_transformers": asdict(st_config),
                }
            )

        elif self.config.embeddings.model_type == "openai":
            # OpenAI ì„ë² ë”© ì„¤ì • (í–¥í›„ í™•ì¥ìš©)
            base_config.update(
                {
                    "model_name": "text-embedding-ada-002",  # OpenAI ê¸°ë³¸ê°’
                    "device": "auto",
                    "batch_size": 16,
                    "cache_dir": "./cache/embeddings",
                    "max_length": 8192,
                }
            )

        elif self.config.embeddings.model_type == "huggingface_api":
            # HuggingFace API ì„ë² ë”© ì„¤ì • (í–¥í›„ í™•ì¥ìš©)
            base_config.update(
                {
                    "model_name": "sentence-transformers/all-MiniLM-L6-v2",
                    "device": "auto",
                    "batch_size": 32,
                    "cache_dir": "./cache/embeddings",
                    "max_length": 512,
                }
            )

        else:
            # ê¸°ë³¸ê°’ (ì•ˆì „ì¥ì¹˜)
            logger.warning(
                f"âš ï¸ Unknown model_type: {self.config.embeddings.model_type}, using defaults"
            )
            base_config.update(
                {
                    "model_name": "sentence-transformers/all-MiniLM-L6-v2",
                    "device": "auto",
                    "batch_size": 32,
                    "cache_dir": "./cache/embeddings",
                    "max_length": 512,
                }
            )

        return base_config

    def get_paths_config(self) -> Dict[str, Any]:
        """ê²½ë¡œ ì„¤ì • ë°˜í™˜"""
        return asdict(self.config.paths)

    def get_config_summary(self) -> str:
        """ì„¤ì • ìš”ì•½ ì •ë³´"""
        summary = f"""
GraphRAG Configuration Summary (Complete Version):
================================================
LLM Provider: {self.config.llm.provider}
Model Path: {getattr(self.config.llm.huggingface_local, 'model_path', 'N/A')}
Vector Store: {self.config.vector_store.store_type}
Embeddings Model: {getattr(self.config.embeddings.sentence_transformers, 'model_name', 'N/A')}
Log Level: {self.config.logging.level} (System: {self.config.system.log_level})
Verbose: {self.config.system.verbose}
Max Workers: {self.config.performance.max_workers}
Data Directory: {self.config.paths.data_dir}
Vector Store Root: {self.config.paths.vector_store_root}
"""
        return summary.strip()

    def get_embedding_config(self) -> Dict[str, Any]:
        """ì„ë² ë”© ì„¤ì • ë°˜í™˜ (pipeline í˜¸í™˜ì„±)"""
        return {
            "model_name": self.config.embeddings.sentence_transformers.model_name,
            "device": self.config.embeddings.sentence_transformers.device,
            "batch_size": self.config.embeddings.sentence_transformers.batch_size,
            "save_directory": self.config.embeddings.save_directory,
            "cache_dir": self.config.embeddings.sentence_transformers.cache_dir,
        }

    def get_system_config(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì„¤ì • ë°˜í™˜ (pipeline í˜¸í™˜ì„±)"""
        return asdict(self.config.system)

    def get_graph_config(self) -> Dict[str, Any]:
        """ê·¸ë˜í”„ ì„¤ì • ë°˜í™˜ (pipeline í˜¸í™˜ì„±)"""
        return asdict(self.config.graph)

    def get_qa_config(self) -> Dict[str, Any]:
        """QA ì„¤ì • ë°˜í™˜ (pipeline í˜¸í™˜ì„±)"""
        return asdict(self.config.qa)


# ============================================================================
# íŒ©í† ë¦¬ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ============================================================================


def create_config_manager(
    config_file: Optional[str] = None, env_file: Optional[str] = None
) -> GraphRAGConfigManager:
    """ì„¤ì • ê´€ë¦¬ì íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return GraphRAGConfigManager(config_file=config_file, env_file=env_file)


def main():
    """ì™„ì „í•œ ì„¤ì • ê´€ë¦¬ì í…ŒìŠ¤íŠ¸"""

    print("ğŸ§ª Testing Complete Config Manager (1ë‹¨ê³„+2ë‹¨ê³„)...")

    try:
        # ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”
        config_manager = GraphRAGConfigManager(auto_load=False)
        print("âœ… Basic initialization")

        # ì„¤ì • ë¡œë”©
        config_manager.load_all()
        print("âœ… Configuration loading")

        # ì¤‘ì²© êµ¬ì¡° ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        print(f"âœ… LLM Provider: {config_manager.config.llm.provider}")
        print(
            f"âœ… Model Path: {config_manager.config.llm.huggingface_local.model_path}"
        )
        print(f"âœ… Vector Store: {config_manager.config.vector_store.store_type}")
        print(
            f"âœ… FAISS Dir: {config_manager.config.vector_store.faiss.persist_directory}"
        )

        # Pipeline í˜¸í™˜ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
        llm_config = config_manager.get_llm_config()
        print(f"âœ… Pipeline LLM Config: {llm_config.get('provider')}")

        vector_config = config_manager.get_vector_store_config()
        print(f"âœ… Pipeline Vector Config: {vector_config.get('store_type')}")

        # ì„¤ì • ìš”ì•½
        print(f"\nğŸ“‹ Configuration Summary:")
        print(config_manager.get_config_summary())

        print(f"\nâœ… Complete config manager test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
