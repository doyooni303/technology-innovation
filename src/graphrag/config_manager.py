"""
GraphRAG ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ
Configuration Manager for GraphRAG System

ì‹œìŠ¤í…œ ì „ì²´ ì„¤ì • ë° ì¸ì¦ ì •ë³´ í†µí•© ê´€ë¦¬
- í™˜ê²½ ë³€ìˆ˜ ë° .env íŒŒì¼ ì§€ì›
- YAML/JSON ì„¤ì • íŒŒì¼ ë¡œë”©
- API í‚¤ ë° ì¸ì¦ ì •ë³´ ë³´ì•ˆ ê´€ë¦¬
- ì„¤ì • ê²€ì¦ ë° ê¸°ë³¸ê°’ ì œê³µ
- ëŸ°íƒ€ì„ ì„¤ì • ì—…ë°ì´íŠ¸
"""

import os
import json
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import warnings

# ì„¤ì • íŒŒì¼ ë¡œë”©
try:
    from dotenv import load_dotenv

    _dotenv_available = True
except ImportError:
    _dotenv_available = False
    warnings.warn(
        "python-dotenv not available. Install with: pip install python-dotenv"
    )

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class ConfigSource(Enum):
    """ì„¤ì • ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ (ë†’ì€ ìˆ«ìê°€ ìš°ì„ )"""

    DEFAULT = 1
    CONFIG_FILE = 2
    ENV_FILE = 3
    ENVIRONMENT = 4
    RUNTIME = 5


@dataclass
class LLMConfig:
    """LLM ì„¤ì •"""

    provider: str = "openai"
    model_name: str = "gpt-3.5-turbo"
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    temperature: float = 0.1
    max_tokens: Optional[int] = None
    streaming: bool = False
    timeout: int = 60


@dataclass
class EmbeddingConfig:
    """ì„ë² ë”© ì„¤ì •"""

    model_name: str = "auto"
    device: str = "auto"
    batch_size: int = 32
    max_length: int = 512
    cache_dir: Optional[str] = None


@dataclass
class GraphConfig:
    """ê·¸ë˜í”„ ì„¤ì •"""

    unified_graph_path: str = ""
    vector_store_path: str = ""
    graphs_directory: str = "./graphs"
    cache_enabled: bool = True
    cache_ttl_hours: int = 24


@dataclass
class QAConfig:
    """QA ì²´ì¸ ì„¤ì •"""

    chain_type: str = "retrieval_qa"
    max_docs: int = 10
    min_relevance_score: float = 0.3
    return_source_documents: bool = True
    enable_memory: bool = False
    memory_type: str = "buffer"
    max_memory_tokens: int = 4000


@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì •"""

    log_level: str = "INFO"
    verbose: bool = False
    parallel_processing: bool = True
    max_workers: int = 4
    temp_directory: str = "./tmp"
    enable_monitoring: bool = False


@dataclass
class GraphRAGConfig:
    """GraphRAG ì „ì²´ ì„¤ì •"""

    # í•˜ìœ„ ì„¤ì •ë“¤
    llm: LLMConfig = field(default_factory=LLMConfig)
    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)
    graph: GraphConfig = field(default_factory=GraphConfig)
    qa: QAConfig = field(default_factory=QAConfig)
    system: SystemConfig = field(default_factory=SystemConfig)

    # ë©”íƒ€ë°ì´í„°
    version: str = "1.0.0"
    config_source: ConfigSource = ConfigSource.DEFAULT
    last_updated: Optional[str] = None


class GraphRAGConfigManager:
    """GraphRAG ì„¤ì • ê´€ë¦¬ì"""

    def __init__(
        self,
        config_file: Optional[str] = None,
        env_file: Optional[str] = None,
        auto_load: bool = True,
    ):
        """
        Args:
            config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ (YAML/JSON)
            env_file: .env íŒŒì¼ ê²½ë¡œ
            auto_load: ìë™ ë¡œë”© ì—¬ë¶€
        """
        self.config_file = Path(config_file) if config_file else None
        self.env_file = Path(env_file) if env_file else None

        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œì‘
        self.config = GraphRAGConfig()

        # ì„¤ì • ì†ŒìŠ¤ë³„ ì €ì¥ (ë””ë²„ê¹…ìš©)
        self.config_sources: Dict[str, Any] = {}

        if auto_load:
            self.load_all()

        logger.info("âœ… GraphRAGConfigManager initialized")

    def load_all(self) -> None:
        """ëª¨ë“  ì„¤ì • ì†ŒìŠ¤ë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¡œë”©"""

        logger.info("ğŸ”§ Loading configuration from all sources...")

        # 1. ê¸°ë³¸ ì„¤ì • (ì´ë¯¸ ì ìš©ë¨)
        self.config_sources["default"] = asdict(self.config)

        # 2. ì„¤ì • íŒŒì¼
        if self.config_file and self.config_file.exists():
            self._load_config_file()

        # 3. .env íŒŒì¼
        if self.env_file and self.env_file.exists():
            self._load_env_file()
        elif Path(".env").exists():
            self._load_env_file(Path(".env"))

        # 4. í™˜ê²½ ë³€ìˆ˜
        self._load_environment_variables()

        # 5. ì„¤ì • ê²€ì¦
        self._validate_config()

        logger.info("âœ… Configuration loaded successfully")

    def _load_config_file(self) -> None:
        """ì„¤ì • íŒŒì¼ ë¡œë”© (YAML/JSON)"""

        logger.info(f"ğŸ“‚ Loading config file: {self.config_file}")

        try:
            with open(self.config_file, "r", encoding="utf-8") as f:
                if self.config_file.suffix.lower() in [".yml", ".yaml"]:
                    file_config = yaml.safe_load(f)
                elif self.config_file.suffix.lower() == ".json":
                    file_config = json.load(f)
                else:
                    raise ValueError(
                        f"Unsupported config file format: {self.config_file.suffix}"
                    )

            self.config_sources["config_file"] = file_config
            self._merge_config(file_config, ConfigSource.CONFIG_FILE)

            logger.info("âœ… Config file loaded")

        except Exception as e:
            logger.error(f"âŒ Failed to load config file: {e}")
            raise

    def _load_env_file(self, env_path: Optional[Path] = None) -> None:
        """í™˜ê²½ íŒŒì¼ ë¡œë”©"""

        if not _dotenv_available:
            logger.warning("âš ï¸ python-dotenv not available, skipping .env file")
            return

        env_path = env_path or self.env_file
        logger.info(f"ğŸ” Loading environment file: {env_path}")

        try:
            # .env íŒŒì¼ ë¡œë”©
            load_dotenv(env_path, override=False)

            # GraphRAG ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ë“¤ ì¶”ì¶œ
            env_config = self._extract_env_variables()
            self.config_sources["env_file"] = env_config
            self._merge_config(env_config, ConfigSource.ENV_FILE)

            logger.info("âœ… Environment file loaded")

        except Exception as e:
            logger.error(f"âŒ Failed to load environment file: {e}")

    def _load_environment_variables(self) -> None:
        """í™˜ê²½ ë³€ìˆ˜ ë¡œë”©"""

        logger.info("ğŸŒ Loading environment variables...")

        env_config = self._extract_env_variables()
        self.config_sources["environment"] = env_config
        self._merge_config(env_config, ConfigSource.ENVIRONMENT)

    def _extract_env_variables(self) -> Dict[str, Any]:
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ GraphRAG ì„¤ì • ì¶”ì¶œ"""

        env_config = {}

        # LLM ì„¤ì •
        llm_config = {}
        if os.getenv("OPENAI_API_KEY"):
            llm_config["api_key"] = os.getenv("OPENAI_API_KEY")
            llm_config["provider"] = "openai"
        if os.getenv("ANTHROPIC_API_KEY"):
            llm_config["api_key"] = os.getenv("ANTHROPIC_API_KEY")
            llm_config["provider"] = "anthropic"
        if os.getenv("HUGGINGFACE_API_TOKEN"):
            llm_config["api_key"] = os.getenv("HUGGINGFACE_API_TOKEN")
            llm_config["provider"] = "huggingface"

        if os.getenv("GRAPHRAG_LLM_MODEL"):
            llm_config["model_name"] = os.getenv("GRAPHRAG_LLM_MODEL")
        if os.getenv("GRAPHRAG_LLM_TEMPERATURE"):
            llm_config["temperature"] = float(os.getenv("GRAPHRAG_LLM_TEMPERATURE"))
        if os.getenv("GRAPHRAG_LLM_MAX_TOKENS"):
            llm_config["max_tokens"] = int(os.getenv("GRAPHRAG_LLM_MAX_TOKENS"))

        if llm_config:
            env_config["llm"] = llm_config

        # ì„ë² ë”© ì„¤ì •
        embedding_config = {}
        if os.getenv("GRAPHRAG_EMBEDDING_MODEL"):
            embedding_config["model_name"] = os.getenv("GRAPHRAG_EMBEDDING_MODEL")
        if os.getenv("GRAPHRAG_EMBEDDING_DEVICE"):
            embedding_config["device"] = os.getenv("GRAPHRAG_EMBEDDING_DEVICE")
        if os.getenv("GRAPHRAG_EMBEDDING_BATCH_SIZE"):
            embedding_config["batch_size"] = int(
                os.getenv("GRAPHRAG_EMBEDDING_BATCH_SIZE")
            )

        if embedding_config:
            env_config["embedding"] = embedding_config

        # ê·¸ë˜í”„ ì„¤ì •
        graph_config = {}
        if os.getenv("GRAPHRAG_UNIFIED_GRAPH_PATH"):
            graph_config["unified_graph_path"] = os.getenv(
                "GRAPHRAG_UNIFIED_GRAPH_PATH"
            )
        if os.getenv("GRAPHRAG_VECTOR_STORE_PATH"):
            graph_config["vector_store_path"] = os.getenv("GRAPHRAG_VECTOR_STORE_PATH")
        if os.getenv("GRAPHRAG_GRAPHS_DIRECTORY"):
            graph_config["graphs_directory"] = os.getenv("GRAPHRAG_GRAPHS_DIRECTORY")

        if graph_config:
            env_config["graph"] = graph_config

        # ì‹œìŠ¤í…œ ì„¤ì •
        system_config = {}
        if os.getenv("GRAPHRAG_LOG_LEVEL"):
            system_config["log_level"] = os.getenv("GRAPHRAG_LOG_LEVEL")
        if os.getenv("GRAPHRAG_VERBOSE"):
            system_config["verbose"] = os.getenv("GRAPHRAG_VERBOSE").lower() == "true"
        if os.getenv("GRAPHRAG_MAX_WORKERS"):
            system_config["max_workers"] = int(os.getenv("GRAPHRAG_MAX_WORKERS"))

        if system_config:
            env_config["system"] = system_config

        return env_config

    def _merge_config(self, new_config: Dict[str, Any], source: ConfigSource) -> None:
        """ìƒˆë¡œìš´ ì„¤ì •ì„ ê¸°ì¡´ ì„¤ì •ì— ë³‘í•©"""

        # ê¹Šì€ ë³‘í•© ìˆ˜í–‰
        def deep_merge(base: Dict, update: Dict) -> Dict:
            for key, value in update.items():
                if (
                    key in base
                    and isinstance(base[key], dict)
                    and isinstance(value, dict)
                ):
                    deep_merge(base[key], value)
                else:
                    base[key] = value
            return base

        # dataclassë¥¼ dictë¡œ ë³€í™˜
        config_dict = asdict(self.config)

        # ë³‘í•©
        deep_merge(config_dict, new_config)

        # dictë¥¼ ë‹¤ì‹œ dataclassë¡œ ë³€í™˜
        self.config = self._dict_to_config(config_dict)
        self.config.config_source = source

    def _dict_to_config(self, config_dict: Dict[str, Any]) -> GraphRAGConfig:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ GraphRAGConfigë¡œ ë³€í™˜"""

        # í•˜ìœ„ ì„¤ì •ë“¤ ìƒì„±
        llm_config = LLMConfig(**config_dict.get("llm", {}))
        embedding_config = EmbeddingConfig(**config_dict.get("embedding", {}))
        graph_config = GraphConfig(**config_dict.get("graph", {}))
        qa_config = QAConfig(**config_dict.get("qa", {}))
        system_config = SystemConfig(**config_dict.get("system", {}))

        # ë©”íƒ€ë°ì´í„°
        meta_fields = {
            "version": config_dict.get("version", "1.0.0"),
            "config_source": config_dict.get("config_source", ConfigSource.DEFAULT),
            "last_updated": config_dict.get("last_updated"),
        }

        return GraphRAGConfig(
            llm=llm_config,
            embedding=embedding_config,
            graph=graph_config,
            qa=qa_config,
            system=system_config,
            **meta_fields,
        )

    def _validate_config(self) -> None:
        """ì„¤ì • ê²€ì¦"""

        errors = []
        warnings = []

        # LLM ì„¤ì • ê²€ì¦
        if not self.config.llm.api_key:
            warnings.append(
                f"No API key found for LLM provider: {self.config.llm.provider}"
            )

        if self.config.llm.temperature < 0 or self.config.llm.temperature > 1:
            errors.append("LLM temperature must be between 0 and 1")

        # ê·¸ë˜í”„ ê²½ë¡œ ê²€ì¦
        if self.config.graph.unified_graph_path:
            graph_path = Path(self.config.graph.unified_graph_path)
            if not graph_path.exists():
                warnings.append(f"Unified graph file not found: {graph_path}")

        if self.config.graph.vector_store_path:
            vector_path = Path(self.config.graph.vector_store_path)
            if not vector_path.exists():
                warnings.append(f"Vector store directory not found: {vector_path}")

        # QA ì„¤ì • ê²€ì¦
        if (
            self.config.qa.min_relevance_score < 0
            or self.config.qa.min_relevance_score > 1
        ):
            errors.append("QA relevance score must be between 0 and 1")

        # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
        if errors:
            error_msg = "Configuration validation failed:\n" + "\n".join(
                f"- {err}" for err in errors
            )
            raise ValueError(error_msg)

        # ê²½ê³  ì¶œë ¥
        if warnings:
            for warning in warnings:
                logger.warning(f"âš ï¸ Config warning: {warning}")

    def update_config(self, **kwargs) -> None:
        """ëŸ°íƒ€ì„ì— ì„¤ì • ì—…ë°ì´íŠ¸"""

        logger.info("ğŸ“ Updating configuration at runtime...")

        # ì¤‘ì²©ëœ ì„¤ì • ì—…ë°ì´íŠ¸ ì§€ì›
        config_dict = asdict(self.config)

        def update_nested(base: Dict, updates: Dict, path: str = "") -> None:
            for key, value in updates.items():
                full_path = f"{path}.{key}" if path else key

                if "." in key:
                    # ì¤‘ì²©ëœ í‚¤ (ì˜ˆ: "llm.temperature")
                    parts = key.split(".", 1)
                    section, sub_key = parts[0], parts[1]

                    if section not in base:
                        base[section] = {}

                    update_nested(base[section], {sub_key: value}, section)
                else:
                    base[key] = value
                    logger.info(f"   Updated {full_path} = {value}")

        update_nested(config_dict, kwargs)

        # ìƒˆë¡œìš´ ì„¤ì •ìœ¼ë¡œ êµì²´
        self.config = self._dict_to_config(config_dict)
        self.config.config_source = ConfigSource.RUNTIME

        # ì—…ë°ì´íŠ¸ëœ ì„¤ì • ì¬ê²€ì¦
        self._validate_config()

    def get_llm_config(self) -> Dict[str, Any]:
        """LLM ì„¤ì •ì„ LangChain í˜¸í™˜ í˜•íƒœë¡œ ë°˜í™˜"""

        llm_config = {
            "model": self.config.llm.model_name,
            "temperature": self.config.llm.temperature,
            "timeout": self.config.llm.timeout,
        }

        if self.config.llm.api_key:
            # ì œê³µìë³„ API í‚¤ ì„¤ì •
            if self.config.llm.provider == "openai":
                llm_config["openai_api_key"] = self.config.llm.api_key
            elif self.config.llm.provider == "anthropic":
                llm_config["anthropic_api_key"] = self.config.llm.api_key
            # í™˜ê²½ ë³€ìˆ˜ë¡œë„ ì„¤ì • (LangChainì´ ìë™ìœ¼ë¡œ ì½ìŒ)
            if self.config.llm.provider == "openai":
                os.environ["OPENAI_API_KEY"] = self.config.llm.api_key
            elif self.config.llm.provider == "anthropic":
                os.environ["ANTHROPIC_API_KEY"] = self.config.llm.api_key

        if self.config.llm.api_base:
            llm_config["base_url"] = self.config.llm.api_base

        if self.config.llm.max_tokens:
            llm_config["max_tokens"] = self.config.llm.max_tokens

        return llm_config

    def get_retriever_config(self) -> Dict[str, Any]:
        """ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ë°˜í™˜"""
        return {
            "unified_graph_path": self.config.graph.unified_graph_path,
            "vector_store_path": self.config.graph.vector_store_path,
            "max_docs": self.config.qa.max_docs,
            "min_relevance_score": self.config.qa.min_relevance_score,
            "enable_caching": self.config.graph.cache_enabled,
        }

    def save_config(self, file_path: Optional[str] = None) -> None:
        """í˜„ì¬ ì„¤ì •ì„ íŒŒì¼ë¡œ ì €ì¥"""

        output_path = Path(file_path) if file_path else self.config_file
        if not output_path:
            output_path = Path("graphrag_config.yaml")

        config_dict = asdict(self.config)

        # ë¯¼ê°í•œ ì •ë³´ ì œê±° (ì €ì¥ì‹œ)
        if "llm" in config_dict and "api_key" in config_dict["llm"]:
            config_dict["llm"]["api_key"] = "***REDACTED***"

        logger.info(f"ğŸ’¾ Saving config to: {output_path}")

        try:
            with open(output_path, "w", encoding="utf-8") as f:
                if output_path.suffix.lower() in [".yml", ".yaml"]:
                    yaml.dump(config_dict, f, default_flow_style=False, indent=2)
                else:
                    json.dump(config_dict, f, indent=2)

            logger.info("âœ… Config saved successfully")

        except Exception as e:
            logger.error(f"âŒ Failed to save config: {e}")
            raise

    def get_config_summary(self) -> str:
        """ì„¤ì • ìš”ì•½ ì •ë³´ ë°˜í™˜"""

        summary = [
            f"GraphRAG Configuration Summary",
            f"================================",
            f"LLM: {self.config.llm.provider} / {self.config.llm.model_name}",
            f"API Key: {'âœ… Set' if self.config.llm.api_key else 'âŒ Missing'}",
            f"Embedding Model: {self.config.embedding.model_name}",
            f"Graph Path: {self.config.graph.unified_graph_path or 'Not set'}",
            f"Vector Store: {self.config.graph.vector_store_path or 'Not set'}",
            f"Log Level: {self.config.system.log_level}",
            f"Config Source: {self.config.config_source.name}",
        ]

        return "\n".join(summary)


# í¸ì˜ í•¨ìˆ˜ë“¤
def load_config(
    config_file: Optional[str] = None, env_file: Optional[str] = None
) -> GraphRAGConfigManager:
    """ì„¤ì • ë§¤ë‹ˆì € ìƒì„± ë° ë¡œë”©"""
    return GraphRAGConfigManager(config_file=config_file, env_file=env_file)


def create_sample_config(file_path: str = "graphrag_config.yaml") -> None:
    """ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„±"""

    sample_config = {
        "llm": {
            "provider": "openai",
            "model_name": "gpt-4",
            "temperature": 0.1,
            "max_tokens": 2000,
        },
        "embedding": {"model_name": "auto", "device": "auto", "batch_size": 32},
        "graph": {
            "unified_graph_path": "./graphs/unified/unified_knowledge_graph.json",
            "vector_store_path": "./graphs/embeddings",
            "cache_enabled": True,
        },
        "qa": {
            "chain_type": "retrieval_qa",
            "max_docs": 10,
            "min_relevance_score": 0.3,
            "enable_memory": False,
        },
        "system": {"log_level": "INFO", "verbose": False, "max_workers": 4},
    }

    with open(file_path, "w", encoding="utf-8") as f:
        yaml.dump(sample_config, f, default_flow_style=False, indent=2)

    print(f"âœ… Sample config created: {file_path}")


def main():
    """ConfigManager í…ŒìŠ¤íŠ¸"""

    print("ğŸ§ª Testing GraphRAGConfigManager...")

    try:
        # 1. ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”
        config_manager = GraphRAGConfigManager(auto_load=False)
        print("âœ… Basic initialization successful")

        # 2. ì„¤ì • ì—…ë°ì´íŠ¸
        config_manager.update_config(
            **{
                "llm.temperature": 0.2,
                "llm.model_name": "gpt-4",
                "qa.max_docs": 15,
                "system.verbose": True,
            }
        )
        print("âœ… Configuration update successful")

        # 3. LLM ì„¤ì • ì¡°íšŒ
        llm_config = config_manager.get_llm_config()
        print(f"ğŸ¤– LLM Config: {llm_config}")

        # 4. ì„¤ì • ìš”ì•½
        print(f"\nğŸ“‹ Configuration Summary:")
        print(config_manager.get_config_summary())

        # 5. ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„±
        create_sample_config("test_config.yaml")

        print(f"\nâœ… GraphRAGConfigManager test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
