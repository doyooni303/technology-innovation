"""
GraphRAG ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ
Configuration Manager for GraphRAG System

ì‹œìŠ¤í…œ ì „ì²´ ì„¤ì • ë° ì¸ì¦ ì •ë³´ í†µí•© ê´€ë¦¬
- í™˜ê²½ ë³€ìˆ˜ ë° .env íŒŒì¼ ì§€ì›
- YAML/JSON ì„¤ì • íŒŒì¼ ë¡œë”©
- API í‚¤ ë° ì¸ì¦ ì •ë³´ ë³´ì•ˆ ê´€ë¦¬
- ì„¤ì • ê²€ì¦ ë° ê¸°ë³¸ê°’ ì œê³µ
- ëŸ°íƒ€ì„ ì„¤ì • ì—…ë°ì´íŠ¸
"""

import os
import json
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import warnings

# ì„¤ì • íŒŒì¼ ë¡œë”©
try:
    from dotenv import load_dotenv

    _dotenv_available = True
except ImportError:
    _dotenv_available = False
    warnings.warn(
        "python-dotenv not available. Install with: pip install python-dotenv"
    )

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class ConfigSource(Enum):
    """ì„¤ì • ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ (ë†’ì€ ìˆ«ìê°€ ìš°ì„ )"""

    DEFAULT = 1
    CONFIG_FILE = 2
    ENV_FILE = 3
    ENVIRONMENT = 4
    RUNTIME = 5


@dataclass
class LLMConfig:
    """LLM ì„¤ì • - YAML êµ¬ì¡°ì— ë§ì¶¤"""

    provider: str = "huggingface_local"  # YAML ê¸°ë³¸ê°’ì— ë§ì¶¤
    model_name: str = "gpt-3.5-turbo"
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    temperature: float = 0.1
    max_tokens: Optional[int] = None
    streaming: bool = False
    timeout: int = 60

    # ë¡œì»¬ ëª¨ë¸ ì„¤ì • (YAMLì—ì„œ huggingface_local ì„¹ì…˜)
    model_path: Optional[str] = (
        "/DATA/MODELS/models--meta-llama--Llama-3.1-8B-Instruct"  # YAML ê¸°ë³¸ê°’
    )
    device_map: str = "auto"  # GPU í• ë‹¹
    torch_dtype: str = "bfloat16"  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
    max_new_tokens: int = 2048  # ìƒì„± í† í° ìˆ˜
    trust_remote_code: bool = True  # HF ëª¨ë¸ìš©
    load_in_8bit: bool = False  # ì–‘ìí™” ì˜µì…˜
    load_in_4bit: bool = False  # ë” ê°•í•œ ì–‘ìí™”

    # ìƒì„± ì„¤ì • (YAMLì— ë§ì¶¤)
    do_sample: bool = True
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.1
    batch_size: int = 1


@dataclass
class PathConfig:
    """ê²½ë¡œ ì„¤ì • í´ë˜ìŠ¤ - YAML paths ì„¹ì…˜ì— ë§ì¶¤"""

    # ê¸°ë³¸ ë””ë ‰í† ë¦¬ (YAMLê³¼ ì¼ì¹˜)
    data_dir: str = "./data"
    processed_dir: str = "./data/processed"

    # ê·¸ë˜í”„ ê´€ë ¨ (YAML pathsì— ë§ì¶¤)
    unified_graph: str = "./data/processed/graphs/unified/unified_knowledge_graph.json"
    individual_graphs_dir: str = "./data/processed/graphs"

    # ë²¡í„° ì €ì¥ì†Œ êµ¬ì¡° (YAML vector_store ì„¹ì…˜ê³¼ ì¼ì¹˜)
    vector_store_root: str = "./data/processed/vector_store"
    vector_store_embeddings: str = "./data/processed/vector_store/embeddings"
    vector_store_faiss: str = "./data/processed/vector_store/faiss"
    vector_store_chromadb: str = "./data/processed/vector_store/chromadb"
    vector_store_simple: str = "./data/processed/vector_store/simple"

    # ìºì‹œ ë””ë ‰í† ë¦¬ (YAMLê³¼ ì¼ì¹˜)
    cache_dir: str = "./cache"
    embeddings_cache: str = "./cache/embeddings"
    query_cache: str = "./cache/queries"
    logs_dir: str = "./logs"

    # ì„œë²„ ëª¨ë¸ ê²½ë¡œ (YAMLê³¼ ì¼ì¹˜)
    models_dir: str = "/DATA/MODELS"


@dataclass
class EmbeddingConfig:
    """ì„ë² ë”© ì„¤ì • - YAML embeddings ì„¹ì…˜ì— ë§ì¶¤"""

    model_type: str = "sentence-transformers"  # YAML ê¸°ë³¸ê°’
    model_name: str = "paraphrase-multilingual-mpnet-base-v2"  # YAML ê¸°ë³¸ê°’
    device: str = "auto"
    batch_size: int = 32
    max_length: int = 512
    cache_dir: str = "./cache/embeddings"

    # ì„ë² ë”© íŒŒì¼ ì €ì¥ ê²½ë¡œ (YAMLê³¼ ì¼ì¹˜)
    save_directory: str = "./data/processed/vector_store/embeddings"


@dataclass
class VectorStoreConfig:
    """ë²¡í„° ì €ì¥ì†Œ ì„¤ì • - YAML vector_store ì„¹ì…˜ì— ë§ì¶¤"""

    store_type: str = "faiss"  # YAML ê¸°ë³¸ê°’
    persist_directory: str = "./data/processed/vector_store"  # YAML ë£¨íŠ¸ ê²½ë¡œ
    collection_name: str = "graphrag_embeddings"
    distance_metric: str = "cosine"
    index_type: str = "flat"
    batch_size: int = 128  # YAML ê°’ì— ë§ì¶¤
    cache_size: int = 10000

    # ì¤‘ì²© êµ¬ì¡° ì €ì¥ì†Œë³„ ì„¤ì • (YAMLì—ì„œ ë¡œë“œ)
    faiss: Optional[Dict[str, Any]] = None
    chromadb: Optional[Dict[str, Any]] = None
    simple: Optional[Dict[str, Any]] = None

    # ê¸°ì¡´ í‰ë©´í™”ëœ ì†ì„±ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±ìš©)
    faiss_directory: str = "./data/processed/vector_store/faiss"
    chromadb_directory: str = "./data/processed/vector_store/chromadb"
    simple_directory: str = "./data/processed/vector_store/simple"

    # FAISS ê´€ë ¨ ì„¤ì •ë“¤ (YAML ê¸°ë³¸ê°’ì— ë§ì¶¤)
    faiss_index_type: str = "flat"
    faiss_distance_metric: str = "cosine"
    use_gpu: bool = False  # YAMLì—ì„œ ê°œë°œ ë‹¨ê³„ëŠ” CPU ì‚¬ìš©
    gpu_id: int = 0
    gpu_memory_fraction: float = 0.5

    # ChromaDB ê´€ë ¨ ì„¤ì •ë“¤
    chromadb_collection_name: str = "graphrag_embeddings"
    chromadb_distance_metric: str = "cosine"

    def __post_init__(self):
        """ì„œë¸Œ ë””ë ‰í† ë¦¬ ìë™ ì„¤ì • - ì¤‘ì²© êµ¬ì¡°ì™€ í‰ë©´ êµ¬ì¡° ëª¨ë‘ ì§€ì›"""
        # ì¤‘ì²© êµ¬ì¡°ì—ì„œ í‰ë©´ êµ¬ì¡°ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
        if self.faiss and isinstance(self.faiss, dict):
            self.faiss_directory = self.faiss.get(
                "persist_directory", f"{self.persist_directory}/faiss"
            )
            self.faiss_index_type = self.faiss.get("index_type", "flat")
            self.faiss_distance_metric = self.faiss.get("distance_metric", "cosine")
            self.use_gpu = self.faiss.get("use_gpu", False)
            self.gpu_id = self.faiss.get("gpu_id", 0)
            self.gpu_memory_fraction = self.faiss.get("gpu_memory_fraction", 0.5)

        if self.chromadb and isinstance(self.chromadb, dict):
            self.chromadb_directory = self.chromadb.get(
                "persist_directory", f"{self.persist_directory}/chromadb"
            )
            self.chromadb_collection_name = self.chromadb.get(
                "collection_name", "graphrag_embeddings"
            )
            self.chromadb_distance_metric = self.chromadb.get(
                "distance_metric", "cosine"
            )

        if self.simple and isinstance(self.simple, dict):
            self.simple_directory = self.simple.get(
                "persist_directory", f"{self.persist_directory}/simple"
            )


@dataclass
class GraphConfig:
    """ê·¸ë˜í”„ ì„¤ì • - YAML graph ì„¹ì…˜ì— ë§ì¶¤"""

    unified_graph_path: str = (
        "./data/processed/graphs/unified/unified_knowledge_graph.json"
    )
    vector_store_path: str = "./data/processed/vector_store"  # ë£¨íŠ¸ ê²½ë¡œ
    graphs_directory: str = "./data/processed/graphs"
    cache_enabled: bool = True
    cache_ttl_hours: int = 24


@dataclass
class QAConfig:
    """QA ì²´ì¸ ì„¤ì •"""

    chain_type: str = "retrieval_qa"
    max_docs: int = 10
    min_relevance_score: float = 0.3
    return_source_documents: bool = True
    enable_memory: bool = False
    memory_type: str = "buffer"
    max_memory_tokens: int = 4000


@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • - YAML ì„±ëŠ¥ ìµœì í™” ì„¹ì…˜ ë°˜ì˜"""

    log_level: str = "INFO"
    verbose: bool = False
    parallel_processing: bool = True
    max_workers: int = 4  # YAML performance ì„¹ì…˜ê³¼ ì¼ì¹˜
    temp_directory: str = "./tmp"
    enable_monitoring: bool = False

    # YAML performance ì„¹ì…˜ ì„¤ì •ë“¤ ì¶”ê°€
    enable_parallel: bool = True
    enable_caching: bool = True
    cache_size_limit: str = "8GB"
    batch_processing: bool = True
    memory_limit: str = "16GB"

    # ê³ ê¸‰ ìµœì í™” (YAMLì—ì„œ)
    enable_flash_attention: bool = True
    enable_model_parallelism: bool = True


@dataclass
class GraphRAGConfig:
    """GraphRAG ì „ì²´ ì„¤ì • - YAML êµ¬ì¡°ì— ì™„ì „íˆ ë§ì¶¤"""

    # í•˜ìœ„ ì„¤ì •ë“¤ - ê¸°ë³¸ê°’ë“¤ì´ YAMLê³¼ ì¼ì¹˜
    llm: LLMConfig = field(default_factory=LLMConfig)
    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)
    graph: GraphConfig = field(default_factory=GraphConfig)
    qa: QAConfig = field(default_factory=QAConfig)
    system: SystemConfig = field(default_factory=SystemConfig)

    # ê²½ë¡œ ë° ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
    paths: PathConfig = field(default_factory=PathConfig)
    vector_store: VectorStoreConfig = field(
        default_factory=lambda: VectorStoreConfig(store_type="faiss")
    )

    # ë©”íƒ€ë°ì´í„°
    version: str = "1.0.0"
    config_source: ConfigSource = ConfigSource.DEFAULT
    last_updated: Optional[str] = None


@dataclass
class QAConfig:
    """QA ì²´ì¸ ì„¤ì •"""

    chain_type: str = "retrieval_qa"
    max_docs: int = 10
    min_relevance_score: float = 0.3
    return_source_documents: bool = True
    enable_memory: bool = False
    memory_type: str = "buffer"
    max_memory_tokens: int = 4000


@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì •"""

    log_level: str = "INFO"
    verbose: bool = False
    parallel_processing: bool = True
    max_workers: int = 4
    temp_directory: str = "./tmp"
    enable_monitoring: bool = False


class GraphRAGConfigManager:
    """GraphRAG ì„¤ì • ê´€ë¦¬ì"""

    def __init__(
        self,
        config_file: Optional[str] = None,
        env_file: Optional[str] = None,
        auto_load: bool = True,
    ):
        """
        Args:
            config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ (YAML/JSON)
            env_file: .env íŒŒì¼ ê²½ë¡œ
            auto_load: ìë™ ë¡œë”© ì—¬ë¶€
        """
        self.config_file = Path(config_file) if config_file else None
        self.env_file = Path(env_file) if env_file else None

        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œì‘
        self.config = GraphRAGConfig()

        # ì„¤ì • ì†ŒìŠ¤ë³„ ì €ì¥ (ë””ë²„ê¹…ìš©)
        self.config_sources: Dict[str, Any] = {}

        if auto_load:
            self.load_all()

        logger.info("âœ… GraphRAGConfigManager initialized")

    def load_all(self) -> None:
        """ëª¨ë“  ì„¤ì • ì†ŒìŠ¤ë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¡œë”©"""

        logger.info("ğŸ”§ Loading configuration from all sources...")

        # 1. ê¸°ë³¸ ì„¤ì • (ì´ë¯¸ ì ìš©ë¨)
        self.config_sources["default"] = asdict(self.config)

        # 2. ì„¤ì • íŒŒì¼
        if self.config_file and self.config_file.exists():
            self._load_config_file()

        # 3. .env íŒŒì¼
        if self.env_file and self.env_file.exists():
            self._load_env_file()
        elif Path(".env").exists():
            self._load_env_file(Path(".env"))

        # 4. í™˜ê²½ ë³€ìˆ˜
        self._load_environment_variables()

        # 5. ì„¤ì • ê²€ì¦
        self._validate_config()

        logger.info("âœ… Configuration loaded successfully")

    def _load_config_file(self) -> None:
        """ì„¤ì • íŒŒì¼ ë¡œë”© (YAML/JSON)"""

        logger.info(f"ğŸ“‚ Loading config file: {self.config_file}")

        try:
            with open(self.config_file, "r", encoding="utf-8") as f:
                if self.config_file.suffix.lower() in [".yml", ".yaml"]:
                    file_config = yaml.safe_load(f)
                elif self.config_file.suffix.lower() == ".json":
                    file_config = json.load(f)
                else:
                    raise ValueError(
                        f"Unsupported config file format: {self.config_file.suffix}"
                    )

            self.config_sources["config_file"] = file_config
            self._merge_config(file_config, ConfigSource.CONFIG_FILE)

            logger.info("âœ… Config file loaded")

        except Exception as e:
            logger.error(f"âŒ Failed to load config file: {e}")
            raise

    def _load_env_file(self, env_path: Optional[Path] = None) -> None:
        """í™˜ê²½ íŒŒì¼ ë¡œë”©"""

        if not _dotenv_available:
            logger.warning("âš ï¸ python-dotenv not available, skipping .env file")
            return

        env_path = env_path or self.env_file
        logger.info(f"ğŸ” Loading environment file: {env_path}")

        try:
            # .env íŒŒì¼ ë¡œë”©
            load_dotenv(env_path, override=False)

            # GraphRAG ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ë“¤ ì¶”ì¶œ
            env_config = self._extract_env_variables()
            self.config_sources["env_file"] = env_config
            self._merge_config(env_config, ConfigSource.ENV_FILE)

            logger.info("âœ… Environment file loaded")

        except Exception as e:
            logger.error(f"âŒ Failed to load environment file: {e}")

    def _load_environment_variables(self) -> None:
        """í™˜ê²½ ë³€ìˆ˜ ë¡œë”©"""

        logger.info("ğŸŒ Loading environment variables...")

        env_config = self._extract_env_variables()
        self.config_sources["environment"] = env_config
        self._merge_config(env_config, ConfigSource.ENVIRONMENT)

    def _extract_env_variables(self) -> Dict[str, Any]:
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ GraphRAG ì„¤ì • ì¶”ì¶œ"""

        env_config = {}

        # LLM ì„¤ì •
        llm_config = {}
        if os.getenv("OPENAI_API_KEY"):
            llm_config["api_key"] = os.getenv("OPENAI_API_KEY")
            llm_config["provider"] = "openai"
        if os.getenv("ANTHROPIC_API_KEY"):
            llm_config["api_key"] = os.getenv("ANTHROPIC_API_KEY")
            llm_config["provider"] = "anthropic"
        if os.getenv("HUGGINGFACE_API_TOKEN"):
            llm_config["api_key"] = os.getenv("HUGGINGFACE_API_TOKEN")
            llm_config["provider"] = "huggingface"

        if os.getenv("GRAPHRAG_LLM_MODEL"):
            llm_config["model_name"] = os.getenv("GRAPHRAG_LLM_MODEL")
        if os.getenv("GRAPHRAG_LLM_TEMPERATURE"):
            llm_config["temperature"] = float(os.getenv("GRAPHRAG_LLM_TEMPERATURE"))
        if os.getenv("GRAPHRAG_LLM_MAX_TOKENS"):
            llm_config["max_tokens"] = int(os.getenv("GRAPHRAG_LLM_MAX_TOKENS"))

        if os.getenv("GRAPHRAG_LOCAL_MODEL_PATH"):
            llm_config["model_path"] = os.getenv("GRAPHRAG_LOCAL_MODEL_PATH")
            llm_config["provider"] = "huggingface_local"

        if os.getenv("GRAPHRAG_MODEL_DEVICE_MAP"):
            llm_config["device_map"] = os.getenv("GRAPHRAG_MODEL_DEVICE_MAP")

        if os.getenv("GRAPHRAG_MODEL_DTYPE"):
            llm_config["torch_dtype"] = os.getenv("GRAPHRAG_MODEL_DTYPE")

        if llm_config:
            env_config["llm"] = llm_config

        # ì„ë² ë”© ì„¤ì •
        embedding_config = {}
        if os.getenv("GRAPHRAG_EMBEDDING_MODEL"):
            embedding_config["model_name"] = os.getenv("GRAPHRAG_EMBEDDING_MODEL")
        if os.getenv("GRAPHRAG_EMBEDDING_DEVICE"):
            embedding_config["device"] = os.getenv("GRAPHRAG_EMBEDDING_DEVICE")
        if os.getenv("GRAPHRAG_EMBEDDING_BATCH_SIZE"):
            embedding_config["batch_size"] = int(
                os.getenv("GRAPHRAG_EMBEDDING_BATCH_SIZE")
            )

        if embedding_config:
            env_config["embedding"] = embedding_config

        # ê·¸ë˜í”„ ì„¤ì •
        graph_config = {}
        if os.getenv("GRAPHRAG_UNIFIED_GRAPH_PATH"):
            graph_config["unified_graph_path"] = os.getenv(
                "GRAPHRAG_UNIFIED_GRAPH_PATH"
            )
        if os.getenv("GRAPHRAG_VECTOR_STORE_PATH"):
            graph_config["vector_store_path"] = os.getenv("GRAPHRAG_VECTOR_STORE_PATH")
        if os.getenv("GRAPHRAG_GRAPHS_DIRECTORY"):
            graph_config["graphs_directory"] = os.getenv("GRAPHRAG_GRAPHS_DIRECTORY")

        if graph_config:
            env_config["graph"] = graph_config

        # ì‹œìŠ¤í…œ ì„¤ì •
        system_config = {}
        if os.getenv("GRAPHRAG_LOG_LEVEL"):
            system_config["log_level"] = os.getenv("GRAPHRAG_LOG_LEVEL")
        if os.getenv("GRAPHRAG_VERBOSE"):
            system_config["verbose"] = os.getenv("GRAPHRAG_VERBOSE").lower() == "true"
        if os.getenv("GRAPHRAG_MAX_WORKERS"):
            system_config["max_workers"] = int(os.getenv("GRAPHRAG_MAX_WORKERS"))

        if system_config:
            env_config["system"] = system_config

        return env_config

    def _merge_config(self, new_config: Dict[str, Any], source: ConfigSource) -> None:
        """ìƒˆë¡œìš´ ì„¤ì •ì„ ê¸°ì¡´ ì„¤ì •ì— ë³‘í•©"""

        # ê¹Šì€ ë³‘í•© ìˆ˜í–‰
        def deep_merge(base: Dict, update: Dict) -> Dict:
            for key, value in update.items():
                if (
                    key in base
                    and isinstance(base[key], dict)
                    and isinstance(value, dict)
                ):
                    deep_merge(base[key], value)
                else:
                    base[key] = value
            return base

        # dataclassë¥¼ dictë¡œ ë³€í™˜
        config_dict = asdict(self.config)

        # ë³‘í•©
        deep_merge(config_dict, new_config)

        # dictë¥¼ ë‹¤ì‹œ dataclassë¡œ ë³€í™˜
        self.config = self._dict_to_config(config_dict)
        self.config.config_source = source

    def _dict_to_config(self, config_dict: Dict[str, Any]) -> GraphRAGConfig:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ GraphRAGConfigë¡œ ë³€í™˜ (ì¤‘ì²© êµ¬ì¡° ì§€ì›)"""
        print(config_dict.keys())
        import pdb

        pdb.set_trace()
        # LLM ì„¤ì • ì²˜ë¦¬ - ì¤‘ì²© êµ¬ì¡° í‰ë©´í™”
        llm_data = config_dict.get("llm", {}).copy()
        embedding_data = config_dict.get("embedding", {}).copy()
        if not embedding_data:
            embedding_data = config_dict.get("embeddings", {}).copy()

        if "provider" in llm_data:
            provider = llm_data["provider"]

            # providerë³„ ì¤‘ì²© ì„¤ì • í‰ë©´í™”
            if provider in llm_data:  # huggingface_local, openai, anthropic ë“±
                logger.info(f"ğŸ”§ Processing nested config for provider: {provider}")

                nested_config = llm_data[provider]

                # ì¤‘ì²©ëœ ì„¤ì •ì„ ìƒìœ„ë¡œ ë³‘í•© (ê¸°ì¡´ í‚¤ ìš°ì„ )
                for key, value in nested_config.items():
                    if key not in llm_data:  # ê¸°ì¡´ í‚¤ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                        llm_data[key] = value
                        logger.debug(f"   Added {key} = {value}")

                # ì¤‘ì²© ì„¹ì…˜ ì œê±°
                del llm_data[provider]
                logger.info(f"âœ… Flattened {provider} config")

        # ì„ë² ë”© ì„¤ì • ì²˜ë¦¬ - ì €ì¥ ê²½ë¡œ ì¶”ê°€
        if "model_type" in embedding_data:
            model_type = embedding_data["model_type"]
            if model_type in embedding_data:
                logger.info(f"ğŸ”§ Processing nested embedding config for: {model_type}")
                nested_config = embedding_data[model_type]
                for key, value in nested_config.items():
                    if key not in embedding_data:
                        embedding_data[key] = value
                        logger.debug(f"   Added embedding {key} = {value}")
                del embedding_data[model_type]
                logger.info(f"âœ… Flattened {model_type} embedding config")

        # ê²½ë¡œ ì„¤ì • ì²˜ë¦¬
        paths_data = config_dict.get("paths", {})

        # pathsì—ì„œ vector_store ì„¹ì…˜ ì²˜ë¦¬
        if "vector_store" in paths_data:
            vs_paths = paths_data["vector_store"]
            paths_data.update(
                {
                    "vector_store_embeddings": vs_paths.get("embeddings", ""),
                    "vector_store_faiss": vs_paths.get("faiss", ""),
                    "vector_store_chromadb": vs_paths.get("chromadb", ""),
                    "vector_store_simple": vs_paths.get("simple", ""),
                }
            )
            # ì¤‘ì²© êµ¬ì¡° ì œê±°
            del paths_data["vector_store"]

        # ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ì²˜ë¦¬
        vector_store_data = config_dict.get("vector_store", {}).copy()

        # YAMLì—ì„œ ë¡œë“œëœ ì¤‘ì²© êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ í•„ìš”í•œ í•„ë“œë§Œ í‰ë©´í™”
        store_type = vector_store_data.get("store_type", "faiss")

        # ê° ì €ì¥ì†Œë³„ ì¤‘ì²© ì„¤ì •ì„ attributesë¡œ ë³´ì¡´
        if "faiss" in vector_store_data:
            vector_store_data["faiss"] = vector_store_data["faiss"]  # ì¤‘ì²© êµ¬ì¡° ë³´ì¡´
        if "chromadb" in vector_store_data:
            vector_store_data["chromadb"] = vector_store_data[
                "chromadb"
            ]  # ì¤‘ì²© êµ¬ì¡° ë³´ì¡´
        if "simple" in vector_store_data:
            vector_store_data["simple"] = vector_store_data["simple"]  # ì¤‘ì²© êµ¬ì¡° ë³´ì¡´

        # ê¸°ë³¸ê°’ ì„¤ì •
        if "store_type" not in vector_store_data:
            vector_store_data["store_type"] = store_type

        # ì„ë² ë”©ì— ì €ì¥ ê²½ë¡œ ì¶”ê°€
        if "save_directory" not in embedding_data:
            embedding_data["save_directory"] = (
                "./data/processed/vector_store/embeddings"
            )

        # ê° ì„¤ì • í´ë˜ìŠ¤ ìƒì„±
        try:
            llm_config = LLMConfig(**llm_data)
            logger.info("âœ… LLMConfig created successfully")
        except Exception as e:
            logger.error(f"âŒ LLMConfig creation failed: {e}")
            llm_config = LLMConfig()

        try:
            embedding_config = EmbeddingConfig(**embedding_data)
            logger.info("âœ… EmbeddingConfig created successfully")
        except Exception as e:
            logger.error(f"âŒ EmbeddingConfig creation failed: {e}")
            embedding_config = EmbeddingConfig()

        try:
            paths_config = PathConfig(**paths_data)
            logger.info("âœ… PathConfig created successfully")
        except Exception as e:
            logger.error(f"âŒ PathConfig creation failed: {e}")
            paths_config = PathConfig()

        try:
            vector_store_config = VectorStoreConfig(**vector_store_data)
            logger.info("âœ… VectorStoreConfig created successfully")
        except Exception as e:
            logger.error(f"âŒ VectorStoreConfig creation failed: {e}")
            vector_store_config = VectorStoreConfig()

        print()
        # ë‚˜ë¨¸ì§€ ì„¤ì •ë“¤
        graph_config = GraphConfig(**config_dict.get("graph", {}))
        qa_config = QAConfig(**config_dict.get("qa", {}))
        system_config = SystemConfig(**config_dict.get("system", {}))

        # ë©”íƒ€ë°ì´í„°
        meta_fields = {
            "version": config_dict.get("version", "1.0.0"),
            "config_source": config_dict.get("config_source", ConfigSource.DEFAULT),
            "last_updated": config_dict.get("last_updated"),
        }

        return GraphRAGConfig(
            llm=llm_config,
            embedding=embedding_config,
            graph=graph_config,
            qa=qa_config,
            system=system_config,
            paths=paths_config,
            vector_store=vector_store_config,
            **meta_fields,
        )

    def _validate_config(self) -> None:
        """ì„¤ì • ê²€ì¦"""

        errors = []
        warnings = []

        # LLM ì„¤ì • ê²€ì¦
        if not self.config.llm.api_key:
            warnings.append(
                f"No API key found for LLM provider: {self.config.llm.provider}"
            )

        if self.config.llm.temperature < 0 or self.config.llm.temperature > 1:
            errors.append("LLM temperature must be between 0 and 1")

        # ğŸ†• ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ê²€ì¦
        if self.config.llm.provider == "huggingface_local":
            if not self.config.llm.model_path:
                errors.append(
                    "Local model path is required for huggingface_local provider"
                )
            elif not Path(self.config.llm.model_path).exists():
                warnings.append(
                    f"Local model path not found: {self.config.llm.model_path}"
                )

        # ê·¸ë˜í”„ ê²½ë¡œ ê²€ì¦
        if self.config.graph.unified_graph_path:
            graph_path = Path(self.config.graph.unified_graph_path)
            if not graph_path.exists():
                warnings.append(f"Unified graph file not found: {graph_path}")

        if self.config.graph.vector_store_path:
            vector_path = Path(self.config.graph.vector_store_path)
            if not vector_path.exists():
                warnings.append(f"Vector store directory not found: {vector_path}")

        # QA ì„¤ì • ê²€ì¦
        if (
            self.config.qa.min_relevance_score < 0
            or self.config.qa.min_relevance_score > 1
        ):
            errors.append("QA relevance score must be between 0 and 1")

        # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
        if errors:
            error_msg = "Configuration validation failed:\n" + "\n".join(
                f"- {err}" for err in errors
            )
            raise ValueError(error_msg)

        # ê²½ê³  ì¶œë ¥
        if warnings:
            for warning in warnings:
                logger.warning(f"âš ï¸ Config warning: {warning}")

    def update_config(self, **kwargs) -> None:
        """ëŸ°íƒ€ì„ì— ì„¤ì • ì—…ë°ì´íŠ¸"""

        logger.info("ğŸ“ Updating configuration at runtime...")

        # ì¤‘ì²©ëœ ì„¤ì • ì—…ë°ì´íŠ¸ ì§€ì›
        config_dict = asdict(self.config)

        def update_nested(base: Dict, updates: Dict, path: str = "") -> None:
            for key, value in updates.items():
                full_path = f"{path}.{key}" if path else key

                if "." in key:
                    # ì¤‘ì²©ëœ í‚¤ (ì˜ˆ: "llm.temperature")
                    parts = key.split(".", 1)
                    section, sub_key = parts[0], parts[1]

                    if section not in base:
                        base[section] = {}

                    update_nested(base[section], {sub_key: value}, section)
                else:
                    base[key] = value
                    logger.info(f"   Updated {full_path} = {value}")

        update_nested(config_dict, kwargs)

        # ìƒˆë¡œìš´ ì„¤ì •ìœ¼ë¡œ êµì²´
        self.config = self._dict_to_config(config_dict)
        self.config.config_source = ConfigSource.RUNTIME

        # ì—…ë°ì´íŠ¸ëœ ì„¤ì • ì¬ê²€ì¦
        self._validate_config()

    def get_llm_config(self) -> Dict[str, Any]:
        """LLM ì„¤ì •ì„ LangChain í˜¸í™˜ í˜•íƒœë¡œ ë°˜í™˜"""
        if self.config.llm.provider == "huggingface_local":
            return {
                "model_path": self.config.llm.model_path,
                "device_map": self.config.llm.device_map,
                "torch_dtype": self.config.llm.torch_dtype,
                "max_new_tokens": self.config.llm.max_new_tokens,
                "temperature": self.config.llm.temperature,
                "trust_remote_code": self.config.llm.trust_remote_code,
                "load_in_8bit": self.config.llm.load_in_8bit,
                "load_in_4bit": self.config.llm.load_in_4bit,
            }
        llm_config = {
            "model": self.config.llm.model_name,
            "temperature": self.config.llm.temperature,
            "timeout": self.config.llm.timeout,
        }

        if self.config.llm.api_key:
            # ì œê³µìë³„ API í‚¤ ì„¤ì •
            if self.config.llm.provider == "openai":
                llm_config["openai_api_key"] = self.config.llm.api_key
            elif self.config.llm.provider == "anthropic":
                llm_config["anthropic_api_key"] = self.config.llm.api_key
            # í™˜ê²½ ë³€ìˆ˜ë¡œë„ ì„¤ì • (LangChainì´ ìë™ìœ¼ë¡œ ì½ìŒ)
            if self.config.llm.provider == "openai":
                os.environ["OPENAI_API_KEY"] = self.config.llm.api_key
            elif self.config.llm.provider == "anthropic":
                os.environ["ANTHROPIC_API_KEY"] = self.config.llm.api_key

        if self.config.llm.api_base:
            llm_config["base_url"] = self.config.llm.api_base

        if self.config.llm.max_tokens:
            llm_config["max_tokens"] = self.config.llm.max_tokens

        return llm_config

    def get_retriever_config(self) -> Dict[str, Any]:
        """ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ë°˜í™˜"""
        return {
            "unified_graph_path": self.config.graph.unified_graph_path,
            "vector_store_path": self.config.graph.vector_store_path,
            "max_docs": self.config.qa.max_docs,
            "min_relevance_score": self.config.qa.min_relevance_score,
            "enable_caching": self.config.graph.cache_enabled,
        }

    def get_vector_store_config(
        self, store_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """ë²¡í„° ì €ì¥ì†Œ ì„¤ì •ì„ íƒ€ì…ë³„ë¡œ ë°˜í™˜ - YAML ì¤‘ì²© êµ¬ì¡° ì§€ì›"""

        if store_type is None:
            store_type = self.config.vector_store.store_type

        base_config = {
            "store_type": store_type,
            "batch_size": self.config.vector_store.batch_size,
            "persist_directory": self.config.vector_store.persist_directory,
        }

        if store_type == "faiss":
            # YAMLì˜ vector_store.faiss ì„¹ì…˜ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            faiss_config = getattr(self.config.vector_store, "faiss", {})

            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°ì™€ ê°ì²´ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            if isinstance(faiss_config, dict):
                persist_dir = faiss_config.get(
                    "persist_directory",
                    self.config.vector_store.persist_directory + "/faiss",
                )
                index_type = faiss_config.get("index_type", "flat")
                distance_metric = faiss_config.get("distance_metric", "cosine")
                use_gpu = faiss_config.get("use_gpu", False)
                gpu_id = faiss_config.get("gpu_id", 0)
                gpu_memory_fraction = faiss_config.get("gpu_memory_fraction", 0.5)
            else:
                # ê¸°ë³¸ê°’ ì‚¬ìš© (í‰ë©´í™”ë˜ì§€ ì•Šì€ ê²½ìš°)
                persist_dir = getattr(
                    self.config.vector_store,
                    "faiss_directory",
                    self.config.vector_store.persist_directory + "/faiss",
                )
                index_type = getattr(
                    self.config.vector_store, "faiss_index_type", "flat"
                )
                distance_metric = getattr(
                    self.config.vector_store, "faiss_distance_metric", "cosine"
                )
                use_gpu = getattr(self.config.vector_store, "use_gpu", False)
                gpu_id = getattr(self.config.vector_store, "gpu_id", 0)
                gpu_memory_fraction = getattr(
                    self.config.vector_store, "gpu_memory_fraction", 0.5
                )

            # ì¤‘ë³µ ê²½ë¡œ ë°©ì§€
            if persist_dir.endswith("/faiss/faiss"):
                persist_dir = persist_dir.replace("/faiss/faiss", "/faiss")

            base_config.update(
                {
                    "persist_directory": persist_dir,
                    "index_type": index_type,
                    "distance_metric": distance_metric,
                    "use_gpu": use_gpu,
                    "gpu_id": gpu_id,
                    "gpu_memory_fraction": gpu_memory_fraction,
                }
            )

        elif store_type == "chromadb":
            # YAMLì˜ vector_store.chromadb ì„¹ì…˜ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            chromadb_config = getattr(self.config.vector_store, "chromadb", {})

            if isinstance(chromadb_config, dict):
                persist_dir = chromadb_config.get(
                    "persist_directory",
                    self.config.vector_store.persist_directory + "/chromadb",
                )
                collection_name = chromadb_config.get(
                    "collection_name", "graphrag_embeddings"
                )
                distance_metric = chromadb_config.get("distance_metric", "cosine")
            else:
                # ê¸°ë³¸ê°’ ì‚¬ìš©
                persist_dir = getattr(
                    self.config.vector_store,
                    "chromadb_directory",
                    self.config.vector_store.persist_directory + "/chromadb",
                )
                collection_name = getattr(
                    self.config.vector_store,
                    "chromadb_collection_name",
                    "graphrag_embeddings",
                )
                distance_metric = getattr(
                    self.config.vector_store, "chromadb_distance_metric", "cosine"
                )

            # ì¤‘ë³µ ê²½ë¡œ ë°©ì§€
            if persist_dir.endswith("/chromadb/chromadb"):
                persist_dir = persist_dir.replace("/chromadb/chromadb", "/chromadb")

            base_config.update(
                {
                    "persist_directory": persist_dir,
                    "collection_name": collection_name,
                    "distance_metric": distance_metric,
                }
            )

        elif store_type == "simple":
            # YAMLì˜ vector_store.simple ì„¹ì…˜ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            simple_config = getattr(self.config.vector_store, "simple", {})

            if isinstance(simple_config, dict):
                persist_dir = simple_config.get(
                    "persist_directory",
                    self.config.vector_store.persist_directory + "/simple",
                )
            else:
                # ê¸°ë³¸ê°’ ì‚¬ìš©
                persist_dir = getattr(
                    self.config.vector_store,
                    "simple_directory",
                    self.config.vector_store.persist_directory + "/simple",
                )

            # ì¤‘ë³µ ê²½ë¡œ ë°©ì§€
            if persist_dir.endswith("/simple/simple"):
                persist_dir = persist_dir.replace("/simple/simple", "/simple")

            base_config.update(
                {
                    "persist_directory": persist_dir,
                }
            )

        return base_config

    def get_embeddings_config(self) -> Dict[str, Any]:
        """ì„ë² ë”© ì„¤ì • ë°˜í™˜ (ì €ì¥ ê²½ë¡œ í¬í•¨)"""
        return {
            "model_name": self.config.embedding.model_name,
            "device": self.config.embedding.device,
            "batch_size": self.config.embedding.batch_size,
            "max_length": self.config.embedding.max_length,
            "cache_dir": self.config.embedding.cache_dir,
            "save_directory": self.config.embedding.save_directory,
            "model_type": self.config.embedding.model_type,
        }

    def get_all_directories(self) -> List[str]:
        """ìƒì„±í•´ì•¼ í•  ëª¨ë“  ë””ë ‰í† ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return [
            self.config.paths.data_dir,
            self.config.paths.processed_dir,
            self.config.paths.vector_store_root,
            self.config.paths.vector_store_embeddings,
            self.config.paths.vector_store_faiss,
            self.config.paths.vector_store_chromadb,
            self.config.paths.vector_store_simple,
            self.config.paths.cache_dir,
            self.config.paths.embeddings_cache,
            self.config.paths.query_cache,
            self.config.paths.logs_dir,
        ]

    def create_directories(self) -> None:
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ ìƒì„±"""
        from pathlib import Path

        directories = self.get_all_directories()

        for directory in directories:
            if directory:  # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                Path(directory).mkdir(parents=True, exist_ok=True)
                logger.debug(f"ğŸ“ Created directory: {directory}")

        logger.info(f"âœ… Created {len(directories)} directories")

    def save_config(self, file_path: Optional[str] = None) -> None:
        """í˜„ì¬ ì„¤ì •ì„ íŒŒì¼ë¡œ ì €ì¥"""

        output_path = Path(file_path) if file_path else self.config_file
        if not output_path:
            output_path = Path("graphrag_config.yaml")

        config_dict = asdict(self.config)

        # ë¯¼ê°í•œ ì •ë³´ ì œê±° (ì €ì¥ì‹œ)
        if "llm" in config_dict and "api_key" in config_dict["llm"]:
            config_dict["llm"]["api_key"] = "***REDACTED***"

        logger.info(f"ğŸ’¾ Saving config to: {output_path}")

        try:
            with open(output_path, "w", encoding="utf-8") as f:
                if output_path.suffix.lower() in [".yml", ".yaml"]:
                    yaml.dump(config_dict, f, default_flow_style=False, indent=2)
                else:
                    json.dump(config_dict, f, indent=2)

            logger.info("âœ… Config saved successfully")

        except Exception as e:
            logger.error(f"âŒ Failed to save config: {e}")
            raise

    def get_config_summary(self) -> str:
        """ì„¤ì • ìš”ì•½ ì •ë³´ ë°˜í™˜"""

        summary = [
            f"GraphRAG Configuration Summary",
            f"================================",
            f"LLM: {self.config.llm.provider} / {self.config.llm.model_name}",
            f"API Key: {'âœ… Set' if self.config.llm.api_key else 'âŒ Missing'}",
            f"Embedding Model: {self.config.embedding.model_name}",
            f"Graph Path: {self.config.graph.unified_graph_path or 'Not set'}",
            f"Vector Store: {self.config.graph.vector_store_path or 'Not set'}",
            f"Log Level: {self.config.system.log_level}",
            f"Config Source: {self.config.config_source.name}",
        ]

        return "\n".join(summary)


# í¸ì˜ í•¨ìˆ˜ë“¤
def load_config(
    config_file: Optional[str] = None, env_file: Optional[str] = None
) -> GraphRAGConfigManager:
    """ì„¤ì • ë§¤ë‹ˆì € ìƒì„± ë° ë¡œë”©"""
    return GraphRAGConfigManager(config_file=config_file, env_file=env_file)


def create_sample_config(file_path: str = "graphrag_config.yaml") -> None:
    """ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„±"""

    sample_config = {
        "llm": {
            # ğŸ†• ë¡œì»¬ ëª¨ë¸ ì„¤ì • ì˜ˆì‹œ ì¶”ê°€
            "provider": "huggingface_local",  # ë˜ëŠ” "openai", "anthropic"
            "model_path": "/DATA/MODELS/models--meta-llama--Llama-3.1-8B-Instruct",
            "device_map": "auto",
            "torch_dtype": "bfloat16",
            "temperature": 0.1,
            "max_new_tokens": 2048,
            # API ê¸°ë°˜ ëŒ€ì•ˆ ì„¤ì •
            # "provider": "openai",
            # "model_name": "gpt-4",
            # "api_key": "${OPENAI_API_KEY}",
        },
        "embedding": {"model_name": "auto", "device": "auto", "batch_size": 32},
        "graph": {
            "unified_graph_path": "./graphs/unified/unified_knowledge_graph.json",
            "vector_store_path": "./graphs/embeddings",
            "cache_enabled": True,
        },
        "qa": {
            "chain_type": "retrieval_qa",
            "max_docs": 10,
            "min_relevance_score": 0.3,
            "enable_memory": False,
        },
        "system": {"log_level": "INFO", "verbose": False, "max_workers": 4},
    }

    with open(file_path, "w", encoding="utf-8") as f:
        yaml.dump(sample_config, f, default_flow_style=False, indent=2)

    print(f"âœ… Sample config created: {file_path}")


def main():
    """ConfigManager í…ŒìŠ¤íŠ¸"""

    print("ğŸ§ª Testing GraphRAGConfigManager...")

    try:
        # 1. ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”
        config_manager = GraphRAGConfigManager(auto_load=False)
        print("âœ… Basic initialization successful")

        # 2. ì„¤ì • ì—…ë°ì´íŠ¸
        config_manager.update_config(
            **{
                "llm.temperature": 0.2,
                "llm.model_name": "gpt-4",
                "qa.max_docs": 15,
                "system.verbose": True,
            }
        )
        print("âœ… Configuration update successful")

        # 3. LLM ì„¤ì • ì¡°íšŒ
        llm_config = config_manager.get_llm_config()
        print(f"ğŸ¤– LLM Config: {llm_config}")

        # 4. ì„¤ì • ìš”ì•½
        print(f"\nğŸ“‹ Configuration Summary:")
        print(config_manager.get_config_summary())

        # 5. ìƒ˜í”Œ ì„¤ì • íŒŒì¼ ìƒì„±
        create_sample_config("test_config.yaml")

        print(f"\nâœ… GraphRAGConfigManager test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
