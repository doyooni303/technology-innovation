"""
ë‹¤ì¤‘ ë…¸ë“œ ì„ë² ë”© ìƒì„±ê¸°
MultiNodeEmbedder for GraphRAG System

í†µí•© ì§€ì‹ ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œ íƒ€ì…ì— ëŒ€í•´ ìµœì í™”ëœ ì„ë² ë”© ìƒì„±
- ë…¸ë“œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìµœì í™”
- ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- ì§„í–‰ë¥  ì¶”ì  ë° ìºì‹± ì§€ì›
- ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ ì§€ì›
"""

import os
import json
import pickle
import hashlib
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import numpy as np
import pandas as pd
from tqdm import tqdm

# GraphRAG imports
from .embedding_models import BaseEmbeddingModel, create_embedding_model
from .node_text_processors import BaseNodeTextProcessor, create_text_processor

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class EmbeddingResult:
    """ì„ë² ë”© ê²°ê³¼ í´ë˜ìŠ¤"""

    node_id: str
    node_type: str
    text: str
    embedding: np.ndarray
    metadata: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ì„ë² ë”© ì œì™¸)"""
        return {
            "node_id": self.node_id,
            "node_type": self.node_type,
            "text": self.text,
            "embedding_shape": self.embedding.shape,
            "metadata": self.metadata,
        }


@dataclass
class EmbeddingStats:
    """ì„ë² ë”© í†µê³„ ì •ë³´"""

    total_nodes: int
    nodes_by_type: Dict[str, int]
    embedding_dimension: int
    total_size_mb: float
    processing_time_seconds: float
    model_info: Dict[str, Any]
    failed_nodes: List[str]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return asdict(self)


class MultiNodeEmbedder:
    """ë‹¤ì¤‘ ë…¸ë“œ ì„ë² ë”© ìƒì„±ê¸°"""

    def __init__(
        self,
        unified_graph_path: str,
        embedding_model: Union[str, BaseEmbeddingModel] = "auto",
        text_processors: Optional[Dict[str, BaseNodeTextProcessor]] = None,
        batch_size: int = 32,
        max_text_length: int = 512,
        language: str = "mixed",
        cache_dir: Optional[str] = None,
        device: str = "auto",
        **kwargs,
    ):
        """
        Args:
            unified_graph_path: í†µí•© ê·¸ë˜í”„ JSON íŒŒì¼ ê²½ë¡œ
            embedding_model: ì„ë² ë”© ëª¨ë¸ (ë¬¸ìì—´ ë˜ëŠ” ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤)
            text_processors: ë…¸ë“œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ë”•ì…”ë„ˆë¦¬
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_text_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
            language: ì£¼ìš” ì–¸ì–´ ("ko", "en", "mixed")
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            device: ë””ë°”ì´ìŠ¤ ì„¤ì •
            **kwargs: ì¶”ê°€ ì„¤ì •
        """
        self.unified_graph_path = Path(unified_graph_path)
        self.batch_size = batch_size
        self.max_text_length = max_text_length
        self.language = language
        self.cache_dir = Path(cache_dir) if cache_dir else None

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        if isinstance(embedding_model, str):
            self.embedding_model = create_embedding_model(
                model_name=embedding_model,
                device=device,
                batch_size=batch_size,
                **kwargs,
            )
        else:
            self.embedding_model = embedding_model

        # í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        if text_processors:
            self.text_processors = text_processors
        else:
            self.text_processors = self._create_default_processors()

        # ë°ì´í„° ì €ì¥
        self.graph_data = None
        self.embeddings_cache = {}
        self.node_index = {}  # node_id -> index ë§¤í•‘

        # ìºì‹œ ì„¤ì •
        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self._cache_key = self._generate_cache_key()

        logger.info("âœ… MultiNodeEmbedder initialized")
        logger.info(f"   ğŸ“ Graph file: {self.unified_graph_path}")
        logger.info(f"   ğŸ¤– Model: {self.embedding_model.config.model_name}")
        logger.info(f"   ğŸ“ Batch size: {batch_size}")
        logger.info(f"   ğŸŒ Language: {language}")

    def _create_default_processors(self) -> Dict[str, BaseNodeTextProcessor]:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ìƒì„±"""
        processors = {}

        supported_types = ["paper", "author", "keyword", "journal"]

        for node_type in supported_types:
            try:
                processors[node_type] = create_text_processor(
                    node_type=node_type,
                    max_length=self.max_text_length,
                    language=self.language,
                )
            except Exception as e:
                logger.warning(f"Failed to create processor for {node_type}: {e}")

        return processors

    def _generate_cache_key(self) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (ì„¤ì • ê¸°ë°˜)"""
        key_data = {
            "graph_file": str(self.unified_graph_path),
            "model_name": self.embedding_model.config.model_name,
            "model_type": self.embedding_model.config.model_type,
            "max_length": self.max_text_length,
            "language": self.language,
            "file_mtime": (
                self.unified_graph_path.stat().st_mtime
                if self.unified_graph_path.exists()
                else 0
            ),
        }

        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()

    def load_unified_graph(self) -> Dict[str, Any]:
        """í†µí•© ê·¸ë˜í”„ ë°ì´í„° ë¡œë“œ"""
        if self.graph_data is not None:
            return self.graph_data

        if not self.unified_graph_path.exists():
            raise FileNotFoundError(
                f"Unified graph file not found: {self.unified_graph_path}"
            )

        logger.info(f"ğŸ“‚ Loading unified graph from {self.unified_graph_path}")

        try:
            with open(self.unified_graph_path, "r", encoding="utf-8") as f:
                self.graph_data = json.load(f)

            # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
            if "nodes" not in self.graph_data or "edges" not in self.graph_data:
                raise ValueError("Invalid graph format: missing 'nodes' or 'edges'")

            # ë…¸ë“œ ì¸ë±ìŠ¤ ìƒì„±
            self.node_index = {
                node["id"]: idx for idx, node in enumerate(self.graph_data["nodes"])
            }

            logger.info(f"âœ… Graph loaded successfully")
            logger.info(f"   ğŸ“„ Nodes: {len(self.graph_data['nodes']):,}")
            logger.info(f"   ğŸ”— Edges: {len(self.graph_data['edges']):,}")

            return self.graph_data

        except Exception as e:
            logger.error(f"âŒ Failed to load graph: {e}")
            raise

    def analyze_graph_structure(self) -> Dict[str, Any]:
        """ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„"""
        if self.graph_data is None:
            self.load_unified_graph()

        logger.info("ğŸ“Š Analyzing graph structure...")

        # ë…¸ë“œ íƒ€ì…ë³„ í†µê³„
        node_types = Counter()
        node_type_samples = defaultdict(list)

        for node in self.graph_data["nodes"]:
            node_type = node.get("node_type", "unknown")
            node_types[node_type] += 1

            # ê° íƒ€ì…ë³„ ìƒ˜í”Œ ìˆ˜ì§‘ (ì²˜ìŒ 3ê°œ)
            if len(node_type_samples[node_type]) < 3:
                node_type_samples[node_type].append(
                    {"id": node["id"], "sample_keys": list(node.keys())}
                )

        # ì—£ì§€ íƒ€ì…ë³„ í†µê³„
        edge_types = Counter()
        for edge in self.graph_data["edges"]:
            edge_type = edge.get("edge_type", "unknown")
            edge_types[edge_type] += 1

        analysis = {
            "total_nodes": len(self.graph_data["nodes"]),
            "total_edges": len(self.graph_data["edges"]),
            "node_types": dict(node_types),
            "edge_types": dict(edge_types),
            "node_type_samples": dict(node_type_samples),
            "supported_processors": list(self.text_processors.keys()),
        }

        logger.info(f"ğŸ“‹ Graph Analysis:")
        logger.info(f"   ğŸ“„ Total nodes: {analysis['total_nodes']:,}")
        for ntype, count in node_types.most_common():
            supported = "âœ…" if ntype in self.text_processors else "âŒ"
            logger.info(f"   {supported} {ntype}: {count:,}")

        return analysis

    def process_nodes_to_text(
        self, node_types: Optional[List[str]] = None, show_progress: bool = True
    ) -> Dict[str, List[Tuple[str, str, Dict[str, Any]]]]:
        """ë…¸ë“œë“¤ì„ íƒ€ì…ë³„ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬

        Returns:
            {node_type: [(node_id, processed_text, metadata), ...]}
        """
        if self.graph_data is None:
            self.load_unified_graph()

        # ì²˜ë¦¬í•  ë…¸ë“œ íƒ€ì… ê²°ì •
        if node_types is None:
            node_types = list(self.text_processors.keys())

        # ë…¸ë“œ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        nodes_by_type = defaultdict(list)
        for node in self.graph_data["nodes"]:
            node_type = node.get("node_type", "unknown")
            if node_type in node_types and node_type in self.text_processors:
                nodes_by_type[node_type].append(node)

        logger.info(f"ğŸ”¤ Processing nodes to text...")
        for ntype, nodes in nodes_by_type.items():
            logger.info(f"   ğŸ“ {ntype}: {len(nodes):,} nodes")

        processed_data = {}
        failed_nodes = []

        # íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
        for node_type, nodes in nodes_by_type.items():
            logger.info(f"ğŸ“ Processing {node_type} nodes...")

            processor = self.text_processors[node_type]
            type_results = []

            for node in tqdm(
                nodes, desc=f"Processing {node_type}", disable=not show_progress
            ):
                try:
                    # í…ìŠ¤íŠ¸ ìƒì„±
                    processed_text = processor.process_node(node)

                    # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                    metadata = {
                        "node_type": node_type,
                        "original_keys": list(node.keys()),
                        "text_length": len(processed_text),
                        "word_count": len(processed_text.split()),
                    }

                    # íŠ¹ë³„ ë©”íƒ€ë°ì´í„° (ë…¸ë“œ íƒ€ì…ë³„)
                    if node_type == "paper":
                        metadata.update(
                            {
                                "has_abstract": bool(node.get("abstract", "")),
                                "keyword_count": len(node.get("keywords", [])),
                                "author_count": len(node.get("authors", [])),
                            }
                        )
                    elif node_type == "author":
                        metadata.update(
                            {
                                "paper_count": node.get("paper_count", 0),
                                "productivity_type": node.get("productivity_type", ""),
                            }
                        )
                    elif node_type == "keyword":
                        metadata.update({"frequency": node.get("frequency", 0)})
                    elif node_type == "journal":
                        metadata.update(
                            {
                                "paper_count": node.get("paper_count", 0),
                                "journal_type": node.get("journal_type", ""),
                            }
                        )

                    type_results.append((node["id"], processed_text, metadata))

                except Exception as e:
                    logger.warning(f"Failed to process node {node['id']}: {e}")
                    failed_nodes.append(node["id"])

            processed_data[node_type] = type_results
            logger.info(
                f"âœ… {node_type}: {len(type_results)} processed, {len(nodes) - len(type_results)} failed"
            )

        if failed_nodes:
            logger.warning(f"âš ï¸ {len(failed_nodes)} nodes failed processing")

        return processed_data

    def generate_embeddings(
        self,
        node_types: Optional[List[str]] = None,
        use_cache: bool = True,
        save_cache: bool = True,
        show_progress: bool = True,
    ) -> Dict[str, List[EmbeddingResult]]:
        """ì „ì²´ ì„ë² ë”© ìƒì„± íŒŒì´í”„ë¼ì¸"""

        # ìºì‹œ í™•ì¸
        if use_cache and self.cache_dir:
            cached_results = self._load_from_cache()
            if cached_results:
                logger.info("âœ… Loaded embeddings from cache")
                return cached_results

        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        processed_data = self.process_nodes_to_text(node_types, show_progress)

        # ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë”©)
        if not self.embedding_model.is_loaded():
            logger.info("ğŸ“¥ Loading embedding model...")
            self.embedding_model.load_model()

        embedding_results = {}
        total_nodes = sum(len(data) for data in processed_data.values())

        logger.info(f"ğŸš€ Generating embeddings for {total_nodes:,} nodes...")

        with tqdm(
            total=total_nodes, desc="Generating embeddings", disable=not show_progress
        ) as pbar:

            for node_type, type_data in processed_data.items():
                logger.info(f"ğŸ¤– Embedding {node_type} nodes ({len(type_data):,})...")

                type_results = []

                # ë°°ì¹˜ ì²˜ë¦¬
                for i in range(0, len(type_data), self.batch_size):
                    batch_data = type_data[i : i + self.batch_size]

                    # ë°°ì¹˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    batch_texts = [item[1] for item in batch_data]

                    try:
                        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
                        batch_embeddings = self.embedding_model.encode(
                            batch_texts, batch_size=self.batch_size, show_progress=False
                        )

                        # ê²°ê³¼ ê°ì²´ ìƒì„±
                        for j, (node_id, text, metadata) in enumerate(batch_data):
                            embedding_result = EmbeddingResult(
                                node_id=node_id,
                                node_type=node_type,
                                text=text,
                                embedding=batch_embeddings[j],
                                metadata=metadata,
                            )
                            type_results.append(embedding_result)

                        pbar.update(len(batch_data))

                    except Exception as e:
                        logger.error(f"âŒ Batch embedding failed: {e}")
                        # ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
                        for node_id, text, metadata in batch_data:
                            try:
                                embedding = self.embedding_model.encode([text])[0]
                                embedding_result = EmbeddingResult(
                                    node_id=node_id,
                                    node_type=node_type,
                                    text=text,
                                    embedding=embedding,
                                    metadata=metadata,
                                )
                                type_results.append(embedding_result)
                            except Exception as e2:
                                logger.warning(f"Failed to embed node {node_id}: {e2}")

                            pbar.update(1)

                embedding_results[node_type] = type_results
                logger.info(f"âœ… {node_type}: {len(type_results)} embeddings generated")

        # ìºì‹œ ì €ì¥
        if save_cache and self.cache_dir:
            self._save_to_cache(embedding_results)

        return embedding_results

    def _load_from_cache(self) -> Optional[Dict[str, List[EmbeddingResult]]]:
        """ìºì‹œì—ì„œ ì„ë² ë”© ë¡œë“œ"""
        if not self.cache_dir or not self._cache_key:
            return None

        cache_file = self.cache_dir / f"embeddings_{self._cache_key}.pkl"

        if not cache_file.exists():
            return None

        try:
            logger.info(f"ğŸ“‚ Loading from cache: {cache_file}")
            with open(cache_file, "rb") as f:
                cached_data = pickle.load(f)

            # ìºì‹œ ìœ íš¨ì„± ê²€ì¦
            if "embeddings" in cached_data and "metadata" in cached_data:
                cache_meta = cached_data["metadata"]

                # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
                current_mtime = self.unified_graph_path.stat().st_mtime
                if (
                    abs(cache_meta.get("file_mtime", 0) - current_mtime) < 1
                ):  # 1ì´ˆ ì˜¤ì°¨ í—ˆìš©
                    return cached_data["embeddings"]
                else:
                    logger.info("ğŸ”„ Cache outdated due to file modification")

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load cache: {e}")

        return None

    def _save_to_cache(
        self, embedding_results: Dict[str, List[EmbeddingResult]]
    ) -> None:
        """ìºì‹œì— ì„ë² ë”© ì €ì¥"""
        if not self.cache_dir or not self._cache_key:
            return

        cache_file = self.cache_dir / f"embeddings_{self._cache_key}.pkl"

        try:
            cache_data = {
                "embeddings": embedding_results,
                "metadata": {
                    "cache_key": self._cache_key,
                    "file_mtime": self.unified_graph_path.stat().st_mtime,
                    "model_name": self.embedding_model.config.model_name,
                    "embedding_dim": self.embedding_model.get_embedding_dimension(),
                    "total_nodes": sum(
                        len(results) for results in embedding_results.values()
                    ),
                    "created_at": pd.Timestamp.now().isoformat(),
                },
            }

            logger.info(f"ğŸ’¾ Saving to cache: {cache_file}")
            with open(cache_file, "wb") as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)

            # ìºì‹œ íŒŒì¼ í¬ê¸° í™•ì¸
            cache_size_mb = cache_file.stat().st_size / 1024 / 1024
            logger.info(f"âœ… Cache saved ({cache_size_mb:.1f} MB)")

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save cache: {e}")

    def save_embeddings(
        self,
        embedding_results: Dict[str, List[EmbeddingResult]],
        output_dir: str,
        formats: List[str] = ["numpy", "json"],
    ) -> Dict[str, Path]:
        """ì„ë² ë”© ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì €ì¥"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        saved_files = {}

        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_nodes = sum(len(results) for results in embedding_results.values())
        if total_nodes == 0:
            logger.warning("âš ï¸ No embeddings to save")
            return saved_files

        first_embedding = next(iter(next(iter(embedding_results.values()))))
        embedding_dim = len(first_embedding.embedding)

        # í†µê³„ ì •ë³´ ìƒì„±
        stats = EmbeddingStats(
            total_nodes=total_nodes,
            nodes_by_type={
                ntype: len(results) for ntype, results in embedding_results.items()
            },
            embedding_dimension=embedding_dim,
            total_size_mb=(total_nodes * embedding_dim * 4)
            / 1024
            / 1024,  # float32 ê¸°ì¤€
            processing_time_seconds=0,  # ì‹¤ì œë¡œëŠ” ì¸¡ì • í•„ìš”
            model_info={
                "model_name": self.embedding_model.config.model_name,
                "model_type": self.embedding_model.config.model_type,
                "dimension": embedding_dim,
            },
            failed_nodes=[],
        )

        logger.info(f"ğŸ’¾ Saving embeddings to {output_dir}")

        # 1. NumPy í˜•íƒœ ì €ì¥ (ë²¡í„° ê²€ìƒ‰ìš©)
        if "numpy" in formats:
            embeddings_array = []
            node_ids = []
            node_types = []

            for node_type, results in embedding_results.items():
                for result in results:
                    embeddings_array.append(result.embedding)
                    node_ids.append(result.node_id)
                    node_types.append(result.node_type)

            embeddings_array = np.array(embeddings_array)

            # ë°°ì—´ ì €ì¥
            np.save(output_dir / "embeddings.npy", embeddings_array)
            np.save(output_dir / "node_ids.npy", np.array(node_ids))
            np.save(output_dir / "node_types.npy", np.array(node_types))

            saved_files["numpy_embeddings"] = output_dir / "embeddings.npy"
            saved_files["numpy_node_ids"] = output_dir / "node_ids.npy"
            saved_files["numpy_node_types"] = output_dir / "node_types.npy"

        # 2. JSON ë©”íƒ€ë°ì´í„° ì €ì¥
        if "json" in formats:
            # ë©”íƒ€ë°ì´í„°ë§Œ (ì„ë² ë”© ì œì™¸)
            metadata_dict = {}
            for node_type, results in embedding_results.items():
                metadata_dict[node_type] = [result.to_dict() for result in results]

            metadata_file = output_dir / "embeddings_metadata.json"
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata_dict, f, ensure_ascii=False, indent=2)

            saved_files["metadata"] = metadata_file

        # 3. í†µê³„ ì •ë³´ ì €ì¥
        stats_file = output_dir / "embedding_statistics.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats.to_dict(), f, ensure_ascii=False, indent=2)

        saved_files["statistics"] = stats_file

        # 4. ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„± (ê²€ìƒ‰ìš©)
        index_data = {
            "node_id_to_index": {node_id: idx for idx, node_id in enumerate(node_ids)},
            "index_to_node_id": {idx: node_id for idx, node_id in enumerate(node_ids)},
            "node_type_mapping": {
                node_id: node_type for node_id, node_type in zip(node_ids, node_types)
            },
            "embedding_dimension": embedding_dim,
            "total_nodes": total_nodes,
        }

        index_file = output_dir / "node_index.json"
        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

        saved_files["index"] = index_file

        logger.info(f"âœ… Embeddings saved successfully:")
        for format_name, file_path in saved_files.items():
            file_size = (
                file_path.stat().st_size / 1024 / 1024 if file_path.exists() else 0
            )
            logger.info(f"   ğŸ“„ {format_name}: {file_path} ({file_size:.1f} MB)")

        return saved_files

    def run_full_pipeline(
        self,
        output_dir: str,
        node_types: Optional[List[str]] = None,
        use_cache: bool = True,
        save_formats: List[str] = ["numpy", "json"],
        show_progress: bool = True,
    ) -> Tuple[Dict[str, List[EmbeddingResult]], Dict[str, Path]]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        logger.info("ğŸš€ Starting MultiNodeEmbedder full pipeline...")

        # 1. ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„
        analysis = self.analyze_graph_structure()

        # 2. ì„ë² ë”© ìƒì„±
        embedding_results = self.generate_embeddings(
            node_types=node_types, use_cache=use_cache, show_progress=show_progress
        )

        # 3. ê²°ê³¼ ì €ì¥
        saved_files = self.save_embeddings(
            embedding_results=embedding_results,
            output_dir=output_dir,
            formats=save_formats,
        )

        # 4. ìš”ì•½ ì¶œë ¥
        total_nodes = sum(len(results) for results in embedding_results.values())
        embedding_dim = self.embedding_model.get_embedding_dimension()

        logger.info("ğŸ‰ Pipeline completed successfully!")
        logger.info(f"ğŸ“Š Results:")
        logger.info(f"   ğŸ“„ Total nodes embedded: {total_nodes:,}")
        logger.info(f"   ğŸ“ Embedding dimension: {embedding_dim}")
        logger.info(f"   ğŸ’¾ Output directory: {output_dir}")

        for node_type, results in embedding_results.items():
            logger.info(f"   ğŸ“ {node_type}: {len(results):,} embeddings")

        return embedding_results, saved_files


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # ê¸°ë³¸ import ê²½ë¡œ ì„¤ì •
    import sys
    from pathlib import Path

    # src ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
    src_dir = Path(__file__).parent.parent.parent
    if str(src_dir) not in sys.path:
        sys.path.insert(0, str(src_dir))

    from src import GRAPHS_DIR, RAW_EXTRACTIONS_DIR

    print("ğŸ§ª Testing MultiNodeEmbedder...")

    # í†µí•© ê·¸ë˜í”„ íŒŒì¼ í™•ì¸
    unified_graph_file = GRAPHS_DIR / "unified" / "unified_knowledge_graph.json"

    if not unified_graph_file.exists():
        print(f"âŒ Unified graph not found: {unified_graph_file}")
        print("Please run unified_graph_builder.py first")
        return

    try:
        # MultiNodeEmbedder ì´ˆê¸°í™”
        embedder = MultiNodeEmbedder(
            unified_graph_path=str(unified_graph_file),
            embedding_model="auto",  # ìë™ ëª¨ë¸ ì„ íƒ
            batch_size=16,
            max_text_length=256,
            language="mixed",
            cache_dir=str(GRAPHS_DIR / "embeddings_cache"),
        )

        # ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„
        analysis = embedder.analyze_graph_structure()

        # ìƒ˜í”Œ ì²˜ë¦¬ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        print(f"\nğŸ“ Processing sample nodes...")

        # ì²˜ìŒ 100ê°œ ë…¸ë“œë§Œìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        embedder.graph_data["nodes"] = embedder.graph_data["nodes"][:100]

        # ì„ë² ë”© ìƒì„± ë° ì €ì¥
        embedding_results, saved_files = embedder.run_full_pipeline(
            output_dir=str(GRAPHS_DIR / "embeddings"),
            node_types=["paper", "author", "keyword"],  # ì¼ë¶€ë§Œ í…ŒìŠ¤íŠ¸
            use_cache=True,
            show_progress=True,
        )

        print(f"\nâœ… MultiNodeEmbedder test completed!")
        print(f"ğŸ“ Check output: {GRAPHS_DIR / 'embeddings'}")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
