"""
ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ ëª¨ë“ˆ
Subgraph Extractor for GraphRAG System

ë²¡í„° ê²€ìƒ‰ê³¼ ê·¸ë˜í”„ íƒìƒ‰ì„ ê²°í•©í•˜ì—¬ ì¿¼ë¦¬ ê´€ë ¨ ì„œë¸Œê·¸ë˜í”„ë¥¼ ì¶”ì¶œ
- ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ ì§€ì› (Local, Global, Hybrid)
- ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ íƒìƒ‰
- íš¨ìœ¨ì ì¸ ê·¸ë˜í”„ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜
- LLM í† í° ì œí•œ ê³ ë ¤í•œ í¬ê¸° ì¡°ì ˆ
"""

import json
import logging
import warnings
from enum import Enum
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set, Union
from dataclasses import dataclass
from collections import defaultdict, deque
import numpy as np
import networkx as nx
from tqdm import tqdm

# GraphRAG imports
try:
    from ..embeddings.vector_store_manager import VectorStoreManager, SearchResult
    from ..embeddings.embedding_models import create_embedding_model
    from ..query_analyzer import (
        QueryAnalysisResult,
        QueryComplexity,
        QueryType,
        SearchMode,
    )
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class SearchStrategy(Enum):
    """ê²€ìƒ‰ ì „ëµ"""

    LOCAL = "local"  # íŠ¹ì • ë…¸ë“œ ì¤‘ì‹¬ í™•ì¥
    GLOBAL = "global"  # ì „ì—­ êµ¬ì¡° ê¸°ë°˜
    HYBRID = "hybrid"  # í˜¼í•© ì „ëµ
    COMMUNITY = "community"  # ì»¤ë®¤ë‹ˆí‹° ê¸°ë°˜
    TEMPORAL = "temporal"  # ì‹œê°„ì  ì—°ê´€ì„±
    SEMANTIC = "semantic"  # ì˜ë¯¸ì  ìœ ì‚¬ë„ ìš°ì„ 


@dataclass
class ExtractionConfig:
    """ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ ì„¤ì •"""

    # í¬ê¸° ì œí•œ
    max_nodes: int = 200
    max_edges: int = 500
    max_hops: int = 3

    # ê²€ìƒ‰ ì„¤ì •
    initial_top_k: int = 20  # ì´ˆê¸° ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
    expansion_factor: float = 2.0  # í™•ì¥ ë¹„ìœ¨
    similarity_threshold: float = 0.5

    # ì „ëµë³„ ê°€ì¤‘ì¹˜
    vector_weight: float = 0.4
    graph_weight: float = 0.3
    metadata_weight: float = 0.3

    # ë…¸ë“œ íƒ€ì…ë³„ ì¤‘ìš”ë„
    node_type_weights: Dict[str, float] = None

    # ì„±ëŠ¥ ì„¤ì •
    enable_caching: bool = True
    parallel_processing: bool = False

    def __post_init__(self):
        if self.node_type_weights is None:
            self.node_type_weights = {
                "paper": 1.0,
                "author": 0.8,
                "keyword": 0.6,
                "journal": 0.5,
            }


@dataclass
class SubgraphResult:
    """ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ ê²°ê³¼"""

    # ê·¸ë˜í”„ ë°ì´í„°
    nodes: Dict[str, Dict[str, Any]]  # node_id -> node_data
    edges: List[Dict[str, Any]]  # edge ë¦¬ìŠ¤íŠ¸

    # ë©”íƒ€ë°ì´í„°
    query: str
    query_analysis: Optional[QueryAnalysisResult]
    extraction_strategy: SearchStrategy

    # í†µê³„
    total_nodes: int
    total_edges: int
    nodes_by_type: Dict[str, int]
    extraction_time: float

    # ê²€ìƒ‰ ê²°ê³¼
    initial_matches: List[SearchResult]
    expansion_path: List[Dict[str, Any]]

    # ìŠ¤ì½”ì–´ë§
    relevance_scores: Dict[str, float]  # node_id -> relevance_score
    confidence_score: float

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "nodes": self.nodes,
            "edges": self.edges,
            "query": self.query,
            "extraction_strategy": self.extraction_strategy.value,
            "total_nodes": self.total_nodes,
            "total_edges": self.total_edges,
            "nodes_by_type": self.nodes_by_type,
            "extraction_time": self.extraction_time,
            "confidence_score": self.confidence_score,
            "relevance_scores": self.relevance_scores,
        }

    def get_networkx_graph(self) -> nx.Graph:
        """NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜"""
        G = nx.Graph()

        # ë…¸ë“œ ì¶”ê°€
        for node_id, node_data in self.nodes.items():
            G.add_node(node_id, **node_data)

        # ì—£ì§€ ì¶”ê°€
        for edge in self.edges:
            source = edge.get("source")
            target = edge.get("target")
            if source and target and G.has_node(source) and G.has_node(target):
                edge_attrs = {
                    k: v for k, v in edge.items() if k not in ["source", "target"]
                }
                G.add_edge(source, target, **edge_attrs)

        return G


class SubgraphExtractor:
    """ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œê¸°"""

    def __init__(
        self,
        unified_graph_path: str,
        vector_store_path: str,
        embedding_model: str = "auto",
        config: Optional[ExtractionConfig] = None,
        device: str = "auto",
    ):
        """
        Args:
            unified_graph_path: í†µí•© ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
            vector_store_path: ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ
            embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
            config: ì¶”ì¶œ ì„¤ì •
            device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        """
        self.unified_graph_path = Path(unified_graph_path)
        self.vector_store_path = Path(vector_store_path)
        self.config = config or ExtractionConfig()

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = create_embedding_model(
            model_name=embedding_model, device=device
        )

        # ë°ì´í„° ì €ì¥
        self.graph_data = None
        self.networkx_graph = None
        self.vector_store = None

        # ìºì‹œ
        self.query_cache = {} if self.config.enable_caching else None
        self.node_neighbors_cache = {}

        logger.info("âœ… SubgraphExtractor initialized")
        logger.info(f"   ğŸ“ Graph: {self.unified_graph_path}")
        logger.info(f"   ğŸ—„ï¸ Vector Store: {self.vector_store_path}")

    def load_unified_graph(self) -> Dict[str, Any]:
        """í†µí•© ê·¸ë˜í”„ ë¡œë“œ"""
        if self.graph_data is not None:
            return self.graph_data

        if not self.unified_graph_path.exists():
            raise FileNotFoundError(f"Graph file not found: {self.unified_graph_path}")

        logger.info(f"ğŸ“‚ Loading unified graph...")

        with open(self.unified_graph_path, "r", encoding="utf-8") as f:
            self.graph_data = json.load(f)

        # NetworkX ê·¸ë˜í”„ ìƒì„±
        self._create_networkx_graph()

        logger.info(
            f"âœ… Graph loaded: {len(self.graph_data['nodes'])} nodes, {len(self.graph_data['edges'])} edges"
        )
        return self.graph_data

    def _create_networkx_graph(self) -> None:
        """NetworkX ê·¸ë˜í”„ ìƒì„± (íƒìƒ‰ ìµœì í™”ìš©)"""
        logger.info("ğŸ”§ Creating NetworkX graph for efficient traversal...")

        # ë°©í–¥ ê·¸ë˜í”„ë¡œ ìƒì„± (ì—£ì§€ íƒ€ì… ê³ ë ¤)
        self.networkx_graph = nx.MultiDiGraph()

        # ë…¸ë“œ ì¶”ê°€
        for node in self.graph_data["nodes"]:
            node_id = node["id"]
            node_attrs = {k: v for k, v in node.items() if k != "id"}
            self.networkx_graph.add_node(node_id, **node_attrs)

        # ì—£ì§€ ì¶”ê°€
        for edge in self.graph_data["edges"]:
            source = edge["source"]
            target = edge["target"]
            edge_attrs = {
                k: v for k, v in edge.items() if k not in ["source", "target"]
            }

            # í‚¤ ìƒì„± (multiple edges ì§€ì›)
            edge_type = edge_attrs.get("edge_type", "default")
            self.networkx_graph.add_edge(source, target, key=edge_type, **edge_attrs)

        logger.info(f"âœ… NetworkX graph created")

    def load_vector_store(self) -> VectorStoreManager:
        """ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ - ê°œì„ ëœ ë²„ì „"""
        if self.vector_store is not None:
            return self.vector_store

        if not self.vector_store_path.exists():
            raise FileNotFoundError(f"Vector store not found: {self.vector_store_path}")

        logger.info(f"ğŸ“‚ Loading vector store from: {self.vector_store_path}")

        try:
            # ë²¡í„° ì €ì¥ì†Œ íƒ€ì… ìë™ ê°ì§€
            store_type = "auto"

            # FAISS íŒŒì¼ í™•ì¸
            if (self.vector_store_path / "faiss_index.bin").exists():
                store_type = "faiss"
                actual_path = self.vector_store_path
            # ChromaDB íŒŒì¼ í™•ì¸
            elif (self.vector_store_path / "chroma.sqlite3").exists():
                store_type = "chroma"
                actual_path = self.vector_store_path
            # ì„œë¸Œí´ë”ì—ì„œ ì°¾ê¸°
            elif (self.vector_store_path / "faiss" / "faiss_index.bin").exists():
                store_type = "faiss"
                actual_path = self.vector_store_path / "faiss"
            elif (self.vector_store_path / "chromadb" / "chroma.sqlite3").exists():
                store_type = "chroma"
                actual_path = self.vector_store_path / "chromadb"
            else:
                # ì„ë² ë”© íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
                embeddings_dir = self.vector_store_path / "embeddings"
                if (
                    embeddings_dir.exists()
                    and (embeddings_dir / "embeddings.npy").exists()
                ):
                    logger.info("ğŸ“¥ Building vector store from embeddings...")

                    # ê¸°ë³¸ FAISSë¡œ êµ¬ì¶•
                    store_type = "faiss"
                    actual_path = self.vector_store_path / "faiss"
                    actual_path.mkdir(exist_ok=True)

                    from .vector_store_manager import VectorStoreManager

                    self.vector_store = VectorStoreManager(
                        store_type="faiss", persist_directory=str(actual_path)
                    )

                    self.vector_store.load_from_saved_embeddings(
                        str(self.vector_store_path), embeddings_subdir="embeddings"
                    )

                    logger.info("âœ… Vector store built from embeddings")
                    return self.vector_store
                else:
                    raise FileNotFoundError(
                        f"No vector store or embeddings found in: {self.vector_store_path}"
                    )

            # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
            from .vector_store_manager import VectorStoreManager

            self.vector_store = VectorStoreManager(
                store_type=store_type, persist_directory=str(actual_path)
            )

            logger.info(f"âœ… Vector store loaded: {store_type} from {actual_path}")
            return self.vector_store

        except Exception as e:
            logger.error(f"âŒ Vector store loading failed: {e}")
            raise

    def extract_subgraph(
        self,
        query: str,
        query_analysis: Optional[QueryAnalysisResult] = None,
        strategy: Optional[SearchStrategy] = None,
        custom_config: Optional[ExtractionConfig] = None,
    ) -> SubgraphResult:
        """ë©”ì¸ ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ í•¨ìˆ˜"""

        # ì„¤ì • ê²°ì •
        config = custom_config or self.config

        # ìºì‹œ í™•ì¸
        cache_key = (
            f"{query}_{strategy}_{hash(str(config.__dict__))}"
            if self.query_cache
            else None
        )
        if cache_key and cache_key in self.query_cache:
            logger.info("âœ… Using cached result")
            return self.query_cache[cache_key]

        logger.info(f"ğŸ” Extracting subgraph for query: '{query[:50]}...'")

        import time

        start_time = time.time()

        # ë°ì´í„° ë¡œë“œ
        self.load_unified_graph()
        self.load_vector_store()

        # ê²€ìƒ‰ ì „ëµ ê²°ì •
        if strategy is None:
            strategy = self._determine_strategy(query_analysis)

        logger.info(f"ğŸ¯ Using strategy: {strategy.value}")

        try:
            # 1. ì´ˆê¸° ë²¡í„° ê²€ìƒ‰
            initial_matches = self._initial_vector_search(query, query_analysis, config)

            # 2. ê·¸ë˜í”„ í™•ì¥
            expanded_nodes, expansion_path = self._expand_from_initial_matches(
                initial_matches, strategy, query_analysis, config
            )

            # 3. ì„œë¸Œê·¸ë˜í”„ êµ¬ì„±
            subgraph_nodes, subgraph_edges = self._build_subgraph(
                expanded_nodes, config
            )

            # 4. ê´€ë ¨ì„± ìŠ¤ì½”ì–´ë§
            relevance_scores = self._calculate_relevance_scores(
                subgraph_nodes, query, initial_matches, config
            )

            # 5. ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_confidence_score(
                initial_matches, len(subgraph_nodes), relevance_scores
            )

            # 6. í†µê³„ ìƒì„±
            nodes_by_type = defaultdict(int)
            for node_data in subgraph_nodes.values():
                node_type = node_data.get("node_type", "unknown")
                nodes_by_type[node_type] += 1

            extraction_time = time.time() - start_time

            # ê²°ê³¼ ìƒì„±
            result = SubgraphResult(
                nodes=subgraph_nodes,
                edges=subgraph_edges,
                query=query,
                query_analysis=query_analysis,
                extraction_strategy=strategy,
                total_nodes=len(subgraph_nodes),
                total_edges=len(subgraph_edges),
                nodes_by_type=dict(nodes_by_type),
                extraction_time=extraction_time,
                initial_matches=initial_matches,
                expansion_path=expansion_path,
                relevance_scores=relevance_scores,
                confidence_score=confidence_score,
            )

            # ìºì‹œ ì €ì¥
            if cache_key:
                self.query_cache[cache_key] = result

            logger.info(
                f"âœ… Subgraph extracted: {result.total_nodes} nodes, {result.total_edges} edges"
            )
            logger.info(f"   â±ï¸ Time: {extraction_time:.2f}s")
            logger.info(f"   ğŸ¯ Confidence: {confidence_score:.3f}")

            return result

        except Exception as e:
            logger.error(f"âŒ Subgraph extraction failed: {e}")
            raise

    def _determine_strategy(
        self, query_analysis: Optional[QueryAnalysisResult]
    ) -> SearchStrategy:
        """ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ë¡œë¶€í„° ìµœì  ì „ëµ ê²°ì •"""

        if not query_analysis:
            return SearchStrategy.HYBRID

        # ë³µì¡ë„ ê¸°ë°˜ ì „ëµ
        if query_analysis.complexity in [QueryComplexity.SIMPLE]:
            return SearchStrategy.LOCAL
        elif query_analysis.complexity == QueryComplexity.EXPLORATORY:
            return SearchStrategy.GLOBAL

        # ì¿¼ë¦¬ íƒ€ì… ê¸°ë°˜ ì „ëµ
        if query_analysis.query_type == QueryType.CITATION_ANALYSIS:
            return SearchStrategy.LOCAL
        elif query_analysis.query_type == QueryType.COLLABORATION_ANALYSIS:
            return SearchStrategy.COMMUNITY
        elif query_analysis.query_type == QueryType.TREND_ANALYSIS:
            return SearchStrategy.TEMPORAL
        elif query_analysis.query_type == QueryType.SIMILARITY_ANALYSIS:
            return SearchStrategy.SEMANTIC
        elif query_analysis.query_type == QueryType.COMPREHENSIVE_ANALYSIS:
            return SearchStrategy.GLOBAL

        # ê²€ìƒ‰ ëª¨ë“œ ê¸°ë°˜ ì „ëµ
        if query_analysis.search_mode == SearchMode.LOCAL:
            return SearchStrategy.LOCAL
        elif query_analysis.search_mode == SearchMode.GLOBAL:
            return SearchStrategy.GLOBAL
        else:
            return SearchStrategy.HYBRID

    def _initial_vector_search(
        self,
        query: str,
        query_analysis: Optional[QueryAnalysisResult],
        config: ExtractionConfig,
    ) -> List[SearchResult]:
        """ì´ˆê¸° ë²¡í„° ê²€ìƒ‰"""

        logger.info(f"ğŸ” Initial vector search (top_k={config.initial_top_k})...")

        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        if not self.embedding_model.is_loaded():
            self.embedding_model.load_model()

        query_embedding = self.embedding_model.encode([query])[0]

        # ë…¸ë“œ íƒ€ì… í•„í„°ë§ (í•„ìš”ì‹œ)
        node_types = None
        if query_analysis and query_analysis.required_node_types:
            node_types = [nt.value for nt in query_analysis.required_node_types]

        # ë²¡í„° ê²€ìƒ‰
        search_results = self.vector_store.search_similar_nodes(
            query_embedding=query_embedding,
            top_k=config.initial_top_k,
            node_types=node_types,
        )

        # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
        filtered_results = [
            result
            for result in search_results
            if result.similarity_score >= config.similarity_threshold
        ]

        logger.info(
            f"âœ… Found {len(filtered_results)} initial matches (threshold: {config.similarity_threshold})"
        )

        return filtered_results

    def _expand_from_initial_matches(
        self,
        initial_matches: List[SearchResult],
        strategy: SearchStrategy,
        query_analysis: Optional[QueryAnalysisResult],
        config: ExtractionConfig,
    ) -> Tuple[Set[str], List[Dict[str, Any]]]:
        """ì´ˆê¸° ë§¤ì¹˜ë¡œë¶€í„° ê·¸ë˜í”„ í™•ì¥"""

        logger.info(f"ğŸš€ Expanding graph using {strategy.value} strategy...")

        # ì´ˆê¸° ë…¸ë“œë“¤
        seed_nodes = {result.node_id for result in initial_matches}
        expansion_path = []

        # ì „ëµë³„ í™•ì¥
        if strategy == SearchStrategy.LOCAL:
            expanded_nodes = self._local_expansion(seed_nodes, config, expansion_path)
        elif strategy == SearchStrategy.GLOBAL:
            expanded_nodes = self._global_expansion(seed_nodes, config, expansion_path)
        elif strategy == SearchStrategy.COMMUNITY:
            expanded_nodes = self._community_expansion(
                seed_nodes, config, expansion_path
            )
        elif strategy == SearchStrategy.TEMPORAL:
            expanded_nodes = self._temporal_expansion(
                seed_nodes, query_analysis, config, expansion_path
            )
        elif strategy == SearchStrategy.SEMANTIC:
            expanded_nodes = self._semantic_expansion(
                seed_nodes, config, expansion_path
            )
        else:  # HYBRID
            expanded_nodes = self._hybrid_expansion(
                seed_nodes, query_analysis, config, expansion_path
            )

        logger.info(f"âœ… Expanded to {len(expanded_nodes)} nodes")

        return expanded_nodes, expansion_path

    def _local_expansion(
        self,
        seed_nodes: Set[str],
        config: ExtractionConfig,
        expansion_path: List[Dict[str, Any]],
    ) -> Set[str]:
        """ì§€ì—­ì  í™•ì¥ (BFS)"""

        expanded = set(seed_nodes)
        current_level = seed_nodes

        for hop in range(config.max_hops):
            if len(expanded) >= config.max_nodes:
                break

            next_level = set()

            for node_id in current_level:
                if len(expanded) >= config.max_nodes:
                    break

                # ì´ì›ƒ ë…¸ë“œë“¤ ì°¾ê¸°
                neighbors = self._get_node_neighbors(node_id)

                for neighbor_id, edge_data in neighbors:
                    if neighbor_id not in expanded:
                        next_level.add(neighbor_id)

                        if len(expanded) + len(next_level) >= config.max_nodes:
                            break

            # ë‹¤ìŒ ë ˆë²¨ ì¶”ê°€
            expanded.update(next_level)
            current_level = next_level

            expansion_path.append(
                {
                    "hop": hop + 1,
                    "added_nodes": len(next_level),
                    "total_nodes": len(expanded),
                    "strategy": "local_bfs",
                }
            )

            if not next_level:  # ë” ì´ìƒ í™•ì¥í•  ë…¸ë“œê°€ ì—†ìŒ
                break

        return expanded

    def _global_expansion(
        self,
        seed_nodes: Set[str],
        config: ExtractionConfig,
        expansion_path: List[Dict[str, Any]],
    ) -> Set[str]:
        """ì „ì—­ì  í™•ì¥ (ì¤‘ì‹¬ì„± ê¸°ë°˜)"""

        # ì¤‘ì‹¬ì„± ê³„ì‚° (ë¯¸ë¦¬ ê³„ì‚°ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´)
        if not hasattr(self, "_centrality_scores"):
            logger.info("ğŸ“Š Computing centrality scores...")

            # íš¨ìœ¨ì„ ìœ„í•´ Degree Centrality ì‚¬ìš©
            self._centrality_scores = nx.degree_centrality(
                self.networkx_graph.to_undirected()
            )

        # ë†’ì€ ì¤‘ì‹¬ì„±ì„ ê°€ì§„ ë…¸ë“œë“¤ ìš°ì„  ì„ íƒ
        all_nodes = list(self.networkx_graph.nodes())

        # ì¤‘ì‹¬ì„± ì ìˆ˜ë¡œ ì •ë ¬
        sorted_nodes = sorted(
            all_nodes, key=lambda x: self._centrality_scores.get(x, 0), reverse=True
        )

        # ì‹œë“œ ë…¸ë“œë“¤ê³¼ ì—°ê²°ì„±ì´ ìˆëŠ” ê³ ì¤‘ì‹¬ì„± ë…¸ë“œë“¤ ì„ íƒ
        expanded = set(seed_nodes)

        for node_id in sorted_nodes:
            if len(expanded) >= config.max_nodes:
                break

            if node_id in expanded:
                continue

            # ê¸°ì¡´ ë…¸ë“œë“¤ê³¼ì˜ ì—°ê²°ì„± í™•ì¸
            has_connection = False
            for existing_node in expanded:
                if self.networkx_graph.has_edge(
                    node_id, existing_node
                ) or self.networkx_graph.has_edge(existing_node, node_id):
                    has_connection = True
                    break

            # ì§ì ‘ ì—°ê²°ì´ ì—†ì–´ë„ ì¤‘ì‹¬ì„±ì´ ë†’ìœ¼ë©´ í¬í•¨ (Global ì „ëµ)
            centrality = self._centrality_scores.get(node_id, 0)
            if has_connection or centrality > 0.1:  # ì„ê³„ê°’
                expanded.add(node_id)

        expansion_path.append(
            {
                "strategy": "global_centrality",
                "total_nodes": len(expanded),
                "centrality_threshold": 0.1,
            }
        )

        return expanded

    def _community_expansion(
        self,
        seed_nodes: Set[str],
        config: ExtractionConfig,
        expansion_path: List[Dict[str, Any]],
    ) -> Set[str]:
        """ì»¤ë®¤ë‹ˆí‹° ê¸°ë°˜ í™•ì¥"""

        # ì»¤ë®¤ë‹ˆí‹° ê°ì§€ (ë¯¸ë¦¬ ê³„ì‚°ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´)
        if not hasattr(self, "_communities"):
            logger.info("ğŸ˜ï¸ Detecting communities...")

            # Louvain ì»¤ë®¤ë‹ˆí‹° ê°ì§€
            undirected_graph = self.networkx_graph.to_undirected()
            try:
                import community as community_louvain

                self._communities = community_louvain.best_partition(undirected_graph)
            except ImportError:
                # ëŒ€ì•ˆ: ê°„ë‹¨í•œ ì—°ê²° ì„±ë¶„ ì‚¬ìš©
                components = list(nx.connected_components(undirected_graph))
                self._communities = {}
                for i, component in enumerate(components):
                    for node in component:
                        self._communities[node] = i

        # ì‹œë“œ ë…¸ë“œë“¤ì´ ì†í•œ ì»¤ë®¤ë‹ˆí‹°ë“¤ ì°¾ê¸°
        seed_communities = set()
        for node_id in seed_nodes:
            if node_id in self._communities:
                seed_communities.add(self._communities[node_id])

        # í•´ë‹¹ ì»¤ë®¤ë‹ˆí‹°ì˜ ëª¨ë“  ë…¸ë“œë“¤ ì¶”ê°€
        expanded = set(seed_nodes)

        for node_id, community_id in self._communities.items():
            if len(expanded) >= config.max_nodes:
                break

            if community_id in seed_communities:
                expanded.add(node_id)

        expansion_path.append(
            {
                "strategy": "community_based",
                "seed_communities": list(seed_communities),
                "total_nodes": len(expanded),
            }
        )

        return expanded

    def _temporal_expansion(
        self,
        seed_nodes: Set[str],
        query_analysis: Optional[QueryAnalysisResult],
        config: ExtractionConfig,
        expansion_path: List[Dict[str, Any]],
    ) -> Set[str]:
        """ì‹œê°„ì  í™•ì¥ (ë…„ë„ ê¸°ë°˜)"""

        # ì‹œë“œ ë…¸ë“œë“¤ì˜ ë…„ë„ ë¶„í¬ íŒŒì•…
        seed_years = []
        for node_id in seed_nodes:
            node_data = self.networkx_graph.nodes.get(node_id, {})
            year = node_data.get("year", "")
            if year and str(year).isdigit():
                seed_years.append(int(year))

        if not seed_years:
            # ë…„ë„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì§€ì—­ í™•ì¥ìœ¼ë¡œ ëŒ€ì²´
            return self._local_expansion(seed_nodes, config, expansion_path)

        # ë…„ë„ ë²”ìœ„ ê²°ì •
        min_year, max_year = min(seed_years), max(seed_years)
        year_window = max(3, max_year - min_year + 2)  # ìµœì†Œ 3ë…„ ìœˆë„ìš°

        # ì‹œê°„ì  ê·¼ì ‘ì„± ê¸°ë°˜ ë…¸ë“œ ì„ íƒ
        expanded = set(seed_nodes)

        for node_id in self.networkx_graph.nodes():
            if len(expanded) >= config.max_nodes:
                break

            if node_id in expanded:
                continue

            node_data = self.networkx_graph.nodes[node_id]
            year = node_data.get("year", "")

            if year and str(year).isdigit():
                node_year = int(year)

                # ì‹œê°„ ìœˆë„ìš° ë‚´ì˜ ë…¸ë“œë“¤ í¬í•¨
                if min_year - year_window <= node_year <= max_year + year_window:
                    expanded.add(node_id)

        expansion_path.append(
            {
                "strategy": "temporal",
                "year_range": [min_year, max_year],
                "year_window": year_window,
                "total_nodes": len(expanded),
            }
        )

        return expanded

    def _semantic_expansion(
        self,
        seed_nodes: Set[str],
        config: ExtractionConfig,
        expansion_path: List[Dict[str, Any]],
    ) -> Set[str]:
        """ì˜ë¯¸ì  í™•ì¥ (ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜)"""

        # ì‹œë“œ ë…¸ë“œë“¤ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
        seed_embeddings = []
        for node_id in seed_nodes:
            embedding = self.vector_store.get_node_embedding(node_id)
            if embedding is not None:
                seed_embeddings.append(embedding)

        if not seed_embeddings:
            # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ ì§€ì—­ í™•ì¥ìœ¼ë¡œ ëŒ€ì²´
            return self._local_expansion(seed_nodes, config, expansion_path)

        # í‰ê·  ì„ë² ë”©
        avg_embedding = np.mean(seed_embeddings, axis=0)

        # ìœ ì‚¬í•œ ë…¸ë“œë“¤ ê²€ìƒ‰
        similar_results = self.vector_store.search_similar_nodes(
            query_embedding=avg_embedding, top_k=config.max_nodes, node_types=None
        )

        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
        expanded = set(seed_nodes)
        for result in similar_results:
            if len(expanded) >= config.max_nodes:
                break

            if (
                result.similarity_score >= config.similarity_threshold
                and result.node_id not in expanded
            ):
                expanded.add(result.node_id)

        expansion_path.append(
            {
                "strategy": "semantic",
                "similarity_threshold": config.similarity_threshold,
                "total_nodes": len(expanded),
            }
        )

        return expanded

    def _hybrid_expansion(
        self,
        seed_nodes: Set[str],
        query_analysis: Optional[QueryAnalysisResult],
        config: ExtractionConfig,
        expansion_path: List[Dict[str, Any]],
    ) -> Set[str]:
        """í•˜ì´ë¸Œë¦¬ë“œ í™•ì¥ (ì—¬ëŸ¬ ì „ëµ ì¡°í•©)"""

        # ê° ì „ëµìœ¼ë¡œ ì¼ë¶€ì”© í™•ì¥
        total_budget = config.max_nodes
        current_expanded = set(seed_nodes)

        # ì „ëµë³„ ì˜ˆì‚° í• ë‹¹
        strategies = [
            (SearchStrategy.LOCAL, 0.4),
            (SearchStrategy.SEMANTIC, 0.3),
            (SearchStrategy.GLOBAL, 0.3),
        ]

        for strategy, weight in strategies:
            if len(current_expanded) >= total_budget:
                break

            # í•´ë‹¹ ì „ëµì˜ ì˜ˆì‚°
            strategy_budget = int((total_budget - len(seed_nodes)) * weight)
            if strategy_budget < 1:
                continue

            # ì„ì‹œ ì„¤ì •ìœ¼ë¡œ í•´ë‹¹ ì „ëµ ì‹¤í–‰
            temp_config = ExtractionConfig()
            temp_config.max_nodes = len(current_expanded) + strategy_budget
            temp_config.max_hops = config.max_hops
            temp_config.similarity_threshold = config.similarity_threshold

            temp_expansion_path = []

            if strategy == SearchStrategy.LOCAL:
                strategy_result = self._local_expansion(
                    current_expanded, temp_config, temp_expansion_path
                )
            elif strategy == SearchStrategy.SEMANTIC:
                strategy_result = self._semantic_expansion(
                    current_expanded, temp_config, temp_expansion_path
                )
            elif strategy == SearchStrategy.GLOBAL:
                strategy_result = self._global_expansion(
                    current_expanded, temp_config, temp_expansion_path
                )
            else:
                strategy_result = current_expanded

            # ìƒˆë¡œ ì¶”ê°€ëœ ë…¸ë“œë“¤ë§Œ ì„ íƒ
            new_nodes = strategy_result - current_expanded

            # ì˜ˆì‚° ë‚´ì—ì„œë§Œ ì¶”ê°€
            if len(new_nodes) > strategy_budget:
                new_nodes = set(list(new_nodes)[:strategy_budget])

            current_expanded.update(new_nodes)

            expansion_path.append(
                {
                    "strategy": f"hybrid_{strategy.value}",
                    "budget": strategy_budget,
                    "added_nodes": len(new_nodes),
                    "total_nodes": len(current_expanded),
                }
            )

        return current_expanded

    def _get_node_neighbors(self, node_id: str) -> List[Tuple[str, Dict[str, Any]]]:
        """ë…¸ë“œì˜ ì´ì›ƒë“¤ ì¡°íšŒ (ìºì‹± ì§€ì›)"""

        if node_id in self.node_neighbors_cache:
            return self.node_neighbors_cache[node_id]

        neighbors = []

        # ë‚˜ê°€ëŠ” ì—£ì§€
        for neighbor in self.networkx_graph.successors(node_id):
            edge_data = self.networkx_graph.get_edge_data(node_id, neighbor)
            if edge_data:
                # Multiple edgesì¸ ê²½ìš° ì²« ë²ˆì§¸ ì—£ì§€ ì‚¬ìš©
                first_edge = next(iter(edge_data.values()))
                neighbors.append((neighbor, first_edge))

        # ë“¤ì–´ì˜¤ëŠ” ì—£ì§€
        for neighbor in self.networkx_graph.predecessors(node_id):
            edge_data = self.networkx_graph.get_edge_data(neighbor, node_id)
            if edge_data:
                first_edge = next(iter(edge_data.values()))
                neighbors.append((neighbor, first_edge))

        # ìºì‹œ ì €ì¥
        if self.config.enable_caching:
            self.node_neighbors_cache[node_id] = neighbors

        return neighbors

    def _build_subgraph(
        self, node_ids: Set[str], config: ExtractionConfig
    ) -> Tuple[Dict[str, Dict[str, Any]], List[Dict[str, Any]]]:
        """ì„ íƒëœ ë…¸ë“œë“¤ë¡œ ì„œë¸Œê·¸ë˜í”„ êµ¬ì„±"""

        logger.info(f"ğŸ”¨ Building subgraph from {len(node_ids)} nodes...")

        # ë…¸ë“œ ë°ì´í„° ìˆ˜ì§‘
        subgraph_nodes = {}
        for node_id in node_ids:
            if node_id in self.networkx_graph:
                node_data = dict(self.networkx_graph.nodes[node_id])
                node_data["id"] = node_id  # ID ì¶”ê°€
                subgraph_nodes[node_id] = node_data

        # ì—£ì§€ ë°ì´í„° ìˆ˜ì§‘
        subgraph_edges = []
        edge_count = 0

        for source in node_ids:
            if edge_count >= config.max_edges:
                break

            for target in node_ids:
                if source == target or edge_count >= config.max_edges:
                    continue

                # ì—£ì§€ ì¡´ì¬ í™•ì¸
                if self.networkx_graph.has_edge(source, target):
                    edge_data = self.networkx_graph.get_edge_data(source, target)

                    # Multiple edges ì²˜ë¦¬
                    for key, attrs in edge_data.items():
                        if edge_count >= config.max_edges:
                            break

                        edge_dict = dict(attrs)
                        edge_dict.update(
                            {"source": source, "target": target, "key": key}
                        )
                        subgraph_edges.append(edge_dict)
                        edge_count += 1

        logger.info(
            f"âœ… Subgraph built: {len(subgraph_nodes)} nodes, {len(subgraph_edges)} edges"
        )

        return subgraph_nodes, subgraph_edges

    def _calculate_relevance_scores(
        self,
        nodes: Dict[str, Dict[str, Any]],
        query: str,
        initial_matches: List[SearchResult],
        config: ExtractionConfig,
    ) -> Dict[str, float]:
        """ë…¸ë“œë³„ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""

        relevance_scores = {}

        # ì´ˆê¸° ë§¤ì¹˜ë“¤ì˜ ì ìˆ˜
        initial_scores = {
            result.node_id: result.similarity_score for result in initial_matches
        }

        for node_id, node_data in nodes.items():
            score = 0.0

            # 1. ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜
            if node_id in initial_scores:
                score += initial_scores[node_id] * config.vector_weight

            # 2. ë…¸ë“œ íƒ€ì… ê°€ì¤‘ì¹˜
            node_type = node_data.get("node_type", "unknown")
            type_weight = config.node_type_weights.get(node_type, 0.5)
            score += type_weight * config.metadata_weight

            # 3. ê·¸ë˜í”„ ì¤‘ì‹¬ì„± (ìˆë‹¤ë©´)
            if hasattr(self, "_centrality_scores"):
                centrality = self._centrality_scores.get(node_id, 0)
                score += centrality * config.graph_weight

            # ì •ê·œí™”
            relevance_scores[node_id] = min(1.0, score)

        return relevance_scores

    def _calculate_confidence_score(
        self,
        initial_matches: List[SearchResult],
        total_nodes: int,
        relevance_scores: Dict[str, float],
    ) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""

        if not initial_matches or not relevance_scores:
            return 0.0

        # ì´ˆê¸° ë§¤ì¹˜ë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
        avg_similarity = np.mean([m.similarity_score for m in initial_matches])

        # ê´€ë ¨ì„± ì ìˆ˜ë“¤ì˜ í‰ê· 
        avg_relevance = np.mean(list(relevance_scores.values()))

        # ë…¸ë“œ ìˆ˜ ëŒ€ë¹„ ì ì ˆì„± (ë„ˆë¬´ ë§ê±°ë‚˜ ì ìœ¼ë©´ ì‹ ë¢°ë„ ê°ì†Œ)
        size_factor = 1.0
        if total_nodes < 5:
            size_factor = 0.7  # ë„ˆë¬´ ì ìŒ
        elif total_nodes > 100:
            size_factor = 0.8  # ë„ˆë¬´ ë§ìŒ

        # ì¢…í•© ì‹ ë¢°ë„
        confidence = avg_similarity * 0.4 + avg_relevance * 0.4 + size_factor * 0.2

        return min(1.0, confidence)


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # ê¸°ë³¸ import ê²½ë¡œ ì„¤ì •
    import sys
    from pathlib import Path

    # src ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
    src_dir = Path(__file__).parent.parent.parent
    if str(src_dir) not in sys.path:
        sys.path.insert(0, str(src_dir))

    try:
        from src import GRAPHS_DIR

        print("ğŸ§ª Testing SubgraphExtractor...")

        # íŒŒì¼ ê²½ë¡œ í™•ì¸
        unified_graph_file = GRAPHS_DIR / "unified" / "unified_knowledge_graph.json"
        vector_store_dir = GRAPHS_DIR / "embeddings"

        if not unified_graph_file.exists():
            print(f"âŒ Unified graph not found: {unified_graph_file}")
            return

        if not vector_store_dir.exists():
            print(f"âŒ Vector store not found: {vector_store_dir}")
            return

        # SubgraphExtractor ì´ˆê¸°í™”
        extractor = SubgraphExtractor(
            unified_graph_path=str(unified_graph_file),
            vector_store_path=str(vector_store_dir),
            embedding_model="auto",
        )

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì€?",
            "ê¹€ì² ìˆ˜ êµìˆ˜ì˜ ì—°êµ¬ ë„¤íŠ¸ì›Œí¬",
            "ì „ê¸°ì°¨ ì¶©ì „ ê´€ë ¨ ìµœì‹  ì—°êµ¬ ë™í–¥",
        ]

        print(f"\nğŸ” Testing subgraph extraction...")

        for i, query in enumerate(test_queries[:1]):  # ì²« ë²ˆì§¸ë§Œ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ“ Query {i+1}: {query}")

            try:
                # ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
                result = extractor.extract_subgraph(
                    query=query, strategy=SearchStrategy.HYBRID
                )

                print(f"âœ… Extraction successful:")
                print(f"   ğŸ“„ Nodes: {result.total_nodes}")
                print(f"   ğŸ”— Edges: {result.total_edges}")
                print(f"   ğŸ“Š Node types: {result.nodes_by_type}")
                print(f"   ğŸ¯ Confidence: {result.confidence_score:.3f}")
                print(f"   â±ï¸ Time: {result.extraction_time:.2f}s")

                # ìƒìœ„ ê´€ë ¨ ë…¸ë“œë“¤
                top_relevant = sorted(
                    result.relevance_scores.items(), key=lambda x: x[1], reverse=True
                )[:5]

                print(f"   ğŸ† Top relevant nodes:")
                for node_id, score in top_relevant:
                    node_type = result.nodes[node_id].get("node_type", "unknown")
                    print(f"      {node_id} ({node_type}): {score:.3f}")

            except Exception as e:
                print(f"âŒ Query {i+1} failed: {e}")
                import traceback

                traceback.print_exc()

        print(f"\nâœ… SubgraphExtractor test completed!")

    except Exception as e:
        print(f"âŒ Test setup failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
