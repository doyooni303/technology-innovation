"""
ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ëª¨ë“ˆ
Vector Store Manager for GraphRAG Embeddings

ë‹¤ì–‘í•œ ë²¡í„° ì €ì¥ì†Œ ì§€ì› ë° íš¨ìœ¨ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
- ChromaDB: ì˜êµ¬ ì €ì¥ì†Œ ì§€ì›
- FAISS: ê³ ì† ê²€ìƒ‰ íŠ¹í™”
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + ë©”íƒ€ë°ì´í„° í•„í„°ë§
- ë°°ì¹˜ ì—°ì‚° ë° ìºì‹± ìµœì í™”
"""

import os
import json
import pickle
import logging
import warnings
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union, Set
from dataclasses import dataclass
import numpy as np
import pandas as pd
from collections import defaultdict

# ë²¡í„° ì €ì¥ì†Œ ì˜ì¡´ì„± ì²´í¬
try:
    import chromadb
    from chromadb.config import Settings

    _chromadb_available = True
except (ImportError, RuntimeError) as e:
    _chromadb_available = False
    if "sqlite3" in str(e):
        warnings.warn(
            "ChromaDB not available due to SQLite compatibility. Using FAISS instead."
        )
    else:
        warnings.warn("ChromaDB not available. Install with: pip install chromadb")

try:
    import faiss

    _faiss_available = True
except (ImportError, AttributeError) as e:
    _faiss_available = False
    if "numpy" in str(e).lower() or "_array_api" in str(e):
        warnings.warn(
            "FAISS not available due to NumPy compatibility. Try: pip install 'numpy<2.0' faiss-cpu"
        )
    else:
        warnings.warn("FAISS not available. Install with: pip install faiss-cpu")

from .multi_node_embedder import EmbeddingResult

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ í´ë˜ìŠ¤"""

    node_id: str
    node_type: str
    similarity_score: float
    text: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "node_id": self.node_id,
            "node_type": self.node_type,
            "similarity_score": self.similarity_score,
            "text": self.text,
            "metadata": self.metadata,
        }


@dataclass
class VectorStoreConfig:
    """ë²¡í„° ì €ì¥ì†Œ ì„¤ì •"""

    store_type: str  # "chroma", "faiss", "hybrid"
    persist_directory: Optional[str] = None
    collection_name: str = "graphrag_embeddings"
    distance_metric: str = "cosine"  # "cosine", "l2", "ip"
    index_type: str = "flat"  # "flat", "ivf", "hnsw"
    batch_size: int = 1000
    cache_size: int = 10000


class BaseVectorStore(ABC):
    """ë²¡í„° ì €ì¥ì†Œ ê¸°ë³¸ ì¶”ìƒ í´ë˜ìŠ¤"""

    def __init__(self, config: VectorStoreConfig):
        self.config = config
        self.is_initialized = False
        self.dimension = None
        self.total_vectors = 0

    @abstractmethod
    def initialize(self, dimension: int) -> None:
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
        pass

    @abstractmethod
    def add_embeddings(
        self,
        embeddings: np.ndarray,
        node_ids: List[str],
        node_types: List[str],
        texts: List[str],
        metadatas: List[Dict[str, Any]],
    ) -> None:
        """ì„ë² ë”© ì¶”ê°€"""
        pass

    @abstractmethod
    def search(
        self,
        query_embedding: np.ndarray,
        top_k: int = 10,
        node_types: Optional[List[str]] = None,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[SearchResult]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        pass

    @abstractmethod
    def get_embedding(self, node_id: str) -> Optional[np.ndarray]:
        """íŠ¹ì • ë…¸ë“œì˜ ì„ë² ë”© ë°˜í™˜"""
        pass

    @abstractmethod
    def save(self) -> None:
        """ì €ì¥ì†Œ ì˜êµ¬ ì €ì¥"""
        pass

    @abstractmethod
    def load(self) -> None:
        """ì €ì¥ì†Œ ë¡œë“œ"""
        pass


class ChromaVectorStore(BaseVectorStore):
    """ChromaDB ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ"""

    def __init__(self, config: VectorStoreConfig):
        super().__init__(config)

        if not _chromadb_available:
            raise ImportError("ChromaDB is required but not installed")

        self.client = None
        self.collection = None

    def initialize(self, dimension: int) -> None:
        """ChromaDB ì´ˆê¸°í™”"""
        self.dimension = dimension

        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        if self.config.persist_directory:
            # ì˜êµ¬ ì €ì¥ì†Œ
            persist_path = Path(self.config.persist_directory)
            persist_path.mkdir(parents=True, exist_ok=True)

            self.client = chromadb.PersistentClient(
                path=str(persist_path),
                settings=Settings(anonymized_telemetry=False, is_persistent=True),
            )
        else:
            # ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
            self.client = chromadb.Client()

        # ê±°ë¦¬ ë©”íŠ¸ë¦­ ì„¤ì •
        distance_function = {"cosine": "cosine", "l2": "l2", "ip": "ip"}.get(
            self.config.distance_metric, "cosine"
        )

        # ì»¬ë ‰ì…˜ ìƒì„±/ë¡œë“œ
        try:
            self.collection = self.client.get_collection(
                name=self.config.collection_name
            )
            logger.info(
                f"âœ… Loaded existing ChromaDB collection: {self.config.collection_name}"
            )
        except:
            self.collection = self.client.create_collection(
                name=self.config.collection_name,
                metadata={"hnsw:space": distance_function},
            )
            logger.info(
                f"âœ… Created new ChromaDB collection: {self.config.collection_name}"
            )

        self.is_initialized = True

    def add_embeddings(
        self,
        embeddings: np.ndarray,
        node_ids: List[str],
        node_types: List[str],
        texts: List[str],
        metadatas: List[Dict[str, Any]],
    ) -> None:
        """ChromaDBì— ì„ë² ë”© ì¶”ê°€"""
        if not self.is_initialized:
            raise ValueError("Vector store not initialized")

        # ChromaDB ë©”íƒ€ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜
        chroma_metadatas = []
        for i, (node_id, node_type, metadata) in enumerate(
            zip(node_ids, node_types, metadatas)
        ):
            chroma_meta = {
                "node_type": node_type,
                "text_length": len(texts[i]),
                "word_count": len(texts[i].split()),
            }

            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ChromaDBëŠ” ê°„ë‹¨í•œ íƒ€ì…ë§Œ ì§€ì›)
            for key, value in metadata.items():
                if isinstance(value, (str, int, float, bool)):
                    chroma_meta[key] = value
                elif isinstance(value, list) and len(value) > 0:
                    if isinstance(value[0], (str, int, float)):
                        chroma_meta[f"{key}_count"] = len(value)
                        if isinstance(value[0], str):
                            chroma_meta[f"{key}_first"] = value[0][:100]  # ì²« ë²ˆì§¸ ê°’ë§Œ

            chroma_metadatas.append(chroma_meta)

        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = self.config.batch_size
        for i in range(0, len(embeddings), batch_size):
            end_idx = min(i + batch_size, len(embeddings))

            batch_embeddings = embeddings[i:end_idx].tolist()
            batch_ids = node_ids[i:end_idx]
            batch_texts = texts[i:end_idx]
            batch_metadatas = chroma_metadatas[i:end_idx]

            try:
                self.collection.add(
                    embeddings=batch_embeddings,
                    documents=batch_texts,
                    metadatas=batch_metadatas,
                    ids=batch_ids,
                )
            except Exception as e:
                logger.warning(f"Failed to add batch {i}-{end_idx}: {e}")

        self.total_vectors += len(embeddings)
        logger.info(f"âœ… Added {len(embeddings)} embeddings to ChromaDB")

    def search(
        self,
        query_embedding: np.ndarray,
        top_k: int = 10,
        node_types: Optional[List[str]] = None,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[SearchResult]:
        """ChromaDB ê²€ìƒ‰"""
        if not self.is_initialized:
            raise ValueError("Vector store not initialized")

        # í•„í„° êµ¬ì„±
        where_filter = {}
        if node_types:
            where_filter["node_type"] = {"$in": node_types}

        if filters:
            for key, value in filters.items():
                if isinstance(value, list):
                    where_filter[key] = {"$in": value}
                else:
                    where_filter[key] = value

        # ê²€ìƒ‰ ì‹¤í–‰
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=top_k,
                where=where_filter if where_filter else None,
                include=["documents", "metadatas", "distances"],
            )

            # ê²°ê³¼ ë³€í™˜
            search_results = []
            if results["ids"] and results["ids"][0]:
                for i, node_id in enumerate(results["ids"][0]):
                    similarity = (
                        1.0 - results["distances"][0][i]
                    )  # ChromaDBëŠ” ê±°ë¦¬ë¥¼ ë°˜í™˜

                    search_result = SearchResult(
                        node_id=node_id,
                        node_type=results["metadatas"][0][i].get(
                            "node_type", "unknown"
                        ),
                        similarity_score=similarity,
                        text=results["documents"][0][i],
                        metadata=results["metadatas"][0][i],
                    )
                    search_results.append(search_result)

            return search_results

        except Exception as e:
            logger.error(f"ChromaDB search failed: {e}")
            return []

    def get_embedding(self, node_id: str) -> Optional[np.ndarray]:
        """íŠ¹ì • ë…¸ë“œ ì„ë² ë”© ì¡°íšŒ"""
        try:
            result = self.collection.get(ids=[node_id], include=["embeddings"])

            if result["embeddings"] and result["embeddings"][0]:
                return np.array(result["embeddings"][0])
            return None

        except Exception as e:
            logger.warning(f"Failed to get embedding for {node_id}: {e}")
            return None

    def save(self) -> None:
        """ChromaDB ì €ì¥ (ì˜êµ¬ ì €ì¥ì†Œì¸ ê²½ìš° ìë™)"""
        if self.config.persist_directory:
            logger.info("âœ… ChromaDB automatically persisted")
        else:
            logger.warning("âš ï¸ In-memory ChromaDB cannot be persisted")

    def load(self) -> None:
        """ChromaDB ë¡œë“œ (ì´ˆê¸°í™” ì‹œ ìë™)"""
        logger.info("âœ… ChromaDB loaded during initialization")


class FAISSVectorStore(BaseVectorStore):
    """FAISS ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ"""

    def __init__(self, config: VectorStoreConfig):
        super().__init__(config)

        if not _faiss_available:
            raise ImportError("FAISS is required but not installed")

        self.index = None
        self.node_id_to_idx = {}
        self.idx_to_node_id = {}
        self.node_metadatas = {}
        self.node_texts = {}
        self.node_types = {}

        # GPU ë¦¬ì†ŒìŠ¤ ì„¤ì •
        self.gpu_resources = None
        self.use_gpu = False

    def initialize(self, dimension: int) -> None:
        """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        self.dimension = dimension

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        try:
            import faiss

            ngpus = faiss.get_num_gpus()
            self.use_gpu = ngpus > 0

            if self.use_gpu:
                self.gpu_resources = faiss.StandardGpuResources()
                print(f"âœ… FAISS will use GPU ({ngpus} GPUs available)")
        except:
            self.use_gpu = False
            print("âš ï¸ GPU not available, using CPU")

        # ì¸ë±ìŠ¤ íƒ€ì…ì— ë”°ë¥¸ ìƒì„±
        if self.config.index_type == "flat":
            if self.config.distance_metric == "cosine":
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš© (ë‚´ì )
                cpu_index = faiss.IndexFlatIP(dimension)
            else:
                # L2 ê±°ë¦¬
                cpu_index = faiss.IndexFlatL2(dimension)

        elif self.config.index_type == "ivf":
            # IVF (Inverted File) ì¸ë±ìŠ¤
            nlist = 100  # í´ëŸ¬ìŠ¤í„° ìˆ˜
            quantizer = faiss.IndexFlatL2(dimension)
            cpu_index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

        elif self.config.index_type == "hnsw":
            # HNSW (Hierarchical Navigable Small World)
            cpu_index = faiss.IndexHNSWFlat(dimension, 32)
            cpu_index.hnsw.efConstruction = 40
            cpu_index.hnsw.efSearch = 16

        else:
            # ê¸°ë³¸ê°’: Flat ì¸ë±ìŠ¤
            cpu_index = faiss.IndexFlatIP(dimension)

        # GPUë¡œ ì „ì†¡ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.use_gpu and cpu_index:
            self.index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, cpu_index)
        else:
            self.index = cpu_index

        self.is_initialized = True
        gpu_status = "GPU" if self.use_gpu else "CPU"
        logger.info(
            f"âœ… FAISS index initialized: {self.config.index_type}, {gpu_status}, dim={dimension}"
        )

    def add_embeddings(
        self,
        embeddings: np.ndarray,
        node_ids: List[str],
        node_types: List[str],
        texts: List[str],
        metadatas: List[Dict[str, Any]],
    ) -> None:
        """FAISSì— ì„ë² ë”© ì¶”ê°€"""
        if not self.is_initialized:
            raise ValueError("Vector store not initialized")

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš© ì •ê·œí™” (í•„ìš”ì‹œ)
        if self.config.distance_metric == "cosine":
            # ì„ë² ë”©ì´ ì´ë¯¸ ì •ê·œí™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            norms = np.linalg.norm(embeddings, axis=1)
            if not np.allclose(norms, 1.0, atol=1e-6):
                embeddings = embeddings / norms[:, np.newaxis]

        # ì¸ë±ìŠ¤ í›ˆë ¨ (IVFì˜ ê²½ìš°)
        if self.config.index_type == "ivf" and not self.index.is_trained:
            logger.info("ğŸ”§ Training FAISS IVF index...")
            self.index.train(embeddings.astype(np.float32))

        # í˜„ì¬ ì¸ë±ìŠ¤ í¬ê¸°
        current_size = self.index.ntotal

        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(embeddings.astype(np.float32))

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        for i, (node_id, node_type, text, metadata) in enumerate(
            zip(node_ids, node_types, texts, metadatas)
        ):
            idx = current_size + i
            self.node_id_to_idx[node_id] = idx
            self.idx_to_node_id[idx] = node_id
            self.node_metadatas[node_id] = metadata
            self.node_texts[node_id] = text
            self.node_types[node_id] = node_type

        self.total_vectors += len(embeddings)
        logger.info(f"âœ… Added {len(embeddings)} embeddings to FAISS index")

    def search(
        self,
        query_embedding: np.ndarray,
        top_k: int = 10,
        node_types: Optional[List[str]] = None,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[SearchResult]:
        """FAISS ê²€ìƒ‰"""
        if not self.is_initialized:
            raise ValueError("Vector store not initialized")

        # ì¿¼ë¦¬ ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
        if self.config.distance_metric == "cosine":
            query_norm = np.linalg.norm(query_embedding)
            if query_norm > 0:
                query_embedding = query_embedding / query_norm

        # ê²€ìƒ‰ ì‹¤í–‰
        query_embedding = query_embedding.astype(np.float32).reshape(1, -1)

        # ë” ë§ì´ ê²€ìƒ‰í•œ í›„ í•„í„°ë§ (FAISSëŠ” ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ)
        search_k = (
            min(top_k * 10, self.total_vectors) if node_types or filters else top_k
        )

        try:
            scores, indices = self.index.search(query_embedding, search_k)

            # ê²°ê³¼ ë³€í™˜ ë° í•„í„°ë§
            search_results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx == -1:  # ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤
                    continue

                node_id = self.idx_to_node_id.get(idx)
                if not node_id:
                    continue

                node_type = self.node_types.get(node_id, "unknown")

                # ë…¸ë“œ íƒ€ì… í•„í„°ë§
                if node_types and node_type not in node_types:
                    continue

                # ì¶”ê°€ í•„í„° ì ìš©
                if filters:
                    metadata = self.node_metadatas.get(node_id, {})
                    if not self._apply_filters(metadata, filters):
                        continue

                # ìœ ì‚¬ë„ ì ìˆ˜ ë³€í™˜
                if self.config.distance_metric == "cosine":
                    similarity = float(score)  # ë‚´ì  ê°’ (ì´ë¯¸ ì •ê·œí™”ë¨)
                else:
                    similarity = 1.0 / (1.0 + float(score))  # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜

                search_result = SearchResult(
                    node_id=node_id,
                    node_type=node_type,
                    similarity_score=similarity,
                    text=self.node_texts.get(node_id, ""),
                    metadata=self.node_metadatas.get(node_id, {}),
                )
                search_results.append(search_result)

                if len(search_results) >= top_k:
                    break

            return search_results

        except Exception as e:
            logger.error(f"FAISS search failed: {e}")
            return []

    def _apply_filters(self, metadata: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©"""
        for key, value in filters.items():
            if key not in metadata:
                return False

            metadata_value = metadata[key]

            if isinstance(value, list):
                if metadata_value not in value:
                    return False
            else:
                if metadata_value != value:
                    return False

        return True

    def get_embedding(self, node_id: str) -> Optional[np.ndarray]:
        """íŠ¹ì • ë…¸ë“œ ì„ë² ë”© ì¡°íšŒ"""
        idx = self.node_id_to_idx.get(node_id)
        if idx is None:
            return None

        try:
            # FAISSì—ì„œ íŠ¹ì • ë²¡í„° ì¶”ì¶œ
            embedding = self.index.reconstruct(idx)
            return embedding
        except Exception as e:
            logger.warning(f"Failed to get embedding for {node_id}: {e}")
            return None

    def save(self) -> None:
        """FAISS ì¸ë±ìŠ¤ ì €ì¥"""
        if not self.config.persist_directory:
            logger.warning("âš ï¸ No persist directory configured")
            return

        persist_path = Path(self.config.persist_directory)
        persist_path.mkdir(parents=True, exist_ok=True)

        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            index_file = persist_path / "faiss_index.bin"
            faiss.write_index(self.index, str(index_file))

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_file = persist_path / "faiss_metadata.pkl"
            metadata = {
                "node_id_to_idx": self.node_id_to_idx,
                "idx_to_node_id": self.idx_to_node_id,
                "node_metadatas": self.node_metadatas,
                "node_texts": self.node_texts,
                "node_types": self.node_types,
                "total_vectors": self.total_vectors,
                "dimension": self.dimension,
                "config": self.config,
            }

            with open(metadata_file, "wb") as f:
                pickle.dump(metadata, f)

            logger.info(f"âœ… FAISS index saved to {persist_path}")

        except Exception as e:
            logger.error(f"âŒ Failed to save FAISS index: {e}")

    def load(self) -> None:
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
        if not self.config.persist_directory:
            logger.warning("âš ï¸ No persist directory configured")
            return

        persist_path = Path(self.config.persist_directory)
        index_file = persist_path / "faiss_index.bin"
        metadata_file = persist_path / "faiss_metadata.pkl"

        if not index_file.exists() or not metadata_file.exists():
            logger.info("ğŸ“‚ No existing FAISS index found")
            return

        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            self.index = faiss.read_index(str(index_file))

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(metadata_file, "rb") as f:
                metadata = pickle.load(f)

            self.node_id_to_idx = metadata["node_id_to_idx"]
            self.idx_to_node_id = metadata["idx_to_node_id"]
            self.node_metadatas = metadata["node_metadatas"]
            self.node_texts = metadata["node_texts"]
            self.node_types = metadata["node_types"]
            self.total_vectors = metadata["total_vectors"]
            self.dimension = metadata["dimension"]

            self.is_initialized = True
            logger.info(f"âœ… FAISS index loaded: {self.total_vectors} vectors")

        except Exception as e:
            logger.error(f"âŒ Failed to load FAISS index: {e}")


class VectorStoreManager:
    """ë²¡í„° ì €ì¥ì†Œ í†µí•© ê´€ë¦¬ì"""

    def __init__(
        self,
        store_type: str = "auto",
        persist_directory: Optional[str] = None,
        collection_name: str = "graphrag_embeddings",
        **kwargs,
    ):
        """
        Args:
            store_type: ì €ì¥ì†Œ íƒ€ì… ("auto", "chroma", "faiss", "hybrid")
            persist_directory: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
            collection_name: ì»¬ë ‰ì…˜/ì¸ë±ìŠ¤ ì´ë¦„
            **kwargs: ì¶”ê°€ ì„¤ì •
        """

        # ìë™ ì €ì¥ì†Œ ì„ íƒ
        if store_type == "auto":
            if _chromadb_available:
                store_type = "chroma"
            elif _faiss_available:
                store_type = "faiss"
            else:
                raise ImportError(
                    "No vector store library available. Install chromadb or faiss-cpu"
                )

        # ì„¤ì • ìƒì„±
        self.config = VectorStoreConfig(
            store_type=store_type,
            persist_directory=persist_directory,
            collection_name=collection_name,
            **kwargs,
        )

        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        if store_type == "chroma":
            self.store = ChromaVectorStore(self.config)
        elif store_type == "faiss":
            self.store = FAISSVectorStore(self.config)
        else:
            raise ValueError(f"Unsupported store type: {store_type}")

        logger.info(f"âœ… VectorStoreManager initialized: {store_type}")

    def load_from_embeddings(
        self, embedding_results: Dict[str, List[EmbeddingResult]]
    ) -> None:
        """EmbeddingResultë¡œë¶€í„° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""

        # ì°¨ì› ê²°ì •
        first_result = next(iter(next(iter(embedding_results.values()))))
        dimension = len(first_result.embedding)

        # ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.store.initialize(dimension)

        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹œë„
        self.store.load()

        logger.info(f"ğŸ“š Loading embeddings into vector store...")

        # íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
        for node_type, results in embedding_results.items():
            if not results:
                continue

            logger.info(f"ğŸ“ Processing {node_type}: {len(results)} embeddings")

            # ë°°ì¹˜ë¡œ ë³€í™˜
            embeddings = np.array([result.embedding for result in results])
            node_ids = [result.node_id for result in results]
            node_types = [result.node_type for result in results]
            texts = [result.text for result in results]
            metadatas = [result.metadata for result in results]

            # ì €ì¥ì†Œì— ì¶”ê°€
            self.store.add_embeddings(
                embeddings=embeddings,
                node_ids=node_ids,
                node_types=node_types,
                texts=texts,
                metadatas=metadatas,
            )

        # ì €ì¥
        self.store.save()

        total_vectors = sum(len(results) for results in embedding_results.values())
        logger.info(f"âœ… Loaded {total_vectors} embeddings into vector store")

    def search_similar_nodes(
        self,
        query_embedding: np.ndarray,
        top_k: int = 10,
        node_types: Optional[List[str]] = None,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[SearchResult]:
        """ìœ ì‚¬ ë…¸ë“œ ê²€ìƒ‰"""
        return self.store.search(
            query_embedding=query_embedding,
            top_k=top_k,
            node_types=node_types,
            filters=filters,
        )

    def get_node_embedding(self, node_id: str) -> Optional[np.ndarray]:
        """íŠ¹ì • ë…¸ë“œ ì„ë² ë”© ì¡°íšŒ"""
        return self.store.get_embedding(node_id)

    def get_store_info(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ ì •ë³´ ë°˜í™˜"""
        return {
            "store_type": self.config.store_type,
            "total_vectors": self.store.total_vectors,
            "dimension": self.store.dimension,
            "is_initialized": self.store.is_initialized,
            "persist_directory": self.config.persist_directory,
            "collection_name": self.config.collection_name,
        }


def create_vector_store(
    store_type: str = "auto", persist_directory: Optional[str] = None, **kwargs
) -> VectorStoreManager:
    """ë²¡í„° ì €ì¥ì†Œ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return VectorStoreManager(
        store_type=store_type, persist_directory=persist_directory, **kwargs
    )


def list_available_stores() -> Dict[str, bool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„° ì €ì¥ì†Œ ëª©ë¡"""
    return {"chromadb": _chromadb_available, "faiss": _faiss_available}


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ§ª Testing Vector Store Manager...")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì €ì¥ì†Œ í™•ì¸
    available = list_available_stores()
    print(f"ğŸ“‹ Available stores: {available}")

    if not any(available.values()):
        print("âŒ No vector stores available for testing")
        print("Install with: pip install chromadb faiss-cpu")
        exit(1)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    import tempfile

    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"ğŸ”§ Testing with temp directory: {temp_dir}")

        # í…ŒìŠ¤íŠ¸ ì„ë² ë”© ë°ì´í„°
        test_embeddings = {
            "paper": [
                EmbeddingResult(
                    node_id="paper_1",
                    node_type="paper",
                    text="Machine learning for battery optimization",
                    embedding=np.random.random(384),
                    metadata={"year": "2023", "has_abstract": True},
                ),
                EmbeddingResult(
                    node_id="paper_2",
                    node_type="paper",
                    text="Deep learning approaches to energy management",
                    embedding=np.random.random(384),
                    metadata={"year": "2022", "has_abstract": False},
                ),
            ],
            "author": [
                EmbeddingResult(
                    node_id="author_1",
                    node_type="author",
                    text="ê¹€ì² ìˆ˜ machine learning researcher",
                    embedding=np.random.random(384),
                    metadata={
                        "paper_count": 15,
                        "productivity_type": "Leading Researcher",
                    },
                )
            ],
        }

        # ê° ì €ì¥ì†Œ íƒ€ì… í…ŒìŠ¤íŠ¸
        for store_type, available in available.items():
            if not available:
                continue

            print(f"\nğŸ”§ Testing {store_type.upper()}...")

            try:
                # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                manager = create_vector_store(
                    store_type=store_type, persist_directory=f"{temp_dir}/{store_type}"
                )

                # ì„ë² ë”© ë¡œë“œ
                manager.load_from_embeddings(test_embeddings)

                # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                query_embedding = np.random.random(384)
                results = manager.search_similar_nodes(
                    query_embedding=query_embedding, top_k=5
                )

                print(f"âœ… {store_type}: {len(results)} search results")
                for result in results[:2]:
                    print(f"   ğŸ“„ {result.node_id}: {result.similarity_score:.3f}")

                # ì •ë³´ ì¶œë ¥
                info = manager.get_store_info()
                print(f"ğŸ“Š Store info: {info}")

            except Exception as e:
                print(f"âŒ {store_type} test failed: {e}")

    print(f"\nâœ… Vector Store Manager tests completed!")
