"""
ì„ë² ë”© ëª¨ë¸ ì¶”ìƒí™” ë° êµ¬í˜„
Embedding Models for GraphRAG System

ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µ
- BaseEmbeddingModel: ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤
- SentenceTransformerModel: sentence-transformers ë˜í¼
- HuggingFaceModel: transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë˜í¼
- ëª¨ë¸ ìë™ ì„ íƒ ë° ì„¤ì • ê´€ë¦¬
"""

import os
import json
import logging
import warnings
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
import numpy as np
from dataclasses import dataclass

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class EmbeddingConfig:
    """ì„ë² ë”© ì„¤ì • í´ë˜ìŠ¤"""

    model_name: str
    model_type: str  # "sentence-transformers", "huggingface", "openai", etc.
    dimension: int
    max_length: int
    batch_size: int = 32
    device: str = "auto"  # "auto", "cpu", "cuda", "mps"
    cache_dir: Optional[str] = None
    model_kwargs: Dict[str, Any] = None

    def __post_init__(self):
        if self.model_kwargs is None:
            self.model_kwargs = {}


class BaseEmbeddingModel(ABC):
    """ì„ë² ë”© ëª¨ë¸ ê¸°ë³¸ ì¶”ìƒ í´ë˜ìŠ¤"""

    def __init__(self, config: EmbeddingConfig):
        """
        Args:
            config: ì„ë² ë”© ì„¤ì •
        """
        self.config = config
        self.model = None
        self._is_loaded = False

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._determine_device(config.device)
        logger.info(f"ğŸ”§ Embedding model will use device: {self.device}")

    def _determine_device(self, device_preference: str) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if device_preference == "auto":
            try:
                import torch

                if torch.cuda.is_available():
                    return "cuda"
                elif (
                    hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
                ):
                    return "mps"  # Apple Silicon
                else:
                    return "cpu"
            except ImportError:
                return "cpu"
        else:
            return device_preference

    @abstractmethod
    def load_model(self) -> None:
        """ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        pass

    @abstractmethod
    def encode(
        self,
        texts: Union[str, List[str]],
        batch_size: Optional[int] = None,
        show_progress: bool = False,
    ) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜

        Args:
            texts: ì¸ì½”ë”©í•  í…ìŠ¤íŠ¸(ë“¤)
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€

        Returns:
            ì„ë² ë”© ë°°ì—´ (n_texts, embedding_dim)
        """
        pass

    @abstractmethod
    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        pass

    def encode_single(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© (í¸ì˜ í•¨ìˆ˜)"""
        return self.encode([text])[0]

    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        return self._is_loaded

    def unload_model(self) -> None:
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)"""
        self.model = None
        self._is_loaded = False
        logger.info("ğŸ—‘ï¸ Model unloaded")


class SentenceTransformerModel(BaseEmbeddingModel):
    """sentence-transformers ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸"""

    def __init__(self, config: EmbeddingConfig):
        super().__init__(config)
        self.model_name = config.model_name

        # sentence-transformers ì˜ì¡´ì„± ì²´í¬
        try:
            import sentence_transformers

            self._st_available = True
        except ImportError:
            self._st_available = False
            logger.warning("sentence-transformers not available")

    def load_model(self) -> None:
        """SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
        if not self._st_available:
            raise ImportError(
                "sentence-transformers is required for this model.\n"
                "Install with: pip install sentence-transformers"
            )

        if self._is_loaded:
            return

        try:
            from sentence_transformers import SentenceTransformer

            logger.info(f"ğŸ“¥ Loading SentenceTransformer: {self.model_name}")

            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            load_kwargs = {"device": self.device, **self.config.model_kwargs}

            if self.config.cache_dir:
                load_kwargs["cache_folder"] = self.config.cache_dir

            self.model = SentenceTransformer(self.model_name, **load_kwargs)
            self._is_loaded = True

            logger.info(f"âœ… Model loaded successfully")
            logger.info(f"   Dimension: {self.get_embedding_dimension()}")
            logger.info(f"   Max length: {self.model.max_seq_length}")

        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            raise

    def encode(
        self,
        texts: Union[str, List[str]],
        batch_size: Optional[int] = None,
        show_progress: bool = False,
    ) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if not self._is_loaded:
            self.load_model()

        if isinstance(texts, str):
            texts = [texts]

        batch_size = batch_size or self.config.batch_size

        try:
            # ê²½ê³  ì–µì œ (tqdmê³¼ ì¶©ëŒ ë°©ì§€)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                embeddings = self.model.encode(
                    texts,
                    batch_size=batch_size,
                    show_progress_bar=show_progress,
                    convert_to_numpy=True,
                    normalize_embeddings=True,  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´ ì •ê·œí™”
                )

            return embeddings

        except Exception as e:
            logger.error(f"âŒ Encoding failed: {e}")
            raise

    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        if not self._is_loaded:
            # ëª¨ë¸ ë¡œë“œ ì—†ì´ ì°¨ì› ìˆ˜ë¥¼ ì•Œê¸° ìœ„í•œ ì„ì‹œ ë¡œë“œ
            temp_model = self.model
            self.load_model()
            dim = self.model.get_sentence_embedding_dimension()
            if temp_model is None:  # ì›ë˜ ë¡œë“œë˜ì§€ ì•Šì•˜ë˜ ê²½ìš° ë‹¤ì‹œ ì–¸ë¡œë“œ
                self.unload_model()
            return dim
        else:
            return self.model.get_sentence_embedding_dimension()


class HuggingFaceModel(BaseEmbeddingModel):
    """HuggingFace transformers ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸"""

    def __init__(self, config: EmbeddingConfig):
        super().__init__(config)
        self.model_name = config.model_name

        # transformers ì˜ì¡´ì„± ì²´í¬
        try:
            import transformers
            import torch

            self._hf_available = True
        except ImportError:
            self._hf_available = False
            logger.warning("transformers or torch not available")

    def load_model(self) -> None:
        """HuggingFace ëª¨ë¸ ë¡œë“œ"""
        if not self._hf_available:
            raise ImportError(
                "transformers and torch are required for this model.\n"
                "Install with: pip install transformers torch"
            )

        if self._is_loaded:
            return

        try:
            from transformers import AutoTokenizer, AutoModel
            import torch

            logger.info(f"ğŸ“¥ Loading HuggingFace model: {self.model_name}")

            # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
            load_kwargs = {**self.config.model_kwargs}

            if self.config.cache_dir:
                load_kwargs["cache_dir"] = self.config.cache_dir

            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, **load_kwargs
            )
            self.model = AutoModel.from_pretrained(self.model_name, **load_kwargs)

            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.device != "cpu":
                self.model = self.model.to(self.device)

            self.model.eval()  # í‰ê°€ ëª¨ë“œ
            self._is_loaded = True

            logger.info(f"âœ… HuggingFace model loaded successfully")

        except Exception as e:
            logger.error(f"âŒ Failed to load HuggingFace model: {e}")
            raise

    def encode(
        self,
        texts: Union[str, List[str]],
        batch_size: Optional[int] = None,
        show_progress: bool = False,
    ) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if not self._is_loaded:
            self.load_model()

        if isinstance(texts, str):
            texts = [texts]

        batch_size = batch_size or self.config.batch_size

        try:
            import torch
            from tqdm import tqdm

            all_embeddings = []

            # ë°°ì¹˜ ì²˜ë¦¬
            for i in tqdm(
                range(0, len(texts), batch_size),
                desc="Encoding",
                disable=not show_progress,
            ):
                batch_texts = texts[i : i + batch_size]

                # í† í¬ë‚˜ì´ì§•
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=self.config.max_length,
                    return_tensors="pt",
                )

                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                # ì¶”ë¡ 
                with torch.no_grad():
                    outputs = self.model(**inputs)

                    # [CLS] í† í° ë˜ëŠ” í‰ê·  í’€ë§
                    if (
                        hasattr(outputs, "pooler_output")
                        and outputs.pooler_output is not None
                    ):
                        # BERT ìŠ¤íƒ€ì¼ (CLS í† í°)
                        embeddings = outputs.pooler_output
                    else:
                        # í‰ê·  í’€ë§
                        token_embeddings = outputs.last_hidden_state
                        attention_mask = inputs["attention_mask"]

                        # ë§ˆìŠ¤í¬ë¥¼ ê³ ë ¤í•œ í‰ê· 
                        input_mask_expanded = (
                            attention_mask.unsqueeze(-1)
                            .expand(token_embeddings.size())
                            .float()
                        )
                        embeddings = torch.sum(
                            token_embeddings * input_mask_expanded, 1
                        ) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

                    # ì •ê·œí™”
                    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

                    # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
                    batch_embeddings = embeddings.cpu().numpy()
                    all_embeddings.append(batch_embeddings)

            return np.vstack(all_embeddings)

        except Exception as e:
            logger.error(f"âŒ HuggingFace encoding failed: {e}")
            raise

    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        if not self._is_loaded:
            self.load_model()

        return self.model.config.hidden_size


# ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
EMBEDDING_MODELS = {
    # Sentence Transformers ëª¨ë¸ë“¤
    "sentence-transformers": {
        # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ë“¤
        "all-MiniLM-L6-v2": {
            "dimension": 384,
            "max_length": 256,
            "description": "Fast and good quality, English focused",
        },
        "all-mpnet-base-v2": {
            "dimension": 768,
            "max_length": 384,
            "description": "High quality, English focused",
        },
        "paraphrase-multilingual-MiniLM-L12-v2": {
            "dimension": 384,
            "max_length": 128,
            "description": "Multilingual, good for Korean/English",
        },
        "paraphrase-multilingual-mpnet-base-v2": {
            "dimension": 768,
            "max_length": 128,
            "description": "High quality multilingual",
        },
        # ê³¼í•™ ë…¼ë¬¸ íŠ¹í™”
        "allenai/specter": {
            "dimension": 768,
            "max_length": 512,
            "description": "Scientific papers specialist",
        },
        "allenai/specter2_base": {
            "dimension": 768,
            "max_length": 512,
            "description": "Updated scientific papers model",
        },
    },
    # HuggingFace ëª¨ë¸ë“¤
    "huggingface": {
        "sentence-transformers/all-MiniLM-L6-v2": {
            "dimension": 384,
            "max_length": 256,
            "description": "Same as ST version but via HF",
        },
        "klue/bert-base": {
            "dimension": 768,
            "max_length": 512,
            "description": "Korean BERT",
        },
        "microsoft/DialoGPT-medium": {
            "dimension": 1024,
            "max_length": 1024,
            "description": "Dialog focused",
        },
    },
}


def get_available_models() -> Dict[str, Dict[str, Any]]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return EMBEDDING_MODELS


def get_model_info(
    model_name: str, model_type: str = "auto"
) -> Optional[Dict[str, Any]]:
    """íŠ¹ì • ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    if model_type == "auto":
        # ëª¨ë“  íƒ€ì…ì—ì„œ ê²€ìƒ‰
        for mtype, models in EMBEDDING_MODELS.items():
            if model_name in models:
                info = models[model_name].copy()
                info["model_type"] = mtype
                return info
        return None
    else:
        return EMBEDDING_MODELS.get(model_type, {}).get(model_name)


def recommend_model(
    language: str = "mixed", quality: str = "balanced", use_case: str = "academic"
) -> Tuple[str, str]:
    """ì‚¬ìš© ì¼€ì´ìŠ¤ì— ë”°ë¥¸ ëª¨ë¸ ì¶”ì²œ

    Args:
        language: "korean", "english", "mixed"
        quality: "fast", "balanced", "high"
        use_case: "academic", "general", "dialog"

    Returns:
        (model_name, model_type) íŠœí”Œ
    """

    # í•™ìˆ  ë…¼ë¬¸ íŠ¹í™”
    if use_case == "academic":
        if quality == "high":
            return "allenai/specter2_base", "sentence-transformers"
        else:
            return "allenai/specter", "sentence-transformers"

    # ë‹¤êµ­ì–´ (í•œêµ­ì–´/ì˜ì–´ í˜¼ìš©)
    if language in ["mixed", "korean"]:
        if quality == "fast":
            return "paraphrase-multilingual-MiniLM-L12-v2", "sentence-transformers"
        else:
            return "paraphrase-multilingual-mpnet-base-v2", "sentence-transformers"

    # ì˜ì–´ ì „ìš©
    if language == "english":
        if quality == "fast":
            return "all-MiniLM-L6-v2", "sentence-transformers"
        else:
            return "all-mpnet-base-v2", "sentence-transformers"

    # ê¸°ë³¸ê°’
    return "paraphrase-multilingual-mpnet-base-v2", "sentence-transformers"


def create_embedding_model(
    model_name: str = "auto", model_type: str = "auto", device: str = "auto", **kwargs
) -> BaseEmbeddingModel:
    """ì„ë² ë”© ëª¨ë¸ íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        model_name: ëª¨ë¸ëª… ("auto"ë©´ ìë™ ì¶”ì²œ)
        model_type: ëª¨ë¸ íƒ€ì… ("auto"ë©´ ìë™ ê°ì§€)
        device: ë””ë°”ì´ìŠ¤ ("auto"ë©´ ìë™ ì„ íƒ)
        **kwargs: ì¶”ê°€ ì„¤ì •

    Returns:
        BaseEmbeddingModel ì¸ìŠ¤í„´ìŠ¤
    """

    # ìë™ ëª¨ë¸ ì„ íƒ
    if model_name == "auto":
        # ê¸°ë³¸ ì¶”ì²œ (í•™ìˆ  ë…¼ë¬¸ìš© ë‹¤êµ­ì–´ ëª¨ë¸)
        model_name, model_type = recommend_model(
            language="mixed", quality="balanced", use_case="academic"
        )
        logger.info(f"ğŸ¯ Auto-selected model: {model_name} ({model_type})")

    # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
    if model_type == "auto":
        model_info = get_model_info(model_name, "auto")
        if model_info:
            model_type = model_info["model_type"]
        else:
            # ê¸°ë³¸ê°’: sentence-transformers
            model_type = "sentence-transformers"

    # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    model_info = get_model_info(model_name, model_type)
    if not model_info:
        logger.warning(f"âš ï¸ Model {model_name} not in registry, using defaults")
        model_info = {"dimension": 768, "max_length": 512}

    # ì„¤ì • ìƒì„±
    config = EmbeddingConfig(
        model_name=model_name,
        model_type=model_type,
        dimension=model_info.get("dimension", 768),
        max_length=model_info.get("max_length", 512),
        device=device,
        **kwargs,
    )

    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    if model_type == "sentence-transformers":
        return SentenceTransformerModel(config)
    elif model_type == "huggingface":
        return HuggingFaceModel(config)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")


def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì¶œë ¥"""
    print("ğŸ¤– Available Embedding Models:")
    print("=" * 50)

    for model_type, models in EMBEDDING_MODELS.items():
        print(f"\nğŸ“¦ {model_type.upper()}:")
        for name, info in models.items():
            dim = info["dimension"]
            desc = info["description"]
            print(f"  â€¢ {name} (dim:{dim}) - {desc}")

    print(f"\nğŸ’¡ Usage:")
    print(f"  model = create_embedding_model('auto')  # Auto selection")
    print(f"  model = create_embedding_model('all-MiniLM-L6-v2')  # Specific model")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ§ª Testing Embedding Models...")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    list_models()

    # ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¯ Model Recommendations:")
    cases = [
        ("mixed", "fast", "academic"),
        ("english", "high", "general"),
        ("korean", "balanced", "academic"),
    ]

    for lang, qual, use in cases:
        rec_model, rec_type = recommend_model(lang, qual, use)
        print(f"  {lang}/{qual}/{use}: {rec_model} ({rec_type})")
