"""
Ïª®ÌÖçÏä§Ìä∏ ÏßÅÎ†¨Ìôî Î™®Îìà
Context Serializer for GraphRAG System

ÏÑúÎ∏åÍ∑∏ÎûòÌîÑÎ•º LLMÏù¥ Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî Íµ¨Ï°∞ÌôîÎêú ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò
- ÏøºÎ¶¨ ÌÉÄÏûÖÎ≥Ñ ÏµúÏ†ÅÌôîÎêú ÌÖçÏä§Ìä∏ Íµ¨Ï°∞
- ÌÜ†ÌÅ∞ Ï†úÌïú Í≥†Î†§Ìïú Ïä§ÎßàÌä∏ ÏïïÏ∂ï
- Í¥ÄÎ†®ÏÑ± Í∏∞Î∞ò Ïö∞ÏÑ†ÏàúÏúÑ Ï†ïÎ†¨
- ÌïúÍµ≠Ïñ¥/ÏòÅÏñ¥ ÌòºÏö© ÏßÄÏõê
"""

import re
import json
import logging
from enum import Enum
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from collections import defaultdict, Counter
import numpy as np

try:
    from .subgraph_extractor import SubgraphResult
    from ..query_analyzer import QueryAnalysisResult, QueryType, QueryComplexity
except ImportError as e:
    import warnings

    warnings.warn(f"Some GraphRAG components not available: {e}")

# Î°úÍπÖ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)


class SerializationFormat(Enum):
    """ÏßÅÎ†¨Ìôî ÌòïÌÉú"""

    STRUCTURED = "structured"  # Íµ¨Ï°∞ÌôîÎêú ÏÑπÏÖòÎ≥Ñ
    NARRATIVE = "narrative"  # ÏûêÏó∞Ïñ¥ ÏÑúÏà†Ìòï
    LIST_BASED = "list_based"  # Î¶¨Ïä§Ìä∏ Í∏∞Î∞ò
    HIERARCHICAL = "hierarchical"  # Í≥ÑÏ∏µÏ†Å Íµ¨Ï°∞
    COMPACT = "compact"  # ÏïïÏ∂ïÌòï


class ContextPriority(Enum):
    """Ïª®ÌÖçÏä§Ìä∏ Ïö∞ÏÑ†ÏàúÏúÑ"""

    CRITICAL = "critical"  # ÌïÑÏàò Ï†ïÎ≥¥
    HIGH = "high"  # ÎÜíÏùÄ Í¥ÄÎ†®ÏÑ±
    MEDIUM = "medium"  # Ï§ëÍ∞Ñ Í¥ÄÎ†®ÏÑ±
    LOW = "low"  # ÎÇÆÏùÄ Í¥ÄÎ†®ÏÑ±
    SUPPLEMENTARY = "supplementary"  # Î≥¥Ï°∞ Ï†ïÎ≥¥


@dataclass
class SerializationConfig:
    """ÏßÅÎ†¨Ìôî ÏÑ§Ï†ï"""

    # Ï∂úÎ†• ÌòïÌÉú
    format_style: SerializationFormat = SerializationFormat.STRUCTURED
    language: str = "mixed"  # "ko", "en", "mixed"

    # ÌÜ†ÌÅ∞ Ï†úÌïú
    max_tokens: int = 8000
    max_nodes_detail: int = 50  # ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìï† ÏµúÎåÄ ÎÖ∏Îìú Ïàò
    max_edges_detail: int = 100  # ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìï† ÏµúÎåÄ Ïó£ÏßÄ Ïàò

    # ÎÇ¥Ïö© Ï†úÏñ¥
    include_metadata: bool = True
    include_statistics: bool = True
    include_relationships: bool = True
    include_node_details: bool = True

    # Ïö∞ÏÑ†ÏàúÏúÑ Ï†úÏñ¥
    priority_threshold: float = 0.5  # Ìè¨Ìï®Ìï† ÏµúÏÜå Í¥ÄÎ†®ÏÑ± Ï†êÏàò
    summarize_low_priority: bool = True

    # Ïñ∏Ïñ¥Î≥Ñ ÏÑ§Ï†ï
    use_english_terms: bool = True  # Ï†ÑÎ¨∏Ïö©Ïñ¥Îäî ÏòÅÏñ¥ ÏÇ¨Ïö©
    add_translations: bool = False  # Î≤àÏó≠ Ï∂îÍ∞Ä

    # ÏïïÏ∂ï ÏÑ§Ï†ï
    compress_similar_nodes: bool = True
    max_items_per_section: int = 20


@dataclass
class SerializedContext:
    """ÏßÅÎ†¨ÌôîÎêú Ïª®ÌÖçÏä§Ìä∏ Í≤∞Í≥º"""

    # Î©îÏù∏ ÌÖçÏä§Ìä∏
    main_text: str

    # ÏÑπÏÖòÎ≥Ñ ÎÇ¥Ïö©
    sections: Dict[str, str]

    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    query: str
    total_tokens: int
    included_nodes: int
    included_edges: int
    compression_ratio: float

    # Ïö∞ÏÑ†ÏàúÏúÑÎ≥Ñ ÌÜµÍ≥Ñ
    priority_distribution: Dict[str, int]

    # Ïñ∏Ïñ¥ Ï†ïÎ≥¥
    language: str
    mixed_language_detected: bool

    def to_dict(self) -> Dict[str, Any]:
        """ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò"""
        return {
            "main_text": self.main_text,
            "sections": self.sections,
            "query": self.query,
            "total_tokens": self.total_tokens,
            "included_nodes": self.included_nodes,
            "included_edges": self.included_edges,
            "compression_ratio": self.compression_ratio,
            "priority_distribution": self.priority_distribution,
            "language": self.language,
            "mixed_language_detected": self.mixed_language_detected,
        }


class ContextSerializer:
    """Ïª®ÌÖçÏä§Ìä∏ ÏßÅÎ†¨ÌôîÍ∏∞"""

    def __init__(self, config: Optional[SerializationConfig] = None):
        """
        Args:
            config: ÏßÅÎ†¨Ìôî ÏÑ§Ï†ï
        """
        self.config = config or SerializationConfig()

        # Ïñ∏Ïñ¥Î≥Ñ ÌÖúÌîåÎ¶ø
        self._load_language_templates()

        # ÌÜ†ÌÅ∞ Ï∂îÏ†ïÍ∏∞ (ÎåÄÎûµÏ†Å)
        self._avg_chars_per_token = 4  # ÌïúÍµ≠Ïñ¥+ÏòÅÏñ¥ ÌòºÏö© Í∏∞Ï§Ä

        logger.info("‚úÖ ContextSerializer initialized")
        logger.info(f"   üìè Max tokens: {self.config.max_tokens}")
        logger.info(f"   üåê Language: {self.config.language}")
        logger.info(f"   üìã Format: {self.config.format_style.value}")

    def _load_language_templates(self) -> None:
        """Ïñ∏Ïñ¥Î≥Ñ ÌÖúÌîåÎ¶ø Î°úÎìú"""
        self.templates = {
            "ko": {
                "section_headers": {
                    "overview": "## üìä Í∞úÏöî",
                    "papers": "## üìÑ Í¥ÄÎ†® ÎÖºÎ¨∏Îì§",
                    "authors": "## üë• Ïó∞Íµ¨ÏûêÎì§",
                    "keywords": "## üî§ Ï£ºÏöî ÌÇ§ÏõåÎìú",
                    "journals": "## üì∞ Ï†ÄÎÑê/ÌïôÌöå",
                    "relationships": "## üîó Í¥ÄÍ≥Ñ Ï†ïÎ≥¥",
                    "statistics": "## üìà ÌÜµÍ≥Ñ Ï†ïÎ≥¥",
                },
                "connection_words": {
                    "cited_by": "ÏóêÏÑú Ïù∏Ïö©Îê®",
                    "cites": "ÏùÑ/Î•º Ïù∏Ïö©Ìï®",
                    "collaborated_with": "Í≥º/ÏôÄ ÌòëÏóÖÌï®",
                    "authored_by": "Ïùò Ï†ÄÏûê",
                    "published_in": "Ïóê Î∞úÌëúÎê®",
                    "has_keyword": "ÌÇ§ÏõåÎìú Ìè¨Ìï®",
                    "similar_to": "ÏôÄ/Í≥º Ïú†ÏÇ¨Ìï®",
                },
                "summary_phrases": {
                    "total_papers": "Ï¥ù {} Ìé∏Ïùò ÎÖºÎ¨∏",
                    "total_authors": "Ï¥ù {} Î™ÖÏùò Ïó∞Íµ¨Ïûê",
                    "year_range": "{}ÎÖÑ ~ {}ÎÖÑ Í∏∞Í∞Ñ",
                    "main_topics": "Ï£ºÏöî Ï£ºÏ†ú: {}",
                    "high_relevance": "ÎÜíÏùÄ Í¥ÄÎ†®ÏÑ±",
                    "medium_relevance": "Ï§ëÍ∞Ñ Í¥ÄÎ†®ÏÑ±",
                    "additional_info": "Ï∂îÍ∞Ä Ï†ïÎ≥¥",
                },
            },
            "en": {
                "section_headers": {
                    "overview": "## üìä Overview",
                    "papers": "## üìÑ Related Papers",
                    "authors": "## üë• Researchers",
                    "keywords": "## üî§ Key Terms",
                    "journals": "## üì∞ Journals/Venues",
                    "relationships": "## üîó Relationships",
                    "statistics": "## üìà Statistics",
                },
                "connection_words": {
                    "cited_by": "cited by",
                    "cites": "cites",
                    "collaborated_with": "collaborated with",
                    "authored_by": "authored by",
                    "published_in": "published in",
                    "has_keyword": "includes keyword",
                    "similar_to": "similar to",
                },
                "summary_phrases": {
                    "total_papers": "{} papers total",
                    "total_authors": "{} researchers total",
                    "year_range": "{} - {} period",
                    "main_topics": "Main topics: {}",
                    "high_relevance": "High relevance",
                    "medium_relevance": "Medium relevance",
                    "additional_info": "Additional information",
                },
            },
        }

    def serialize(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult] = None,
        custom_config: Optional[SerializationConfig] = None,
    ) -> SerializedContext:
        """Î©îÏù∏ ÏßÅÎ†¨Ìôî Ìï®Ïàò"""

        config = custom_config or self.config

        logger.info(
            f"üìù Serializing subgraph with {subgraph_result.total_nodes} nodes..."
        )

        # Ïñ∏Ïñ¥ Í∞êÏßÄ
        detected_language = self._detect_content_language(
            subgraph_result, query_analysis
        )

        # ÎÖ∏Îìú/Ïó£ÏßÄ Ïö∞ÏÑ†ÏàúÏúÑ Í≥ÑÏÇ∞
        node_priorities = self._calculate_node_priorities(
            subgraph_result, query_analysis, config
        )
        edge_priorities = self._calculate_edge_priorities(
            subgraph_result, query_analysis, config
        )

        # ÏÑπÏÖòÎ≥Ñ ÎÇ¥Ïö© ÏÉùÏÑ±
        sections = self._generate_sections(
            subgraph_result,
            query_analysis,
            node_priorities,
            edge_priorities,
            config,
            detected_language,
        )

        # Î©îÏù∏ ÌÖçÏä§Ìä∏ Ï°∞Ìï©
        main_text = self._assemble_main_text(sections, config, detected_language)

        # ÌÜ†ÌÅ∞ Ïàò Ï∂îÏ†ï Î∞è ÏïïÏ∂ï (ÌïÑÏöîÏãú)
        estimated_tokens = self._estimate_tokens(main_text)
        if estimated_tokens > config.max_tokens:
            main_text, sections = self._compress_content(
                main_text, sections, config, target_tokens=config.max_tokens
            )
            estimated_tokens = self._estimate_tokens(main_text)

        # ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
        included_nodes = len(
            [
                nid
                for nid, priority in node_priorities.items()
                if priority != ContextPriority.SUPPLEMENTARY
            ]
        )
        included_edges = len(
            [
                eid
                for eid, priority in edge_priorities.items()
                if priority != ContextPriority.SUPPLEMENTARY
            ]
        )

        compression_ratio = min(1.0, estimated_tokens / max(1, config.max_tokens))

        priority_dist = Counter([p.value for p in node_priorities.values()])

        # Í≤∞Í≥º ÏÉùÏÑ±
        result = SerializedContext(
            main_text=main_text,
            sections=sections,
            query=subgraph_result.query,
            total_tokens=estimated_tokens,
            included_nodes=included_nodes,
            included_edges=included_edges,
            compression_ratio=compression_ratio,
            priority_distribution=dict(priority_dist),
            language=detected_language,
            mixed_language_detected=self._is_mixed_language(main_text),
        )

        logger.info(f"‚úÖ Serialization completed:")
        logger.info(f"   üìè Tokens: {estimated_tokens:,}")
        logger.info(f"   üìÑ Nodes: {included_nodes}/{subgraph_result.total_nodes}")
        logger.info(f"   üîó Edges: {included_edges}/{subgraph_result.total_edges}")
        logger.info(f"   üóúÔ∏è Compression: {compression_ratio:.1%}")

        return result

    def _detect_content_language(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
    ) -> str:
        """Ïª®ÌÖêÏ∏† Ïñ∏Ïñ¥ Í∞êÏßÄ"""

        # ÏøºÎ¶¨ Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏûàÏúºÎ©¥ Ïö∞ÏÑ† ÏÇ¨Ïö©
        if query_analysis and query_analysis.language:
            return query_analysis.language

        # ÎÖ∏Îìú ÌÖçÏä§Ìä∏ÏóêÏÑú Ïñ∏Ïñ¥ Í∞êÏßÄ
        korean_chars = 0
        english_chars = 0

        # ÏÉòÌîå ÎÖ∏ÎìúÎì§Ïùò ÌÖçÏä§Ìä∏ Î∂ÑÏÑù
        sample_texts = []
        for node_id, node_data in list(subgraph_result.nodes.items())[:20]:
            title = node_data.get("title", "")
            if title:
                sample_texts.append(title)

        for text in sample_texts:
            korean_chars += len(re.findall(r"[Í∞Ä-Ìû£]", text))
            english_chars += len(re.findall(r"[a-zA-Z]", text))

        total_chars = korean_chars + english_chars
        if total_chars == 0:
            return self.config.language

        korean_ratio = korean_chars / total_chars

        if korean_ratio > 0.6:
            return "ko"
        elif korean_ratio < 0.3:
            return "en"
        else:
            return "mixed"

    def _calculate_node_priorities(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        config: SerializationConfig,
    ) -> Dict[str, ContextPriority]:
        """ÎÖ∏ÎìúÎ≥Ñ Ïö∞ÏÑ†ÏàúÏúÑ Í≥ÑÏÇ∞"""

        priorities = {}

        for node_id, node_data in subgraph_result.nodes.items():
            # Í∏∞Î≥∏ Í¥ÄÎ†®ÏÑ± Ï†êÏàò
            relevance_score = subgraph_result.relevance_scores.get(node_id, 0.0)

            # Ï¥àÍ∏∞ Í≤ÄÏÉâ Îß§Ïπò Î≥¥ÎÑàÏä§
            is_initial_match = any(
                result.node_id == node_id for result in subgraph_result.initial_matches
            )
            if is_initial_match:
                relevance_score += 0.3

            # ÎÖ∏Îìú ÌÉÄÏûÖÎ≥Ñ Ï§ëÏöîÎèÑ
            node_type = node_data.get("node_type", "unknown")
            type_bonus = {
                "paper": 0.2,
                "author": 0.15,
                "keyword": 0.1,
                "journal": 0.05,
            }.get(node_type, 0.0)
            relevance_score += type_bonus

            # ÏøºÎ¶¨ Î∂ÑÏÑù Í∏∞Î∞ò Î≥¥ÎÑàÏä§
            if query_analysis:
                if node_type in [nt.value for nt in query_analysis.required_node_types]:
                    relevance_score += 0.15

            # Ïö∞ÏÑ†ÏàúÏúÑ Í≤∞Ï†ï
            if relevance_score >= 0.8:
                priorities[node_id] = ContextPriority.CRITICAL
            elif relevance_score >= 0.6:
                priorities[node_id] = ContextPriority.HIGH
            elif relevance_score >= 0.4:
                priorities[node_id] = ContextPriority.MEDIUM
            elif relevance_score >= config.priority_threshold:
                priorities[node_id] = ContextPriority.LOW
            else:
                priorities[node_id] = ContextPriority.SUPPLEMENTARY

        return priorities

    def _calculate_edge_priorities(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        config: SerializationConfig,
    ) -> Dict[str, ContextPriority]:
        """Ïó£ÏßÄÎ≥Ñ Ïö∞ÏÑ†ÏàúÏúÑ Í≥ÑÏÇ∞"""

        priorities = {}

        for i, edge in enumerate(subgraph_result.edges):
            edge_id = f"edge_{i}"
            source = edge["source"]
            target = edge["target"]
            edge_type = edge.get("edge_type", "unknown")

            # ÏÜåÏä§/ÌÉÄÍ≤ü ÎÖ∏ÎìúÏùò Í¥ÄÎ†®ÏÑ±
            source_relevance = subgraph_result.relevance_scores.get(source, 0.0)
            target_relevance = subgraph_result.relevance_scores.get(target, 0.0)
            avg_relevance = (source_relevance + target_relevance) / 2

            # Ïó£ÏßÄ ÌÉÄÏûÖÎ≥Ñ Ï§ëÏöîÎèÑ
            edge_importance = {
                "cites": 0.2,
                "authored_by": 0.15,
                "published_in": 0.1,
                "collaborates_with": 0.15,
                "has_keyword": 0.05,
                "similar_to": 0.1,
            }.get(edge_type, 0.05)

            # Ï¢ÖÌï© Ï†êÏàò
            edge_score = avg_relevance + edge_importance

            # Ïö∞ÏÑ†ÏàúÏúÑ Í≤∞Ï†ï
            if edge_score >= 0.7:
                priorities[edge_id] = ContextPriority.CRITICAL
            elif edge_score >= 0.5:
                priorities[edge_id] = ContextPriority.HIGH
            elif edge_score >= 0.3:
                priorities[edge_id] = ContextPriority.MEDIUM
            else:
                priorities[edge_id] = ContextPriority.LOW

        return priorities

    def _generate_sections(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        node_priorities: Dict[str, ContextPriority],
        edge_priorities: Dict[str, ContextPriority],
        config: SerializationConfig,
        language: str,
    ) -> Dict[str, str]:
        """ÏÑπÏÖòÎ≥Ñ ÎÇ¥Ïö© ÏÉùÏÑ±"""

        sections = {}
        templates = self.templates.get(language, self.templates["en"])

        # 1. Í∞úÏöî ÏÑπÏÖò
        sections["overview"] = self._generate_overview_section(
            subgraph_result, query_analysis, templates, config
        )

        # 2. ÎÖ∏Îìú ÌÉÄÏûÖÎ≥Ñ ÏÑπÏÖò
        if config.include_node_details:
            # ÎÖºÎ¨∏ ÏÑπÏÖò
            paper_nodes = {
                nid: data
                for nid, data in subgraph_result.nodes.items()
                if data.get("node_type") == "paper"
                and node_priorities.get(nid, ContextPriority.SUPPLEMENTARY)
                != ContextPriority.SUPPLEMENTARY
            }

            if paper_nodes:
                sections["papers"] = self._generate_papers_section(
                    paper_nodes, node_priorities, templates, config
                )

            # Ï†ÄÏûê ÏÑπÏÖò
            author_nodes = {
                nid: data
                for nid, data in subgraph_result.nodes.items()
                if data.get("node_type") == "author"
                and node_priorities.get(nid, ContextPriority.SUPPLEMENTARY)
                != ContextPriority.SUPPLEMENTARY
            }

            if author_nodes:
                sections["authors"] = self._generate_authors_section(
                    author_nodes, node_priorities, templates, config
                )

            # ÌÇ§ÏõåÎìú ÏÑπÏÖò
            keyword_nodes = {
                nid: data
                for nid, data in subgraph_result.nodes.items()
                if data.get("node_type") == "keyword"
                and node_priorities.get(nid, ContextPriority.SUPPLEMENTARY)
                != ContextPriority.SUPPLEMENTARY
            }

            if keyword_nodes:
                sections["keywords"] = self._generate_keywords_section(
                    keyword_nodes, node_priorities, templates, config
                )

        # 3. Í¥ÄÍ≥Ñ ÏÑπÏÖò
        if config.include_relationships:
            high_priority_edges = [
                edge
                for i, edge in enumerate(subgraph_result.edges)
                if edge_priorities.get(f"edge_{i}", ContextPriority.LOW)
                in [ContextPriority.CRITICAL, ContextPriority.HIGH]
            ]

            if high_priority_edges:
                sections["relationships"] = self._generate_relationships_section(
                    high_priority_edges, subgraph_result.nodes, templates, config
                )

        # 4. ÌÜµÍ≥Ñ ÏÑπÏÖò
        if config.include_statistics:
            sections["statistics"] = self._generate_statistics_section(
                subgraph_result, templates, config
            )

        return sections

    def _generate_overview_section(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """Í∞úÏöî ÏÑπÏÖò ÏÉùÏÑ±"""

        lines = [templates["section_headers"]["overview"]]

        # ÏøºÎ¶¨ Ï†ïÎ≥¥
        lines.append(f"**Query:** {subgraph_result.query}")

        # Í∏∞Î≥∏ ÌÜµÍ≥Ñ
        summary = templates["summary_phrases"]

        paper_count = len(
            [n for n in subgraph_result.nodes.values() if n.get("node_type") == "paper"]
        )
        author_count = len(
            [
                n
                for n in subgraph_result.nodes.values()
                if n.get("node_type") == "author"
            ]
        )

        if paper_count > 0:
            lines.append(f"- {summary['total_papers'].format(paper_count)}")
        if author_count > 0:
            lines.append(f"- {summary['total_authors'].format(author_count)}")

        # ÎÖÑÎèÑ Î≤îÏúÑ
        years = []
        for node_data in subgraph_result.nodes.values():
            year = node_data.get("year", "")
            if year and str(year).isdigit():
                years.append(int(year))

        if years:
            min_year, max_year = min(years), max(years)
            if min_year != max_year:
                lines.append(f"- {summary['year_range'].format(min_year, max_year)}")

        # Ï£ºÏöî ÌÇ§ÏõåÎìú (ÏÉÅÏúÑ 5Í∞ú)
        keyword_freq = Counter()
        for node_data in subgraph_result.nodes.values():
            if node_data.get("node_type") == "keyword":
                keyword_freq[node_data.get("id", "")] += 1

        if keyword_freq:
            top_keywords = [kw for kw, _ in keyword_freq.most_common(5)]
            lines.append(f"- {summary['main_topics'].format(', '.join(top_keywords))}")

        # Ïã†Î¢∞ÎèÑ Ï†ïÎ≥¥
        confidence = subgraph_result.confidence_score
        confidence_text = (
            "High" if confidence > 0.7 else "Medium" if confidence > 0.4 else "Low"
        )
        lines.append(f"- **Confidence:** {confidence_text} ({confidence:.2f})")

        return "\n".join(lines) + "\n"

    def _generate_papers_section(
        self,
        paper_nodes: Dict[str, Dict[str, Any]],
        node_priorities: Dict[str, ContextPriority],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """ÎÖºÎ¨∏ ÏÑπÏÖò ÏÉùÏÑ±"""

        lines = [templates["section_headers"]["papers"]]

        # Ïö∞ÏÑ†ÏàúÏúÑÎ≥ÑÎ°ú Ï†ïÎ†¨
        sorted_papers = sorted(
            paper_nodes.items(),
            key=lambda x: (
                node_priorities.get(x[0], ContextPriority.LOW).value,
                x[1].get("year", ""),
                x[1].get("title", ""),
            ),
        )

        # Ï†úÌïúÎêú ÏàòÎßå ÏÉÅÏÑ∏ ÌëúÏãú
        detailed_count = min(len(sorted_papers), config.max_nodes_detail)

        for i, (paper_id, paper_data) in enumerate(sorted_papers):
            title = paper_data.get("title", "Unknown Title")
            year = paper_data.get("year", "")
            authors = paper_data.get("authors", [])
            journal = paper_data.get("journal", "")

            priority = node_priorities.get(paper_id, ContextPriority.LOW)

            if i < detailed_count:
                # ÏÉÅÏÑ∏ Ï†ïÎ≥¥
                lines.append(f"\n### üìÑ {title}")
                if year:
                    lines.append(f"- **Year:** {year}")
                if authors:
                    author_list = authors if isinstance(authors, list) else [authors]
                    author_text = ", ".join(author_list[:3])  # ÏµúÎåÄ 3Î™Ö
                    if len(author_list) > 3:
                        author_text += f" (+{len(author_list)-3} others)"
                    lines.append(f"- **Authors:** {author_text}")
                if journal:
                    lines.append(f"- **Journal:** {journal}")
                lines.append(f"- **Relevance:** {priority.value}")
            else:
                # Í∞ÑÎã® ÌëúÏãú
                summary = f"- {title}"
                if year:
                    summary += f" ({year})"
                lines.append(summary)

        # ÏöîÏïΩ Ï†ïÎ≥¥ (ÎßéÏùÄ Í≤ΩÏö∞)
        if len(sorted_papers) > detailed_count:
            remaining = len(sorted_papers) - detailed_count
            lines.append(f"\n*... and {remaining} more papers*")

        return "\n".join(lines) + "\n"

    def _generate_authors_section(
        self,
        author_nodes: Dict[str, Dict[str, Any]],
        node_priorities: Dict[str, ContextPriority],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """Ï†ÄÏûê ÏÑπÏÖò ÏÉùÏÑ±"""

        lines = [templates["section_headers"]["authors"]]

        # Ïö∞ÏÑ†ÏàúÏúÑÎ≥ÑÎ°ú Ï†ïÎ†¨
        sorted_authors = sorted(
            author_nodes.items(),
            key=lambda x: (
                node_priorities.get(x[0], ContextPriority.LOW).value,
                x[1].get("paper_count", 0),
            ),
            reverse=True,
        )

        for i, (author_id, author_data) in enumerate(
            sorted_authors[: config.max_nodes_detail]
        ):
            name = author_data.get("name", author_id)
            paper_count = author_data.get("paper_count", 0)
            productivity_type = author_data.get("productivity_type", "")

            lines.append(f"\n### üë§ {name}")
            if paper_count:
                lines.append(f"- **Papers:** {paper_count}")
            if productivity_type:
                lines.append(f"- **Type:** {productivity_type}")

            # Ï£ºÏöî ÌÇ§ÏõåÎìú (ÏûàÎã§Î©¥)
            top_keywords = author_data.get("top_keywords", [])
            if top_keywords:
                if isinstance(top_keywords[0], (list, tuple)):
                    keyword_names = [kw[0] for kw in top_keywords[:3]]
                else:
                    keyword_names = top_keywords[:3]
                lines.append(f"- **Research Areas:** {', '.join(keyword_names)}")

        return "\n".join(lines) + "\n"

    def _generate_keywords_section(
        self,
        keyword_nodes: Dict[str, Dict[str, Any]],
        node_priorities: Dict[str, ContextPriority],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """ÌÇ§ÏõåÎìú ÏÑπÏÖò ÏÉùÏÑ±"""

        lines = [templates["section_headers"]["keywords"]]

        # ÎπàÎèÑÎ≥ÑÎ°ú Ï†ïÎ†¨
        sorted_keywords = sorted(
            keyword_nodes.items(),
            key=lambda x: (
                node_priorities.get(x[0], ContextPriority.LOW).value,
                x[1].get("frequency", 0),
            ),
            reverse=True,
        )

        # Í∑∏Î£πÎ≥ÑÎ°ú ÌëúÏãú
        critical_keywords = []
        high_keywords = []
        other_keywords = []

        for keyword_id, keyword_data in sorted_keywords:
            keyword = keyword_data.get("name", keyword_id)
            frequency = keyword_data.get("frequency", 0)
            priority = node_priorities.get(keyword_id, ContextPriority.LOW)

            keyword_info = f"{keyword}"
            if frequency:
                keyword_info += f" ({frequency})"

            if priority == ContextPriority.CRITICAL:
                critical_keywords.append(keyword_info)
            elif priority == ContextPriority.HIGH:
                high_keywords.append(keyword_info)
            else:
                other_keywords.append(keyword_info)

        # Ïö∞ÏÑ†ÏàúÏúÑÎ≥Ñ Ï∂úÎ†•
        if critical_keywords:
            lines.append(f"\n**{templates['summary_phrases']['high_relevance']}:**")
            lines.append(f"{', '.join(critical_keywords[:10])}")

        if high_keywords:
            lines.append(f"\n**{templates['summary_phrases']['medium_relevance']}:**")
            lines.append(f"{', '.join(high_keywords[:15])}")

        if other_keywords and not config.summarize_low_priority:
            lines.append(f"\n**{templates['summary_phrases']['additional_info']}:**")
            lines.append(f"{', '.join(other_keywords[:10])}")

        return "\n".join(lines) + "\n"

    def _generate_relationships_section(
        self,
        edges: List[Dict[str, Any]],
        nodes: Dict[str, Dict[str, Any]],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """Í¥ÄÍ≥Ñ ÏÑπÏÖò ÏÉùÏÑ±"""

        lines = [templates["section_headers"]["relationships"]]

        # Ïó£ÏßÄ ÌÉÄÏûÖÎ≥Ñ Í∑∏Î£πÌôî
        edge_groups = defaultdict(list)
        for edge in edges:
            edge_type = edge.get("edge_type", "related_to")
            edge_groups[edge_type].append(edge)

        connection_words = templates["connection_words"]

        for edge_type, type_edges in edge_groups.items():
            if len(type_edges) == 0:
                continue

            connection_word = connection_words.get(edge_type, edge_type)
            lines.append(f"\n**{connection_word.title()} ({len(type_edges)}):**")

            # ÏÉòÌîå Í¥ÄÍ≥ÑÎì§ ÌëúÏãú
            sample_size = min(5, len(type_edges))
            for edge in type_edges[:sample_size]:
                source_id = edge["source"]
                target_id = edge["target"]

                source_data = nodes.get(source_id, {})
                target_data = nodes.get(target_id, {})

                source_name = self._get_node_display_name(source_data)
                target_name = self._get_node_display_name(target_data)

                lines.append(f"- {source_name} ‚Üí {target_name}")

            # Îçî ÏûàÏúºÎ©¥ ÏöîÏïΩ
            if len(type_edges) > sample_size:
                lines.append(f"- *... and {len(type_edges) - sample_size} more*")

        return "\n".join(lines) + "\n"

    def _generate_statistics_section(
        self,
        subgraph_result: SubgraphResult,
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """ÌÜµÍ≥Ñ ÏÑπÏÖò ÏÉùÏÑ±"""

        lines = [templates["section_headers"]["statistics"]]

        # Í∏∞Î≥∏ ÌÜµÍ≥Ñ
        lines.append(f"- **Total Nodes:** {subgraph_result.total_nodes}")
        lines.append(f"- **Total Edges:** {subgraph_result.total_edges}")
        lines.append(
            f"- **Extraction Strategy:** {subgraph_result.extraction_strategy.value}"
        )
        lines.append(f"- **Processing Time:** {subgraph_result.extraction_time:.2f}s")

        # ÎÖ∏Îìú ÌÉÄÏûÖÎ≥Ñ Î∂ÑÌè¨
        if subgraph_result.nodes_by_type:
            lines.append(f"\n**Node Distribution:**")
            for node_type, count in subgraph_result.nodes_by_type.items():
                lines.append(f"- {node_type}: {count}")

        # ÏÉÅÏúÑ Í¥ÄÎ†® ÎÖ∏ÎìúÎì§
        top_relevant = sorted(
            subgraph_result.relevance_scores.items(), key=lambda x: x[1], reverse=True
        )[:5]

        if top_relevant:
            lines.append(f"\n**Most Relevant Nodes:**")
            for node_id, score in top_relevant:
                node_data = subgraph_result.nodes.get(node_id, {})
                name = self._get_node_display_name(node_data)
                lines.append(f"- {name}: {score:.3f}")

        return "\n".join(lines) + "\n"

    def _get_node_display_name(self, node_data: Dict[str, Any]) -> str:
        """ÎÖ∏Îìú ÌëúÏãúÎ™Ö ÏÉùÏÑ±"""
        node_type = node_data.get("node_type", "unknown")

        if node_type == "paper":
            title = node_data.get("title", "Unknown Paper")
            return title[:50] + "..." if len(title) > 50 else title
        elif node_type == "author":
            return node_data.get("name", node_data.get("id", "Unknown Author"))
        elif node_type == "keyword":
            return node_data.get("name", node_data.get("id", "Unknown Keyword"))
        elif node_type == "journal":
            return node_data.get("name", node_data.get("id", "Unknown Journal"))
        else:
            return node_data.get("name", node_data.get("id", "Unknown"))

    def _assemble_main_text(
        self, sections: Dict[str, str], config: SerializationConfig, language: str
    ) -> str:
        """ÏÑπÏÖòÎì§ÏùÑ Î©îÏù∏ ÌÖçÏä§Ìä∏Î°ú Ï°∞Ìï©"""

        if config.format_style == SerializationFormat.STRUCTURED:
            # Íµ¨Ï°∞ÌôîÎêú ÌòïÌÉú
            section_order = [
                "overview",
                "papers",
                "authors",
                "keywords",
                "relationships",
                "statistics",
            ]
            text_parts = []

            for section_name in section_order:
                if section_name in sections and sections[section_name].strip():
                    text_parts.append(sections[section_name])

            return "\n".join(text_parts)

        elif config.format_style == SerializationFormat.NARRATIVE:
            # ÏûêÏó∞Ïñ¥ ÏÑúÏà†Ìòï
            return self._create_narrative_text(sections, language)

        elif config.format_style == SerializationFormat.COMPACT:
            # ÏïïÏ∂ïÌòï
            return self._create_compact_text(sections, config)

        else:
            # Í∏∞Î≥∏Í∞í: STRUCTURED
            return "\n".join(sections.values())

    def _create_narrative_text(self, sections: Dict[str, str], language: str) -> str:
        """ÏûêÏó∞Ïñ¥ ÏÑúÏà†Ìòï ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        # Íµ¨ÌòÑ Í∞ÑÏÜåÌôî - Íµ¨Ï°∞ÌôîÎêú ÌòïÌÉúÎ°ú ÎåÄÏ≤¥
        return "\n".join(sections.values())

    def _create_compact_text(
        self, sections: Dict[str, str], config: SerializationConfig
    ) -> str:
        """ÏïïÏ∂ïÌòï ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        # Ìó§Îçî Ï†úÍ±∞ Î∞è ÏïïÏ∂ï
        compact_lines = []

        for section_text in sections.values():
            lines = section_text.split("\n")
            # Ìó§ÎçîÏôÄ Îπà Ï§Ñ Ï†úÍ±∞
            content_lines = [
                line for line in lines if line.strip() and not line.startswith("#")
            ]
            compact_lines.extend(content_lines[:5])  # Í∞Å ÏÑπÏÖòÏóêÏÑú ÏµúÎåÄ 5Ï§ÑÎßå

        return "\n".join(compact_lines)

    def _estimate_tokens(self, text: str) -> int:
        """ÌÜ†ÌÅ∞ Ïàò Ï∂îÏ†ï (ÎåÄÎûµÏ†Å)"""
        # ÌïúÍµ≠Ïñ¥/ÏòÅÏñ¥ ÌòºÏö© ÌÖçÏä§Ìä∏Ïóê ÎåÄÌïú ÎåÄÎûµÏ†Å Ï∂îÏ†ï
        char_count = len(text)
        return int(char_count / self._avg_chars_per_token)

    def _compress_content(
        self,
        main_text: str,
        sections: Dict[str, str],
        config: SerializationConfig,
        target_tokens: int,
    ) -> Tuple[str, Dict[str, str]]:
        """Ïª®ÌÖêÏ∏† ÏïïÏ∂ï"""

        logger.info(f"üóúÔ∏è Compressing content to fit {target_tokens} tokens...")

        # Ïö∞ÏÑ†ÏàúÏúÑ ÏàúÏÑú (Ï§ëÏöîÎèÑ Ïàú)
        section_priorities = [
            "overview",
            "papers",
            "authors",
            "relationships",
            "keywords",
            "statistics",
        ]

        compressed_sections = {}
        current_tokens = 0

        for section_name in section_priorities:
            if section_name not in sections:
                continue

            section_text = sections[section_name]
            section_tokens = self._estimate_tokens(section_text)

            if current_tokens + section_tokens <= target_tokens:
                # Ï†ÑÏ≤¥ Ìè¨Ìï®
                compressed_sections[section_name] = section_text
                current_tokens += section_tokens
            else:
                # Î∂ÄÎ∂Ñ Ìè¨Ìï®
                remaining_tokens = target_tokens - current_tokens
                if remaining_tokens > 100:  # ÏµúÏÜå 100 ÌÜ†ÌÅ∞ÏùÄ ÏûàÏñ¥Ïïº ÏùòÎØ∏Í∞Ä ÏûàÏùå
                    # ÏÑπÏÖòÏùÑ ÏïïÏ∂ïÌï¥ÏÑú Ìè¨Ìï®
                    compressed_text = self._compress_section(
                        section_text, remaining_tokens
                    )
                    compressed_sections[section_name] = compressed_text
                break

        # ÏïïÏ∂ïÎêú Î©îÏù∏ ÌÖçÏä§Ìä∏ Ïû¨Ï°∞Ìï©
        compressed_main_text = self._assemble_main_text(
            compressed_sections, config, self.config.language
        )

        return compressed_main_text, compressed_sections

    def _compress_section(self, section_text: str, target_tokens: int) -> str:
        """Í∞úÎ≥Ñ ÏÑπÏÖò ÏïïÏ∂ï"""
        lines = section_text.split("\n")

        # Ï§ëÏöîÌïú Ï§ÑÎì§ Ïö∞ÏÑ† ÏÑ†ÌÉù (Ìó§Îçî, ÏöîÏïΩ Ï†ïÎ≥¥ Îì±)
        important_lines = []
        detail_lines = []

        for line in lines:
            if (
                line.startswith("#")
                or line.startswith("**")
                or "total" in line.lower()
                or "relevance" in line.lower()
            ):
                important_lines.append(line)
            else:
                detail_lines.append(line)

        # Ï§ëÏöîÌïú Ï§ÑÎì§ÏùÑ Î®ºÏ†Ä Ìè¨Ìï®
        result_lines = important_lines[:]
        current_tokens = self._estimate_tokens("\n".join(result_lines))

        # ÎÇ®ÏùÄ ÌÜ†ÌÅ∞ÏúºÎ°ú ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        for line in detail_lines:
            line_tokens = self._estimate_tokens(line)
            if current_tokens + line_tokens <= target_tokens:
                result_lines.append(line)
                current_tokens += line_tokens
            else:
                break

        # ÏûòÎ†∏Îã§Îäî ÌëúÏãú Ï∂îÍ∞Ä
        if len(result_lines) < len(lines):
            result_lines.append("*[Content truncated due to length limits]*")

        return "\n".join(result_lines)

    def _is_mixed_language(self, text: str) -> bool:
        """ÌòºÏö© Ïñ∏Ïñ¥ Ïó¨Î∂Ä ÌôïÏù∏"""
        korean_chars = len(re.findall(r"[Í∞Ä-Ìû£]", text))
        english_chars = len(re.findall(r"[a-zA-Z]", text))

        total_chars = korean_chars + english_chars
        if total_chars == 0:
            return False

        korean_ratio = korean_chars / total_chars
        return 0.1 < korean_ratio < 0.9  # 10%-90% ÏÇ¨Ïù¥Î©¥ ÌòºÏö©ÏúºÎ°ú ÌåêÎã®


def main():
    """ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
    print("üß™ Testing ContextSerializer...")

    # ÌÖåÏä§Ìä∏Ïö© ÎçîÎØ∏ SubgraphResult ÏÉùÏÑ±
    from dataclasses import dataclass
    from typing import Dict, List, Any

    # ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
    test_nodes = {
        "paper_1": {
            "id": "paper_1",
            "node_type": "paper",
            "title": "Deep Learning for Battery State of Charge Prediction using LSTM Networks",
            "year": "2023",
            "authors": ["ÍπÄÏ≤†Ïàò", "John Smith"],
            "journal": "IEEE Transactions on Power Electronics",
        },
        "author_1": {
            "id": "author_1",
            "node_type": "author",
            "name": "ÍπÄÏ≤†Ïàò",
            "paper_count": 15,
            "productivity_type": "Leading Researcher",
            "top_keywords": [("machine learning", 8), ("battery", 6)],
        },
        "keyword_1": {
            "id": "keyword_1",
            "node_type": "keyword",
            "name": "machine learning",
            "frequency": 25,
        },
    }

    test_edges = [
        {"source": "paper_1", "target": "author_1", "edge_type": "authored_by"},
        {"source": "paper_1", "target": "keyword_1", "edge_type": "has_keyword"},
    ]

    # ÎçîÎØ∏ SearchResult
    from collections import namedtuple

    SearchResult = namedtuple("SearchResult", ["node_id", "similarity_score"])

    test_initial_matches = [
        SearchResult("paper_1", 0.85),
        SearchResult("keyword_1", 0.75),
    ]

    # ÎçîÎØ∏ SubgraphResult
    from enum import Enum

    class SearchStrategy(Enum):
        HYBRID = "hybrid"

    class DummySubgraphResult:
        def __init__(self):
            self.nodes = test_nodes
            self.edges = test_edges
            self.query = "Î∞∞ÌÑ∞Î¶¨ SoC ÏòàÏ∏°Ïóê ÏÇ¨Ïö©Îêú Î®∏Ïã†Îü¨Îãù Í∏∞Î≤ïÎì§ÏùÄ?"
            self.query_analysis = None
            self.extraction_strategy = SearchStrategy.HYBRID
            self.total_nodes = len(test_nodes)
            self.total_edges = len(test_edges)
            self.nodes_by_type = {"paper": 1, "author": 1, "keyword": 1}
            self.extraction_time = 1.5
            self.initial_matches = test_initial_matches
            self.expansion_path = []
            self.relevance_scores = {"paper_1": 0.9, "author_1": 0.7, "keyword_1": 0.8}
            self.confidence_score = 0.85

    # ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    test_result = DummySubgraphResult()

    # ContextSerializer Ï¥àÍ∏∞Ìôî
    config = SerializationConfig(
        format_style=SerializationFormat.STRUCTURED, max_tokens=2000, language="mixed"
    )

    serializer = ContextSerializer(config)

    # ÏßÅÎ†¨Ìôî ÏàòÌñâ
    serialized = serializer.serialize(test_result)

    print(f"‚úÖ Serialization completed:")
    print(f"   üìè Tokens: {serialized.total_tokens}")
    print(f"   üåê Language: {serialized.language}")
    print(f"   üìä Compression: {serialized.compression_ratio:.1%}")

    print(f"\nüìù Generated Text:")
    print("=" * 60)
    print(serialized.main_text)
    print("=" * 60)

    print(f"\n‚úÖ ContextSerializer test completed!")


if __name__ == "__main__":
    main()
