"""
ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™” ëª¨ë“ˆ
Context Serializer for GraphRAG System

ì„œë¸Œê·¸ë˜í”„ë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ í…ìŠ¤íŠ¸ êµ¬ì¡°
- í† í° ì œí•œ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ ì••ì¶•
- ê´€ë ¨ì„± ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì •ë ¬
- í•œêµ­ì–´/ì˜ì–´ í˜¼ìš© ì§€ì›
"""

import re
import json
import logging
from enum import Enum
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from collections import defaultdict, Counter
import numpy as np

try:
    from .subgraph_extractor import SubgraphResult
    from ..query_analyzer import QueryAnalysisResult, QueryType, QueryComplexity
except ImportError as e:
    import warnings

    warnings.warn(f"Some GraphRAG components not available: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class SerializationFormat(Enum):
    """ì§ë ¬í™” í˜•íƒœ"""

    STRUCTURED = "structured"  # êµ¬ì¡°í™”ëœ ì„¹ì…˜ë³„
    NARRATIVE = "narrative"  # ìì—°ì–´ ì„œìˆ í˜•
    LIST_BASED = "list_based"  # ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜
    HIERARCHICAL = "hierarchical"  # ê³„ì¸µì  êµ¬ì¡°
    COMPACT = "compact"  # ì••ì¶•í˜•


class ContextPriority(Enum):
    """ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„"""

    CRITICAL = "critical"  # í•„ìˆ˜ ì •ë³´
    HIGH = "high"  # ë†’ì€ ê´€ë ¨ì„±
    MEDIUM = "medium"  # ì¤‘ê°„ ê´€ë ¨ì„±
    LOW = "low"  # ë‚®ì€ ê´€ë ¨ì„±
    SUPPLEMENTARY = "supplementary"  # ë³´ì¡° ì •ë³´


@dataclass
class SerializationConfig:
    """ì§ë ¬í™” ì„¤ì •"""

    # ì¶œë ¥ í˜•íƒœ
    format_style: SerializationFormat = SerializationFormat.STRUCTURED
    language: str = "mixed"  # "ko", "en", "mixed"

    # í† í° ì œí•œ
    max_tokens: int = 8000
    max_nodes_detail: int = 50  # ìƒì„¸ ì •ë³´ë¥¼ í¬í•¨í•  ìµœëŒ€ ë…¸ë“œ ìˆ˜
    max_edges_detail: int = 100  # ìƒì„¸ ì •ë³´ë¥¼ í¬í•¨í•  ìµœëŒ€ ì—£ì§€ ìˆ˜

    # ë‚´ìš© ì œì–´
    include_metadata: bool = True
    include_statistics: bool = True
    include_relationships: bool = True
    include_node_details: bool = True

    # ìš°ì„ ìˆœìœ„ ì œì–´
    priority_threshold: float = 0.5  # í¬í•¨í•  ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜
    summarize_low_priority: bool = True

    # ì–¸ì–´ë³„ ì„¤ì •
    use_english_terms: bool = True  # ì „ë¬¸ìš©ì–´ëŠ” ì˜ì–´ ì‚¬ìš©
    add_translations: bool = False  # ë²ˆì—­ ì¶”ê°€

    # ì••ì¶• ì„¤ì •
    compress_similar_nodes: bool = True
    max_items_per_section: int = 20


@dataclass
class SerializedContext:
    """ì§ë ¬í™”ëœ ì»¨í…ìŠ¤íŠ¸ ê²°ê³¼"""

    # ë©”ì¸ í…ìŠ¤íŠ¸
    main_text: str

    # ì„¹ì…˜ë³„ ë‚´ìš©
    sections: Dict[str, str]

    # ë©”íƒ€ë°ì´í„°
    query: str
    total_tokens: int
    included_nodes: int
    included_edges: int
    compression_ratio: float

    # ìš°ì„ ìˆœìœ„ë³„ í†µê³„
    priority_distribution: Dict[str, int]

    # ì–¸ì–´ ì •ë³´
    language: str
    mixed_language_detected: bool

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "main_text": self.main_text,
            "sections": self.sections,
            "query": self.query,
            "total_tokens": self.total_tokens,
            "included_nodes": self.included_nodes,
            "included_edges": self.included_edges,
            "compression_ratio": self.compression_ratio,
            "priority_distribution": self.priority_distribution,
            "language": self.language,
            "mixed_language_detected": self.mixed_language_detected,
        }


class ContextSerializer:
    """ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”ê¸°"""

    def __init__(self, config: Optional[SerializationConfig] = None):
        """
        Args:
            config: ì§ë ¬í™” ì„¤ì •
        """
        self.config = config or SerializationConfig()

        # ì–¸ì–´ë³„ í…œí”Œë¦¿
        self._load_language_templates()

        # í† í° ì¶”ì •ê¸° (ëŒ€ëµì )
        self._avg_chars_per_token = 4  # í•œêµ­ì–´+ì˜ì–´ í˜¼ìš© ê¸°ì¤€

        logger.info("âœ… ContextSerializer initialized")
        logger.info(f"   ğŸ“ Max tokens: {self.config.max_tokens}")
        logger.info(f"   ğŸŒ Language: {self.config.language}")
        logger.info(f"   ğŸ“‹ Format: {self.config.format_style.value}")

    def _load_language_templates(self) -> None:
        """ì–¸ì–´ë³„ í…œí”Œë¦¿ ë¡œë“œ"""
        self.templates = {
            "ko": {
                "section_headers": {
                    "overview": "## ğŸ“Š ê°œìš”",
                    "papers": "## ğŸ“„ ê´€ë ¨ ë…¼ë¬¸ë“¤",
                    "authors": "## ğŸ‘¥ ì—°êµ¬ìë“¤",
                    "keywords": "## ğŸ”¤ ì£¼ìš” í‚¤ì›Œë“œ",
                    "journals": "## ğŸ“° ì €ë„/í•™íšŒ",
                    "relationships": "## ğŸ”— ê´€ê³„ ì •ë³´",
                    "statistics": "## ğŸ“ˆ í†µê³„ ì •ë³´",
                },
                "connection_words": {
                    "cited_by": "ì—ì„œ ì¸ìš©ë¨",
                    "cites": "ì„/ë¥¼ ì¸ìš©í•¨",
                    "collaborated_with": "ê³¼/ì™€ í˜‘ì—…í•¨",
                    "authored_by": "ì˜ ì €ì",
                    "published_in": "ì— ë°œí‘œë¨",
                    "has_keyword": "í‚¤ì›Œë“œ í¬í•¨",
                    "similar_to": "ì™€/ê³¼ ìœ ì‚¬í•¨",
                },
                "summary_phrases": {
                    "total_papers": "ì´ {} í¸ì˜ ë…¼ë¬¸",
                    "total_authors": "ì´ {} ëª…ì˜ ì—°êµ¬ì",
                    "year_range": "{}ë…„ ~ {}ë…„ ê¸°ê°„",
                    "main_topics": "ì£¼ìš” ì£¼ì œ: {}",
                    "high_relevance": "ë†’ì€ ê´€ë ¨ì„±",
                    "medium_relevance": "ì¤‘ê°„ ê´€ë ¨ì„±",
                    "additional_info": "ì¶”ê°€ ì •ë³´",
                },
            },
            "en": {
                "section_headers": {
                    "overview": "## ğŸ“Š Overview",
                    "papers": "## ğŸ“„ Related Papers",
                    "authors": "## ğŸ‘¥ Researchers",
                    "keywords": "## ğŸ”¤ Key Terms",
                    "journals": "## ğŸ“° Journals/Venues",
                    "relationships": "## ğŸ”— Relationships",
                    "statistics": "## ğŸ“ˆ Statistics",
                },
                "connection_words": {
                    "cited_by": "cited by",
                    "cites": "cites",
                    "collaborated_with": "collaborated with",
                    "authored_by": "authored by",
                    "published_in": "published in",
                    "has_keyword": "includes keyword",
                    "similar_to": "similar to",
                },
                "summary_phrases": {
                    "total_papers": "{} papers total",
                    "total_authors": "{} researchers total",
                    "year_range": "{} - {} period",
                    "main_topics": "Main topics: {}",
                    "high_relevance": "High relevance",
                    "medium_relevance": "Medium relevance",
                    "additional_info": "Additional information",
                },
            },
        }

    def serialize(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult] = None,
        custom_config: Optional[SerializationConfig] = None,
    ) -> SerializedContext:
        """ë©”ì¸ ì§ë ¬í™” í•¨ìˆ˜"""

        config = custom_config or self.config

        logger.info(
            f"ğŸ“ Serializing subgraph with {subgraph_result.total_nodes} nodes..."
        )

        # ì–¸ì–´ ê°ì§€
        detected_language = self._detect_content_language(
            subgraph_result, query_analysis
        )

        # ë…¸ë“œ/ì—£ì§€ ìš°ì„ ìˆœìœ„ ê³„ì‚°
        node_priorities = self._calculate_node_priorities(
            subgraph_result, query_analysis, config
        )
        edge_priorities = self._calculate_edge_priorities(
            subgraph_result, query_analysis, config
        )

        # ì„¹ì…˜ë³„ ë‚´ìš© ìƒì„±
        sections = self._generate_sections(
            subgraph_result,
            query_analysis,
            node_priorities,
            edge_priorities,
            config,
            detected_language,
        )

        # ë©”ì¸ í…ìŠ¤íŠ¸ ì¡°í•©
        main_text = self._assemble_main_text(sections, config, detected_language)

        # í† í° ìˆ˜ ì¶”ì • ë° ì••ì¶• (í•„ìš”ì‹œ)
        estimated_tokens = self._estimate_tokens(main_text)
        if estimated_tokens > config.max_tokens:
            main_text, sections = self._compress_content(
                main_text, sections, config, target_tokens=config.max_tokens
            )
            estimated_tokens = self._estimate_tokens(main_text)

        # í†µê³„ ê³„ì‚°
        included_nodes = len(
            [
                nid
                for nid, priority in node_priorities.items()
                if priority != ContextPriority.SUPPLEMENTARY
            ]
        )
        included_edges = len(
            [
                eid
                for eid, priority in edge_priorities.items()
                if priority != ContextPriority.SUPPLEMENTARY
            ]
        )

        compression_ratio = min(1.0, estimated_tokens / max(1, config.max_tokens))

        priority_dist = Counter([p.value for p in node_priorities.values()])

        # ê²°ê³¼ ìƒì„±
        result = SerializedContext(
            main_text=main_text,
            sections=sections,
            query=subgraph_result.query,
            total_tokens=estimated_tokens,
            included_nodes=included_nodes,
            included_edges=included_edges,
            compression_ratio=compression_ratio,
            priority_distribution=dict(priority_dist),
            language=detected_language,
            mixed_language_detected=self._is_mixed_language(main_text),
        )

        logger.info(f"âœ… Serialization completed:")
        logger.info(f"   ğŸ“ Tokens: {estimated_tokens:,}")
        logger.info(f"   ğŸ“„ Nodes: {included_nodes}/{subgraph_result.total_nodes}")
        logger.info(f"   ğŸ”— Edges: {included_edges}/{subgraph_result.total_edges}")
        logger.info(f"   ğŸ—œï¸ Compression: {compression_ratio:.1%}")

        return result

    def _detect_content_language(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
    ) -> str:
        """ì»¨í…ì¸  ì–¸ì–´ ê°ì§€"""

        # ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if query_analysis and query_analysis.language:
            return query_analysis.language

        # ë…¸ë“œ í…ìŠ¤íŠ¸ì—ì„œ ì–¸ì–´ ê°ì§€
        korean_chars = 0
        english_chars = 0

        # ìƒ˜í”Œ ë…¸ë“œë“¤ì˜ í…ìŠ¤íŠ¸ ë¶„ì„
        sample_texts = []
        for node_id, node_data in list(subgraph_result.nodes.items())[:20]:
            title = node_data.get("title", "")
            if title:
                sample_texts.append(title)

        for text in sample_texts:
            korean_chars += len(re.findall(r"[ê°€-í£]", text))
            english_chars += len(re.findall(r"[a-zA-Z]", text))

        total_chars = korean_chars + english_chars
        if total_chars == 0:
            return self.config.language

        korean_ratio = korean_chars / total_chars

        if korean_ratio > 0.6:
            return "ko"
        elif korean_ratio < 0.3:
            return "en"
        else:
            return "mixed"

    def _calculate_node_priorities(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        config: SerializationConfig,
    ) -> Dict[str, ContextPriority]:
        """ë…¸ë“œë³„ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""

        priorities = {}

        for node_id, node_data in subgraph_result.nodes.items():
            # ê¸°ë³¸ ê´€ë ¨ì„± ì ìˆ˜
            relevance_score = subgraph_result.relevance_scores.get(node_id, 0.0)

            # ì´ˆê¸° ê²€ìƒ‰ ë§¤ì¹˜ ë³´ë„ˆìŠ¤
            is_initial_match = any(
                result.node_id == node_id for result in subgraph_result.initial_matches
            )
            if is_initial_match:
                relevance_score += 0.3

            # ë…¸ë“œ íƒ€ì…ë³„ ì¤‘ìš”ë„
            node_type = node_data.get("node_type", "unknown")
            type_bonus = {
                "paper": 0.2,
                "author": 0.15,
                "keyword": 0.1,
                "journal": 0.05,
            }.get(node_type, 0.0)
            relevance_score += type_bonus

            # ì¿¼ë¦¬ ë¶„ì„ ê¸°ë°˜ ë³´ë„ˆìŠ¤
            if query_analysis:
                if node_type in [nt.value for nt in query_analysis.required_node_types]:
                    relevance_score += 0.15

            # ìš°ì„ ìˆœìœ„ ê²°ì •
            if relevance_score >= 0.8:
                priorities[node_id] = ContextPriority.CRITICAL
            elif relevance_score >= 0.6:
                priorities[node_id] = ContextPriority.HIGH
            elif relevance_score >= 0.4:
                priorities[node_id] = ContextPriority.MEDIUM
            elif relevance_score >= config.priority_threshold:
                priorities[node_id] = ContextPriority.LOW
            else:
                priorities[node_id] = ContextPriority.SUPPLEMENTARY

        return priorities

    def _calculate_edge_priorities(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        config: SerializationConfig,
    ) -> Dict[str, ContextPriority]:
        """ì—£ì§€ë³„ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""

        priorities = {}

        for i, edge in enumerate(subgraph_result.edges):
            edge_id = f"edge_{i}"
            source = edge["source"]
            target = edge["target"]
            edge_type = edge.get("edge_type", "unknown")

            # ì†ŒìŠ¤/íƒ€ê²Ÿ ë…¸ë“œì˜ ê´€ë ¨ì„±
            source_relevance = subgraph_result.relevance_scores.get(source, 0.0)
            target_relevance = subgraph_result.relevance_scores.get(target, 0.0)
            avg_relevance = (source_relevance + target_relevance) / 2

            # ì—£ì§€ íƒ€ì…ë³„ ì¤‘ìš”ë„
            edge_importance = {
                "cites": 0.2,
                "authored_by": 0.15,
                "published_in": 0.1,
                "collaborates_with": 0.15,
                "has_keyword": 0.05,
                "similar_to": 0.1,
            }.get(edge_type, 0.05)

            # ì¢…í•© ì ìˆ˜
            edge_score = avg_relevance + edge_importance

            # ìš°ì„ ìˆœìœ„ ê²°ì •
            if edge_score >= 0.7:
                priorities[edge_id] = ContextPriority.CRITICAL
            elif edge_score >= 0.5:
                priorities[edge_id] = ContextPriority.HIGH
            elif edge_score >= 0.3:
                priorities[edge_id] = ContextPriority.MEDIUM
            else:
                priorities[edge_id] = ContextPriority.LOW

        return priorities

    def _generate_sections(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        node_priorities: Dict[str, ContextPriority],
        edge_priorities: Dict[str, ContextPriority],
        config: SerializationConfig,
        language: str,
    ) -> Dict[str, str]:
        """ì„¹ì…˜ë³„ ë‚´ìš© ìƒì„±"""

        sections = {}
        templates = self.templates.get(language, self.templates["en"])

        # 1. ê°œìš” ì„¹ì…˜
        sections["overview"] = self._generate_overview_section(
            subgraph_result, query_analysis, templates, config
        )

        # 2. ë…¸ë“œ íƒ€ì…ë³„ ì„¹ì…˜
        if config.include_node_details:
            # ë…¼ë¬¸ ì„¹ì…˜
            paper_nodes = {
                nid: data
                for nid, data in subgraph_result.nodes.items()
                if data.get("node_type") == "paper"
                and node_priorities.get(nid, ContextPriority.SUPPLEMENTARY)
                != ContextPriority.SUPPLEMENTARY
            }

            if paper_nodes:
                sections["papers"] = self._generate_papers_section(
                    paper_nodes, node_priorities, templates, config
                )

            # ì €ì ì„¹ì…˜
            author_nodes = {
                nid: data
                for nid, data in subgraph_result.nodes.items()
                if data.get("node_type") == "author"
                and node_priorities.get(nid, ContextPriority.SUPPLEMENTARY)
                != ContextPriority.SUPPLEMENTARY
            }

            if author_nodes:
                sections["authors"] = self._generate_authors_section(
                    author_nodes, node_priorities, templates, config
                )

            # í‚¤ì›Œë“œ ì„¹ì…˜
            keyword_nodes = {
                nid: data
                for nid, data in subgraph_result.nodes.items()
                if data.get("node_type") == "keyword"
                and node_priorities.get(nid, ContextPriority.SUPPLEMENTARY)
                != ContextPriority.SUPPLEMENTARY
            }

            if keyword_nodes:
                sections["keywords"] = self._generate_keywords_section(
                    keyword_nodes, node_priorities, templates, config
                )

        # 3. ê´€ê³„ ì„¹ì…˜
        if config.include_relationships:
            high_priority_edges = [
                edge
                for i, edge in enumerate(subgraph_result.edges)
                if edge_priorities.get(f"edge_{i}", ContextPriority.LOW)
                in [ContextPriority.CRITICAL, ContextPriority.HIGH]
            ]

            if high_priority_edges:
                sections["relationships"] = self._generate_relationships_section(
                    high_priority_edges, subgraph_result.nodes, templates, config
                )

        # 4. í†µê³„ ì„¹ì…˜
        if config.include_statistics:
            sections["statistics"] = self._generate_statistics_section(
                subgraph_result, templates, config
            )

        return sections

    def _generate_overview_section(
        self,
        subgraph_result: SubgraphResult,
        query_analysis: Optional[QueryAnalysisResult],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """ê°œìš” ì„¹ì…˜ ìƒì„±"""

        lines = [templates["section_headers"]["overview"]]

        # ì¿¼ë¦¬ ì •ë³´
        lines.append(f"**Query:** {subgraph_result.query}")

        # ê¸°ë³¸ í†µê³„
        summary = templates["summary_phrases"]

        paper_count = len(
            [n for n in subgraph_result.nodes.values() if n.get("node_type") == "paper"]
        )
        author_count = len(
            [
                n
                for n in subgraph_result.nodes.values()
                if n.get("node_type") == "author"
            ]
        )

        if paper_count > 0:
            lines.append(f"- {summary['total_papers'].format(paper_count)}")
        if author_count > 0:
            lines.append(f"- {summary['total_authors'].format(author_count)}")

        # ë…„ë„ ë²”ìœ„
        years = []
        for node_data in subgraph_result.nodes.values():
            year = node_data.get("year", "")
            if year and str(year).isdigit():
                years.append(int(year))

        if years:
            min_year, max_year = min(years), max(years)
            if min_year != max_year:
                lines.append(f"- {summary['year_range'].format(min_year, max_year)}")

        # ì£¼ìš” í‚¤ì›Œë“œ (ìƒìœ„ 5ê°œ)
        keyword_freq = Counter()
        for node_data in subgraph_result.nodes.values():
            if node_data.get("node_type") == "keyword":
                keyword_freq[node_data.get("id", "")] += 1

        if keyword_freq:
            top_keywords = [kw for kw, _ in keyword_freq.most_common(5)]
            lines.append(f"- {summary['main_topics'].format(', '.join(top_keywords))}")

        # ì‹ ë¢°ë„ ì •ë³´
        confidence = subgraph_result.confidence_score
        confidence_text = (
            "High" if confidence > 0.7 else "Medium" if confidence > 0.4 else "Low"
        )
        lines.append(f"- **Confidence:** {confidence_text} ({confidence:.2f})")

        return "\n".join(lines) + "\n"

    def _generate_papers_section(
        self,
        paper_nodes: Dict[str, Dict[str, Any]],
        node_priorities: Dict[str, ContextPriority],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """ë…¼ë¬¸ ì„¹ì…˜ ìƒì„±"""

        lines = [templates["section_headers"]["papers"]]

        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
        sorted_papers = sorted(
            paper_nodes.items(),
            key=lambda x: (
                node_priorities.get(x[0], ContextPriority.LOW).value,
                x[1].get("year", ""),
                x[1].get("title", ""),
            ),
        )

        # ì œí•œëœ ìˆ˜ë§Œ ìƒì„¸ í‘œì‹œ
        detailed_count = min(len(sorted_papers), config.max_nodes_detail)

        for i, (paper_id, paper_data) in enumerate(sorted_papers):
            title = paper_data.get("title", "Unknown Title")
            year = paper_data.get("year", "")
            authors = paper_data.get("authors", [])
            journal = paper_data.get("journal", "")

            priority = node_priorities.get(paper_id, ContextPriority.LOW)

            if i < detailed_count:
                # ìƒì„¸ ì •ë³´
                lines.append(f"\n### ğŸ“„ {title}")
                if year:
                    lines.append(f"- **Year:** {year}")
                if authors:
                    author_list = authors if isinstance(authors, list) else [authors]
                    author_text = ", ".join(author_list[:3])  # ìµœëŒ€ 3ëª…
                    if len(author_list) > 3:
                        author_text += f" (+{len(author_list)-3} others)"
                    lines.append(f"- **Authors:** {author_text}")
                if journal:
                    lines.append(f"- **Journal:** {journal}")
                lines.append(f"- **Relevance:** {priority.value}")
            else:
                # ê°„ë‹¨ í‘œì‹œ
                summary = f"- {title}"
                if year:
                    summary += f" ({year})"
                lines.append(summary)

        # ìš”ì•½ ì •ë³´ (ë§ì€ ê²½ìš°)
        if len(sorted_papers) > detailed_count:
            remaining = len(sorted_papers) - detailed_count
            lines.append(f"\n*... and {remaining} more papers*")

        return "\n".join(lines) + "\n"

    def _generate_authors_section(
        self,
        author_nodes: Dict[str, Dict[str, Any]],
        node_priorities: Dict[str, ContextPriority],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """ì €ì ì„¹ì…˜ ìƒì„±"""

        lines = [templates["section_headers"]["authors"]]

        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
        sorted_authors = sorted(
            author_nodes.items(),
            key=lambda x: (
                node_priorities.get(x[0], ContextPriority.LOW).value,
                x[1].get("paper_count", 0),
            ),
            reverse=True,
        )

        for i, (author_id, author_data) in enumerate(
            sorted_authors[: config.max_nodes_detail]
        ):
            name = author_data.get("name", author_id)
            paper_count = author_data.get("paper_count", 0)
            productivity_type = author_data.get("productivity_type", "")

            lines.append(f"\n### ğŸ‘¤ {name}")
            if paper_count:
                lines.append(f"- **Papers:** {paper_count}")
            if productivity_type:
                lines.append(f"- **Type:** {productivity_type}")

            # ì£¼ìš” í‚¤ì›Œë“œ (ìˆë‹¤ë©´)
            top_keywords = author_data.get("top_keywords", [])
            if top_keywords:
                if isinstance(top_keywords[0], (list, tuple)):
                    keyword_names = [kw[0] for kw in top_keywords[:3]]
                else:
                    keyword_names = top_keywords[:3]
                lines.append(f"- **Research Areas:** {', '.join(keyword_names)}")

        return "\n".join(lines) + "\n"

    def _generate_keywords_section(
        self,
        keyword_nodes: Dict[str, Dict[str, Any]],
        node_priorities: Dict[str, ContextPriority],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """í‚¤ì›Œë“œ ì„¹ì…˜ ìƒì„±"""

        lines = [templates["section_headers"]["keywords"]]

        # ë¹ˆë„ë³„ë¡œ ì •ë ¬
        sorted_keywords = sorted(
            keyword_nodes.items(),
            key=lambda x: (
                node_priorities.get(x[0], ContextPriority.LOW).value,
                x[1].get("frequency", 0),
            ),
            reverse=True,
        )

        # ê·¸ë£¹ë³„ë¡œ í‘œì‹œ
        critical_keywords = []
        high_keywords = []
        other_keywords = []

        for keyword_id, keyword_data in sorted_keywords:
            keyword = keyword_data.get("name", keyword_id)
            frequency = keyword_data.get("frequency", 0)
            priority = node_priorities.get(keyword_id, ContextPriority.LOW)

            keyword_info = f"{keyword}"
            if frequency:
                keyword_info += f" ({frequency})"

            if priority == ContextPriority.CRITICAL:
                critical_keywords.append(keyword_info)
            elif priority == ContextPriority.HIGH:
                high_keywords.append(keyword_info)
            else:
                other_keywords.append(keyword_info)

        # ìš°ì„ ìˆœìœ„ë³„ ì¶œë ¥
        if critical_keywords:
            lines.append(f"\n**{templates['summary_phrases']['high_relevance']}:**")
            lines.append(f"{', '.join(critical_keywords[:10])}")

        if high_keywords:
            lines.append(f"\n**{templates['summary_phrases']['medium_relevance']}:**")
            lines.append(f"{', '.join(high_keywords[:15])}")

        if other_keywords and not config.summarize_low_priority:
            lines.append(f"\n**{templates['summary_phrases']['additional_info']}:**")
            lines.append(f"{', '.join(other_keywords[:10])}")

        return "\n".join(lines) + "\n"

    def _generate_relationships_section(
        self,
        edges: List[Dict[str, Any]],
        nodes: Dict[str, Dict[str, Any]],
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """ê´€ê³„ ì„¹ì…˜ ìƒì„±"""

        lines = [templates["section_headers"]["relationships"]]

        # ì—£ì§€ íƒ€ì…ë³„ ê·¸ë£¹í™”
        edge_groups = defaultdict(list)
        for edge in edges:
            edge_type = edge.get("edge_type", "related_to")
            edge_groups[edge_type].append(edge)

        connection_words = templates["connection_words"]

        for edge_type, type_edges in edge_groups.items():
            if len(type_edges) == 0:
                continue

            connection_word = connection_words.get(edge_type, edge_type)
            lines.append(f"\n**{connection_word.title()} ({len(type_edges)}):**")

            # ìƒ˜í”Œ ê´€ê³„ë“¤ í‘œì‹œ
            sample_size = min(5, len(type_edges))
            for edge in type_edges[:sample_size]:
                source_id = edge["source"]
                target_id = edge["target"]

                source_data = nodes.get(source_id, {})
                target_data = nodes.get(target_id, {})

                source_name = self._get_node_display_name(source_data)
                target_name = self._get_node_display_name(target_data)

                lines.append(f"- {source_name} â†’ {target_name}")

            # ë” ìˆìœ¼ë©´ ìš”ì•½
            if len(type_edges) > sample_size:
                lines.append(f"- *... and {len(type_edges) - sample_size} more*")

        return "\n".join(lines) + "\n"

    def _generate_statistics_section(
        self,
        subgraph_result: SubgraphResult,
        templates: Dict[str, Any],
        config: SerializationConfig,
    ) -> str:
        """í†µê³„ ì„¹ì…˜ ìƒì„±"""

        lines = [templates["section_headers"]["statistics"]]

        # ê¸°ë³¸ í†µê³„
        lines.append(f"- **Total Nodes:** {subgraph_result.total_nodes}")
        lines.append(f"- **Total Edges:** {subgraph_result.total_edges}")
        lines.append(
            f"- **Extraction Strategy:** {subgraph_result.extraction_strategy.value}"
        )
        lines.append(f"- **Processing Time:** {subgraph_result.extraction_time:.2f}s")

        # ë…¸ë“œ íƒ€ì…ë³„ ë¶„í¬
        if subgraph_result.nodes_by_type:
            lines.append(f"\n**Node Distribution:**")
            for node_type, count in subgraph_result.nodes_by_type.items():
                lines.append(f"- {node_type}: {count}")

        # ìƒìœ„ ê´€ë ¨ ë…¸ë“œë“¤
        top_relevant = sorted(
            subgraph_result.relevance_scores.items(), key=lambda x: x[1], reverse=True
        )[:5]

        if top_relevant:
            lines.append(f"\n**Most Relevant Nodes:**")
            for node_id, score in top_relevant:
                node_data = subgraph_result.nodes.get(node_id, {})
                name = self._get_node_display_name(node_data)
                lines.append(f"- {name}: {score:.3f}")

        return "\n".join(lines) + "\n"

    def _get_node_display_name(self, node_data: Dict[str, Any]) -> str:
        """ë…¸ë“œ í‘œì‹œëª… ìƒì„±"""
        node_type = node_data.get("node_type", "unknown")

        if node_type == "paper":
            title = node_data.get("title", "Unknown Paper")
            return title[:50] + "..." if len(title) > 50 else title
        elif node_type == "author":
            return node_data.get("name", node_data.get("id", "Unknown Author"))
        elif node_type == "keyword":
            return node_data.get("name", node_data.get("id", "Unknown Keyword"))
        elif node_type == "journal":
            return node_data.get("name", node_data.get("id", "Unknown Journal"))
        else:
            return node_data.get("name", node_data.get("id", "Unknown"))

    def _assemble_main_text(
        self, sections: Dict[str, str], config: SerializationConfig, language: str
    ) -> str:
        """ì„¹ì…˜ë“¤ì„ ë©”ì¸ í…ìŠ¤íŠ¸ë¡œ ì¡°í•©"""

        if config.format_style == SerializationFormat.STRUCTURED:
            # êµ¬ì¡°í™”ëœ í˜•íƒœ
            section_order = [
                "overview",
                "papers",
                "authors",
                "keywords",
                "relationships",
                "statistics",
            ]
            text_parts = []

            for section_name in section_order:
                if section_name in sections and sections[section_name].strip():
                    text_parts.append(sections[section_name])

            return "\n".join(text_parts)

        elif config.format_style == SerializationFormat.NARRATIVE:
            # ìì—°ì–´ ì„œìˆ í˜•
            return self._create_narrative_text(sections, language)

        elif config.format_style == SerializationFormat.COMPACT:
            # ì••ì¶•í˜•
            return self._create_compact_text(sections, config)

        else:
            # ê¸°ë³¸ê°’: STRUCTURED
            return "\n".join(sections.values())

    def _create_narrative_text(self, sections: Dict[str, str], language: str) -> str:
        """ìì—°ì–´ ì„œìˆ í˜• í…ìŠ¤íŠ¸ ìƒì„±"""
        # êµ¬í˜„ ê°„ì†Œí™” - êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ëŒ€ì²´
        return "\n".join(sections.values())

    def _create_compact_text(
        self, sections: Dict[str, str], config: SerializationConfig
    ) -> str:
        """ì••ì¶•í˜• í…ìŠ¤íŠ¸ ìƒì„±"""
        # í—¤ë” ì œê±° ë° ì••ì¶•
        compact_lines = []

        for section_text in sections.values():
            lines = section_text.split("\n")
            # í—¤ë”ì™€ ë¹ˆ ì¤„ ì œê±°
            content_lines = [
                line for line in lines if line.strip() and not line.startswith("#")
            ]
            compact_lines.extend(content_lines[:5])  # ê° ì„¹ì…˜ì—ì„œ ìµœëŒ€ 5ì¤„ë§Œ

        return "\n".join(compact_lines)

    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )"""
        # í•œêµ­ì–´/ì˜ì–´ í˜¼ìš© í…ìŠ¤íŠ¸ì— ëŒ€í•œ ëŒ€ëµì  ì¶”ì •
        char_count = len(text)
        return int(char_count / self._avg_chars_per_token)

    def _compress_content(
        self,
        main_text: str,
        sections: Dict[str, str],
        config: SerializationConfig,
        target_tokens: int,
    ) -> Tuple[str, Dict[str, str]]:
        """ì»¨í…ì¸  ì••ì¶•"""

        logger.info(f"ğŸ—œï¸ Compressing content to fit {target_tokens} tokens...")

        # ìš°ì„ ìˆœìœ„ ìˆœì„œ (ì¤‘ìš”ë„ ìˆœ)
        section_priorities = [
            "overview",
            "papers",
            "authors",
            "relationships",
            "keywords",
            "statistics",
        ]

        compressed_sections = {}
        current_tokens = 0

        for section_name in section_priorities:
            if section_name not in sections:
                continue

            section_text = sections[section_name]
            section_tokens = self._estimate_tokens(section_text)

            if current_tokens + section_tokens <= target_tokens:
                # ì „ì²´ í¬í•¨
                compressed_sections[section_name] = section_text
                current_tokens += section_tokens
            else:
                # ë¶€ë¶„ í¬í•¨
                remaining_tokens = target_tokens - current_tokens
                if remaining_tokens > 100:  # ìµœì†Œ 100 í† í°ì€ ìˆì–´ì•¼ ì˜ë¯¸ê°€ ìˆìŒ
                    # ì„¹ì…˜ì„ ì••ì¶•í•´ì„œ í¬í•¨
                    compressed_text = self._compress_section(
                        section_text, remaining_tokens
                    )
                    compressed_sections[section_name] = compressed_text
                break

        # ì••ì¶•ëœ ë©”ì¸ í…ìŠ¤íŠ¸ ì¬ì¡°í•©
        compressed_main_text = self._assemble_main_text(
            compressed_sections, config, self.config.language
        )

        return compressed_main_text, compressed_sections

    def _compress_section(self, section_text: str, target_tokens: int) -> str:
        """ê°œë³„ ì„¹ì…˜ ì••ì¶•"""
        lines = section_text.split("\n")

        # ì¤‘ìš”í•œ ì¤„ë“¤ ìš°ì„  ì„ íƒ (í—¤ë”, ìš”ì•½ ì •ë³´ ë“±)
        important_lines = []
        detail_lines = []

        for line in lines:
            if (
                line.startswith("#")
                or line.startswith("**")
                or "total" in line.lower()
                or "relevance" in line.lower()
            ):
                important_lines.append(line)
            else:
                detail_lines.append(line)

        # ì¤‘ìš”í•œ ì¤„ë“¤ì„ ë¨¼ì € í¬í•¨
        result_lines = important_lines[:]
        current_tokens = self._estimate_tokens("\n".join(result_lines))

        # ë‚¨ì€ í† í°ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ì¶”ê°€
        for line in detail_lines:
            line_tokens = self._estimate_tokens(line)
            if current_tokens + line_tokens <= target_tokens:
                result_lines.append(line)
                current_tokens += line_tokens
            else:
                break

        # ì˜ë ¸ë‹¤ëŠ” í‘œì‹œ ì¶”ê°€
        if len(result_lines) < len(lines):
            result_lines.append("*[Content truncated due to length limits]*")

        return "\n".join(result_lines)

    def _is_mixed_language(self, text: str) -> bool:
        """í˜¼ìš© ì–¸ì–´ ì—¬ë¶€ í™•ì¸"""
        korean_chars = len(re.findall(r"[ê°€-í£]", text))
        english_chars = len(re.findall(r"[a-zA-Z]", text))

        total_chars = korean_chars + english_chars
        if total_chars == 0:
            return False

        korean_ratio = korean_chars / total_chars
        return 0.1 < korean_ratio < 0.9  # 10%-90% ì‚¬ì´ë©´ í˜¼ìš©ìœ¼ë¡œ íŒë‹¨


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª Testing ContextSerializer...")

    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ SubgraphResult ìƒì„±
    from dataclasses import dataclass
    from typing import Dict, List, Any

    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    test_nodes = {
        "paper_1": {
            "id": "paper_1",
            "node_type": "paper",
            "title": "Deep Learning for Battery State of Charge Prediction using LSTM Networks",
            "year": "2023",
            "authors": ["ê¹€ì² ìˆ˜", "John Smith"],
            "journal": "IEEE Transactions on Power Electronics",
        },
        "author_1": {
            "id": "author_1",
            "node_type": "author",
            "name": "ê¹€ì² ìˆ˜",
            "paper_count": 15,
            "productivity_type": "Leading Researcher",
            "top_keywords": [("machine learning", 8), ("battery", 6)],
        },
        "keyword_1": {
            "id": "keyword_1",
            "node_type": "keyword",
            "name": "machine learning",
            "frequency": 25,
        },
    }

    test_edges = [
        {"source": "paper_1", "target": "author_1", "edge_type": "authored_by"},
        {"source": "paper_1", "target": "keyword_1", "edge_type": "has_keyword"},
    ]

    # ë”ë¯¸ SearchResult
    from collections import namedtuple

    SearchResult = namedtuple("SearchResult", ["node_id", "similarity_score"])

    test_initial_matches = [
        SearchResult("paper_1", 0.85),
        SearchResult("keyword_1", 0.75),
    ]

    # ë”ë¯¸ SubgraphResult
    from enum import Enum

    class SearchStrategy(Enum):
        HYBRID = "hybrid"

    class DummySubgraphResult:
        def __init__(self):
            self.nodes = test_nodes
            self.edges = test_edges
            self.query = "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì€?"
            self.query_analysis = None
            self.extraction_strategy = SearchStrategy.HYBRID
            self.total_nodes = len(test_nodes)
            self.total_edges = len(test_edges)
            self.nodes_by_type = {"paper": 1, "author": 1, "keyword": 1}
            self.extraction_time = 1.5
            self.initial_matches = test_initial_matches
            self.expansion_path = []
            self.relevance_scores = {"paper_1": 0.9, "author_1": 0.7, "keyword_1": 0.8}
            self.confidence_score = 0.85

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_result = DummySubgraphResult()

    # ContextSerializer ì´ˆê¸°í™”
    config = SerializationConfig(
        format_style=SerializationFormat.STRUCTURED, max_tokens=2000, language="mixed"
    )

    serializer = ContextSerializer(config)

    # ì§ë ¬í™” ìˆ˜í–‰
    serialized = serializer.serialize(test_result)

    print(f"âœ… Serialization completed:")
    print(f"   ğŸ“ Tokens: {serialized.total_tokens}")
    print(f"   ğŸŒ Language: {serialized.language}")
    print(f"   ğŸ“Š Compression: {serialized.compression_ratio:.1%}")

    print(f"\nğŸ“ Generated Text:")
    print("=" * 60)
    print(serialized.main_text)
    print("=" * 60)

    print(f"\nâœ… ContextSerializer test completed!")


if __name__ == "__main__":
    main()
