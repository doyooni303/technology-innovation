"""
ë…¸ë“œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëª¨ë“ˆ
Node Text Processors for GraphRAG Embeddings

ê° ë…¸ë“œ íƒ€ì…ì˜ íŠ¹ì„±ì— ë§ëŠ” ìµœì í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
- PaperTextProcessor: ë…¼ë¬¸ ì œëª©, ì´ˆë¡, í‚¤ì›Œë“œ ê²°í•©
- AuthorTextProcessor: ì €ìëª…, ì—°êµ¬ ë¶„ì•¼, ë…¼ë¬¸ ì œëª©ë“¤ ê²°í•©
- KeywordTextProcessor: í‚¤ì›Œë“œì™€ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
- JournalTextProcessor: ì €ë„ëª…ê³¼ ì—°êµ¬ ì˜ì—­ ì •ë³´ ê²°í•©
"""

import re
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Set, Union
from collections import Counter

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class BaseNodeTextProcessor(ABC):
    """ë…¸ë“œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, max_length: int = 512, language: str = "mixed"):
        """
        Args:
            max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´ (í† í° ê¸°ì¤€ ëŒ€ëµì )
            language: ì£¼ìš” ì–¸ì–´ ("ko", "en", "mixed")
        """
        self.max_length = max_length
        self.language = language

        # ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ì„¤ì •
        self.stopwords = self._get_stopwords(language)

    def _get_stopwords(self, language: str) -> Set[str]:
        """ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ì„¸íŠ¸"""
        korean_stopwords = {
            "ì´",
            "ê°€",
            "ì„",
            "ë¥¼",
            "ì˜",
            "ì—",
            "ì—ì„œ",
            "ì™€",
            "ê³¼",
            "ë„",
            "ëŠ”",
            "ì€",
            "í•˜ë‹¤",
            "ìˆë‹¤",
            "ë˜ë‹¤",
            "ì´ë‹¤",
            "ê·¸",
            "ê²ƒ",
            "ìˆ˜",
            "ë“±",
            "ë°",
            "ë˜í•œ",
            "ê·¸ë¦¬ê³ ",
        }

        english_stopwords = {
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
            "from",
            "up",
            "about",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "between",
            "among",
            "this",
            "that",
            "these",
            "those",
            "i",
            "me",
            "my",
            "myself",
            "we",
            "our",
            "ours",
        }

        if language == "ko":
            return korean_stopwords
        elif language == "en":
            return english_stopwords
        else:  # mixed
            return korean_stopwords | english_stopwords

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ê¸°ë³¸ ì •ì œ"""
        if not text:
            return ""

        # ê¸°ë³¸ ì •ì œ
        text = str(text).strip()

        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r"\s+", " ", text)

        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ì˜ë¯¸ìˆëŠ” ê²ƒë“¤ì€ ë³´ì¡´)
        text = re.sub(r"[^\w\s\-\.\,\(\)]", " ", text)

        # ë‹¤ì‹œ ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r"\s+", " ", text).strip()

        return text

    def truncate_text(self, text: str, max_length: Optional[int] = None) -> str:
        """í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë‹¨ì–´ ê¸°ì¤€)"""
        if not text:
            return ""

        max_length = max_length or self.max_length
        words = text.split()

        if len(words) <= max_length:
            return text

        # ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì„ ìš°ì„  ë³´ì¡´
        truncated_words = words[:max_length]
        return " ".join(truncated_words)

    def remove_stopwords(self, text: str) -> str:
        """ë¶ˆìš©ì–´ ì œê±° (ì„ íƒì )"""
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in self.stopwords]
        return " ".join(filtered_words)

    @abstractmethod
    def process_node(self, node_data: Dict[str, Any]) -> str:
        """ë…¸ë“œ ë°ì´í„°ë¥¼ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        pass

    def batch_process(self, nodes_data: List[Dict[str, Any]]) -> List[str]:
        """ì—¬ëŸ¬ ë…¸ë“œë¥¼ ë°°ì¹˜ ì²˜ë¦¬"""
        return [self.process_node(node_data) for node_data in nodes_data]


class PaperTextProcessor(BaseNodeTextProcessor):
    """ë…¼ë¬¸ ë…¸ë“œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°"""

    def __init__(
        self,
        max_length: int = 512,
        language: str = "mixed",
        include_authors: bool = True,
        include_journal: bool = True,
    ):
        """
        Args:
            max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
            language: ì£¼ìš” ì–¸ì–´
            include_authors: ì €ì ì •ë³´ í¬í•¨ ì—¬ë¶€
            include_journal: ì €ë„ ì •ë³´ í¬í•¨ ì—¬ë¶€
        """
        super().__init__(max_length, language)
        self.include_authors = include_authors
        self.include_journal = include_journal

    def process_node(self, node_data: Dict[str, Any]) -> str:
        """ë…¼ë¬¸ ë…¸ë“œ ì²˜ë¦¬

        Format: "Title: [title] Abstract: [abstract] Keywords: [keywords] Authors: [authors] Journal: [journal]"
        """
        parts = []

        # 1. ì œëª© (ê°€ì¥ ì¤‘ìš”)
        title = self.clean_text(node_data.get("title", ""))
        if title:
            parts.append(f"Title: {title}")

        # 2. ì´ˆë¡ (ë§¤ìš° ì¤‘ìš”)
        abstract = self.clean_text(node_data.get("abstract", ""))
        if abstract:
            # ì´ˆë¡ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            abstract = self.truncate_text(abstract, max_length=150)
            parts.append(f"Abstract: {abstract}")

        # 3. í‚¤ì›Œë“œ (ì¤‘ìš”)
        keywords = node_data.get("keywords", [])
        if keywords:
            if isinstance(keywords, str):
                keywords = [kw.strip() for kw in keywords.split(";") if kw.strip()]
            elif isinstance(keywords, list):
                keywords = [str(kw).strip() for kw in keywords if str(kw).strip()]

            # ì¤‘ë³µ ì œê±° ë° ì •ì œ
            clean_keywords = []
            seen = set()
            for kw in keywords[:10]:  # ìµœëŒ€ 10ê°œ
                kw_clean = self.clean_text(kw)
                if kw_clean and kw_clean.lower() not in seen:
                    clean_keywords.append(kw_clean)
                    seen.add(kw_clean.lower())

            if clean_keywords:
                parts.append(f"Keywords: {', '.join(clean_keywords)}")

        # 4. ì €ì (ì„ íƒì )
        if self.include_authors:
            authors = node_data.get("authors", [])
            if authors:
                if isinstance(authors, str):
                    authors = [authors]
                elif isinstance(authors, list):
                    authors = [
                        str(author).strip() for author in authors if str(author).strip()
                    ]

                # ì²˜ìŒ 5ëª…ë§Œ í¬í•¨
                author_names = []
                for author in authors[:5]:
                    author_clean = self.clean_text(author)
                    if author_clean:
                        author_names.append(author_clean)

                if author_names:
                    parts.append(f"Authors: {', '.join(author_names)}")

        # 5. ì €ë„ (ì„ íƒì )
        if self.include_journal:
            journal = self.clean_text(node_data.get("journal", ""))
            if journal:
                parts.append(f"Journal: {journal}")

        # 6. ì—°ë„ (ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸)
        year = node_data.get("year", "")
        if year:
            parts.append(f"Year: {year}")

        # ê²°í•© ë° ê¸¸ì´ ì œí•œ
        combined_text = " ".join(parts)
        return self.truncate_text(combined_text)


class AuthorTextProcessor(BaseNodeTextProcessor):
    """ì €ì ë…¸ë“œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°"""

    def __init__(
        self,
        max_length: int = 512,
        language: str = "mixed",
        include_papers: bool = True,
        max_papers: int = 10,
    ):
        """
        Args:
            max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
            language: ì£¼ìš” ì–¸ì–´
            include_papers: ë…¼ë¬¸ ì •ë³´ í¬í•¨ ì—¬ë¶€
            max_papers: í¬í•¨í•  ìµœëŒ€ ë…¼ë¬¸ ìˆ˜
        """
        super().__init__(max_length, language)
        self.include_papers = include_papers
        self.max_papers = max_papers

    def process_node(self, node_data: Dict[str, Any]) -> str:
        """ì €ì ë…¸ë“œ ì²˜ë¦¬

        Format: "Author: [name] Research Areas: [inferred areas] Papers: [paper titles] Journals: [journals]"
        """
        parts = []

        # 1. ì €ìëª… (í•„ìˆ˜)
        name = self.clean_text(node_data.get("name", ""))
        if not name:
            name = self.clean_text(node_data.get("id", ""))  # fallback to ID

        if name:
            parts.append(f"Author: {name}")

        # 2. ì—°êµ¬ ìƒì‚°ì„± ì •ë³´
        paper_count = node_data.get("paper_count", 0)
        if paper_count:
            parts.append(f"Publications: {paper_count} papers")

        # 3. í™œë™ ê¸°ê°„
        first_year = node_data.get("first_year", "")
        last_year = node_data.get("last_year", "")
        if first_year and last_year:
            if first_year == last_year:
                parts.append(f"Active: {first_year}")
            else:
                parts.append(f"Active: {first_year}-{last_year}")

        # 4. ì£¼ìš” í‚¤ì›Œë“œ/ì—°êµ¬ ë¶„ì•¼ (top_keywordsì—ì„œ ì¶”ì¶œ)
        top_keywords = node_data.get("top_keywords", [])
        if top_keywords:
            if isinstance(top_keywords, list) and len(top_keywords) > 0:
                # íŠœí”Œ í˜•íƒœì¸ ê²½ìš° í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
                keywords = []
                for item in top_keywords[:5]:  # ìƒìœ„ 5ê°œ
                    if isinstance(item, (list, tuple)) and len(item) >= 2:
                        keyword = str(item[0]).strip()
                    else:
                        keyword = str(item).strip()

                    keyword_clean = self.clean_text(keyword)
                    if keyword_clean:
                        keywords.append(keyword_clean)

                if keywords:
                    parts.append(f"Research Areas: {', '.join(keywords)}")

        # 5. ì£¼ìš” ì €ë„
        most_frequent_journal = self.clean_text(
            node_data.get("most_frequent_journal", "")
        )
        if most_frequent_journal:
            parts.append(f"Main Journal: {most_frequent_journal}")

        # 6. ìƒì‚°ì„± íƒ€ì…
        productivity_type = node_data.get("productivity_type", "")
        if productivity_type:
            parts.append(f"Type: {productivity_type}")

        # 7. ë…¼ë¬¸ ì œëª©ë“¤ (ì„ íƒì , ì—°êµ¬ ë¶„ì•¼ ì¶”ë¡ ìš©)
        if self.include_papers:
            papers = node_data.get("papers", [])
            if papers and isinstance(papers, list):
                paper_titles = []
                for paper in papers[: self.max_papers]:
                    if isinstance(paper, dict):
                        title = self.clean_text(paper.get("title", ""))
                    else:
                        title = self.clean_text(str(paper))

                    if title:
                        # ë„ˆë¬´ ê¸´ ì œëª©ì€ ì¤„ì´ê¸°
                        title = self.truncate_text(title, max_length=20)
                        paper_titles.append(title)

                if paper_titles:
                    parts.append(f"Paper Titles: {' | '.join(paper_titles)}")

        # ê²°í•© ë° ê¸¸ì´ ì œí•œ
        combined_text = " ".join(parts)
        return self.truncate_text(combined_text)


class KeywordTextProcessor(BaseNodeTextProcessor):
    """í‚¤ì›Œë“œ ë…¸ë“œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°"""

    def __init__(
        self,
        max_length: int = 512,
        language: str = "mixed",
        include_context: bool = True,
    ):
        """
        Args:
            max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
            language: ì£¼ìš” ì–¸ì–´
            include_context: ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
        """
        super().__init__(max_length, language)
        self.include_context = include_context

    def _generate_keyword_context(self, keyword: str) -> str:
        """í‚¤ì›Œë“œì— ëŒ€í•œ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        keyword_lower = keyword.lower()

        # ë„ë©”ì¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        contexts = {
            # AI/ML í‚¤ì›Œë“œë“¤
            "machine learning": "artificial intelligence algorithm data analysis prediction model training",
            "deep learning": "neural network artificial intelligence machine learning algorithm",
            "neural network": "deep learning artificial intelligence machine learning algorithm",
            "artificial intelligence": "machine learning deep learning neural network algorithm",
            "reinforcement learning": "machine learning artificial intelligence agent reward algorithm",
            # ë°°í„°ë¦¬/ì „ê¸°ì°¨ í‚¤ì›Œë“œë“¤
            "battery": "energy storage lithium ion electric vehicle power management",
            "lithium": "battery energy storage electric vehicle ion cell",
            "soc": "state of charge battery energy management system estimation",
            "electric vehicle": "battery charging power management energy storage automotive",
            "charging": "battery electric vehicle power energy management station",
            # ì¼ë°˜ ê¸°ìˆ  í‚¤ì›Œë“œë“¤
            "optimization": "algorithm performance improvement efficiency solution",
            "control": "system management regulation automation process",
            "system": "architecture design implementation framework platform",
            "algorithm": "computation method procedure optimization solution",
            "model": "simulation representation framework algorithm analysis",
        }

        # í‚¤ì›Œë“œ ë§¤ì¹­ (ë¶€ë¶„ ë§¤ì¹­ í¬í•¨)
        for key, context in contexts.items():
            if key in keyword_lower or keyword_lower in key:
                return context

        return ""

    def process_node(self, node_data: Dict[str, Any]) -> str:
        """í‚¤ì›Œë“œ ë…¸ë“œ ì²˜ë¦¬

        Format: "Keyword: [keyword] Frequency: [freq] Context: [related terms] Papers: [paper count]"
        """
        parts = []

        # 1. í‚¤ì›Œë“œ ìì²´ (í•„ìˆ˜)
        keyword = self.clean_text(node_data.get("id", ""))
        if not keyword:
            keyword = self.clean_text(node_data.get("name", ""))

        if keyword:
            parts.append(f"Keyword: {keyword}")

        # 2. ë¹ˆë„ ì •ë³´
        frequency = node_data.get("frequency", 0)
        if frequency:
            parts.append(f"Frequency: {frequency} papers")

        # 3. ê´€ë ¨ ë…¼ë¬¸ ìˆ˜ (papers í•„ë“œì—ì„œ)
        papers = node_data.get("papers", [])
        if papers and isinstance(papers, list):
            parts.append(f"Used in: {len(papers)} publications")

        # 4. ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
        if self.include_context and keyword:
            context = self._generate_keyword_context(keyword)
            if context:
                parts.append(f"Related: {context}")

        # 5. ë™ì‹œ ì¶œí˜„ í‚¤ì›Œë“œ ì •ë³´ (ìˆë‹¤ë©´)
        # UnifiedGraphì—ì„œ ì—°ê²°ëœ ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ì„ ì°¾ì„ ìˆ˜ ìˆìŒ
        # ì´ëŠ” SubgraphExtractorì—ì„œ í™œìš©ë  ì˜ˆì •

        # ê²°í•© ë° ê¸¸ì´ ì œí•œ
        combined_text = " ".join(parts)
        return self.truncate_text(combined_text)


class JournalTextProcessor(BaseNodeTextProcessor):
    """ì €ë„ ë…¸ë“œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°"""

    def __init__(
        self, max_length: int = 512, language: str = "mixed", include_stats: bool = True
    ):
        """
        Args:
            max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
            language: ì£¼ìš” ì–¸ì–´
            include_stats: í†µê³„ ì •ë³´ í¬í•¨ ì—¬ë¶€
        """
        super().__init__(max_length, language)
        self.include_stats = include_stats

    def _classify_journal_domain(self, journal_name: str, keywords: List[str]) -> str:
        """ì €ë„ ë„ë©”ì¸ ë¶„ë¥˜"""
        name_lower = journal_name.lower()

        # ì €ë„ëª… ê¸°ë°˜ ë¶„ë¥˜
        if any(term in name_lower for term in ["ieee", "acm", "computer", "software"]):
            return "Computer Science"
        elif any(term in name_lower for term in ["nature", "science", "cell"]):
            return "High-Impact General Science"
        elif any(
            term in name_lower for term in ["energy", "power", "electric", "battery"]
        ):
            return "Energy and Power Systems"
        elif any(
            term in name_lower
            for term in ["artificial", "intelligence", "neural", "machine"]
        ):
            return "Artificial Intelligence"
        elif any(
            term in name_lower for term in ["engineering", "industrial", "automation"]
        ):
            return "Engineering"
        elif any(
            term in name_lower for term in ["applied", "international", "journal"]
        ):
            return "Applied Sciences"

        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        if keywords:
            keyword_text = " ".join(str(kw).lower() for kw in keywords)
            if any(
                term in keyword_text for term in ["ai", "machine learning", "neural"]
            ):
                return "Artificial Intelligence"
            elif any(term in keyword_text for term in ["battery", "energy", "power"]):
                return "Energy Systems"
            elif any(
                term in keyword_text for term in ["computer", "algorithm", "software"]
            ):
                return "Computer Science"

        return "General"

    def process_node(self, node_data: Dict[str, Any]) -> str:
        """ì €ë„ ë…¸ë“œ ì²˜ë¦¬

        Format: "Journal: [name] Domain: [domain] Papers: [count] Period: [years] Focus: [keywords]"
        """
        parts = []

        # 1. ì €ë„ëª… (í•„ìˆ˜)
        journal_name = self.clean_text(node_data.get("name", ""))
        if not journal_name:
            journal_name = self.clean_text(node_data.get("id", ""))

        if journal_name:
            parts.append(f"Journal: {journal_name}")

        # 2. ë…¼ë¬¸ ìˆ˜ í†µê³„
        if self.include_stats:
            paper_count = node_data.get("paper_count", 0)
            if paper_count:
                parts.append(f"Publications: {paper_count} papers")

        # 3. í™œë™ ê¸°ê°„
        first_year = node_data.get("first_year", "")
        last_year = node_data.get("last_year", "")
        if first_year and last_year:
            if first_year == last_year:
                parts.append(f"Period: {first_year}")
            else:
                parts.append(f"Period: {first_year}-{last_year}")

                # í™œë™ ë…„ìˆ˜ ê³„ì‚°
                try:
                    years_active = int(last_year) - int(first_year) + 1
                    parts.append(f"Active: {years_active} years")
                except:
                    pass

        # 4. ì €ë„ íƒ€ì…/ë„ë©”ì¸
        journal_type = node_data.get("journal_type", "")
        if journal_type:
            parts.append(f"Type: {journal_type}")

        # 5. ì£¼ìš” í‚¤ì›Œë“œ/ì—°êµ¬ ë¶„ì•¼
        top_keywords = node_data.get("top_keywords", [])
        if top_keywords:
            if isinstance(top_keywords, list) and len(top_keywords) > 0:
                keywords = []
                for item in top_keywords[:8]:  # ìƒìœ„ 8ê°œ
                    if isinstance(item, (list, tuple)) and len(item) >= 2:
                        keyword = str(item[0]).strip()
                    else:
                        keyword = str(item).strip()

                    keyword_clean = self.clean_text(keyword)
                    if keyword_clean:
                        keywords.append(keyword_clean)

                if keywords:
                    parts.append(f"Focus Areas: {', '.join(keywords)}")

                    # ë„ë©”ì¸ ìë™ ë¶„ë¥˜
                    domain = self._classify_journal_domain(journal_name, keywords)
                    parts.append(f"Domain: {domain}")

        # 6. ê¸°íƒ€ í†µê³„ (ì„ íƒì )
        if self.include_stats:
            unique_authors = node_data.get("unique_authors", 0)
            if unique_authors:
                parts.append(f"Authors: {unique_authors} contributors")

        # ê²°í•© ë° ê¸¸ì´ ì œí•œ
        combined_text = " ".join(parts)
        return self.truncate_text(combined_text)


# í”„ë¡œì„¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬
NODE_PROCESSORS = {
    "paper": PaperTextProcessor,
    "author": AuthorTextProcessor,
    "keyword": KeywordTextProcessor,
    "journal": JournalTextProcessor,
}


def create_text_processor(
    node_type: str, max_length: int = 512, language: str = "mixed", **kwargs
) -> BaseNodeTextProcessor:
    """í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        node_type: ë…¸ë“œ íƒ€ì… ("paper", "author", "keyword", "journal")
        max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
        language: ì£¼ìš” ì–¸ì–´
        **kwargs: ê° í”„ë¡œì„¸ì„œë³„ ì¶”ê°€ ì„¤ì •

    Returns:
        BaseNodeTextProcessor ì¸ìŠ¤í„´ìŠ¤
    """
    if node_type not in NODE_PROCESSORS:
        raise ValueError(f"Unsupported node type: {node_type}")

    processor_class = NODE_PROCESSORS[node_type]
    return processor_class(max_length=max_length, language=language, **kwargs)


def get_supported_node_types() -> List[str]:
    """ì§€ì›ë˜ëŠ” ë…¸ë“œ íƒ€ì… ëª©ë¡ ë°˜í™˜"""
    return list(NODE_PROCESSORS.keys())


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ§ª Testing Node Text Processors...")

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        "paper": {
            "id": "paper_1",
            "title": "Deep Learning for Battery State of Charge Prediction",
            "abstract": "This paper presents a novel deep learning approach for accurate battery SoC prediction using LSTM networks.",
            "keywords": ["deep learning", "battery", "SoC", "LSTM", "prediction"],
            "authors": ["ê¹€ì² ìˆ˜", "John Smith", "ë°•ì˜í¬"],
            "journal": "IEEE Transactions on Power Electronics",
            "year": "2023",
        },
        "author": {
            "id": "ê¹€ì² ìˆ˜",
            "name": "ê¹€ì² ìˆ˜",
            "paper_count": 15,
            "first_year": 2018,
            "last_year": 2023,
            "top_keywords": [
                ("machine learning", 8),
                ("battery", 6),
                ("optimization", 4),
            ],
            "most_frequent_journal": "IEEE Transactions on Industrial Electronics",
            "productivity_type": "Leading Researcher",
        },
        "keyword": {
            "id": "machine learning",
            "name": "machine learning",
            "frequency": 25,
            "papers": ["paper_1", "paper_2", "paper_3"],
        },
        "journal": {
            "id": "IEEE Transactions on Power Electronics",
            "name": "IEEE Transactions on Power Electronics",
            "paper_count": 120,
            "first_year": 2015,
            "last_year": 2023,
            "journal_type": "IEEE",
            "top_keywords": [
                ("power electronics", 45),
                ("battery", 32),
                ("control", 28),
            ],
            "unique_authors": 180,
        },
    }

    print("=" * 60)

    for node_type, data in test_data.items():
        print(f"\nğŸ“ Testing {node_type.upper()} processor:")
        print("-" * 40)

        processor = create_text_processor(node_type, max_length=200, language="mixed")
        result = processor.process_node(data)

        print(f"Input: {data}")
        print(f"Output: {result}")
        print(f"Length: {len(result.split())} words")

    print(f"\nâœ… All processors tested successfully!")
    print(f"ğŸ“‹ Supported node types: {get_supported_node_types()}")
