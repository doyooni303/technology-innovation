"""
GraphRAG ì¿¼ë¦¬ ë¶„ì„ ëª¨ë“ˆ
Query Analyzer for GraphRAG System

ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
- ì¿¼ë¦¬ ë³µì¡ë„ ìë™ íŒë‹¨
- í•„ìš”í•œ ê·¸ë˜í”„ ë…¸ë“œ íƒ€ì… ì‹ë³„
- ê²€ìƒ‰ ëª¨ë“œ ì¶”ì²œ
- í•œêµ­ì–´/ì˜ì–´ ì§€ì›
"""

import re
import json
import logging
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
from collections import Counter
import numpy as np

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class QueryComplexity(Enum):
    """ì¿¼ë¦¬ ë³µì¡ë„ ë ˆë²¨"""

    SIMPLE = "simple"  # ë‹¨ìˆœ ì¡°íšŒ (íŠ¹ì • ì €ì, ë…¼ë¬¸ ë“±)
    MEDIUM = "medium"  # ì¤‘ê°„ ë³µì¡ë„ (íŠ¸ë Œë“œ, íŒ¨í„´ ë¶„ì„)
    COMPLEX = "complex"  # ë³µì¡í•œ ë¶„ì„ (ë‹¤ì¤‘ í™‰, ì¢…í•© ë¶„ì„)
    EXPLORATORY = "exploratory"  # íƒìƒ‰ì  ë¶„ì„ (ì „ì²´ êµ¬ì¡°, ìˆ¨ê²¨ì§„ íŒ¨í„´)


class QueryType(Enum):
    """ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜"""

    CITATION_ANALYSIS = "citation_analysis"  # ì¸ìš© ë¶„ì„
    AUTHOR_ANALYSIS = "author_analysis"  # ì—°êµ¬ì ë¶„ì„
    KEYWORD_ANALYSIS = "keyword_analysis"  # í‚¤ì›Œë“œ/ì£¼ì œ ë¶„ì„
    JOURNAL_ANALYSIS = "journal_analysis"  # ì €ë„ ë¶„ì„
    TREND_ANALYSIS = "trend_analysis"  # íŠ¸ë Œë“œ ë¶„ì„
    COLLABORATION_ANALYSIS = "collaboration_analysis"  # í˜‘ì—… ë¶„ì„
    SIMILARITY_ANALYSIS = "similarity_analysis"  # ìœ ì‚¬ë„ ë¶„ì„
    COMPREHENSIVE_ANALYSIS = "comprehensive_analysis"  # ì¢…í•© ë¶„ì„
    FACTUAL_LOOKUP = "factual_lookup"  # ë‹¨ìˆœ ì¡°íšŒ
    COMPARISON = "comparison"  # ë¹„êµ ë¶„ì„


class SearchMode(Enum):
    """ê²€ìƒ‰ ëª¨ë“œ"""

    LOCAL = "local"  # íŠ¹ì • ì—”í‹°í‹° ì¤‘ì‹¬ ê²€ìƒ‰
    GLOBAL = "global"  # ì „ì—­ íŒ¨í„´ ë¶„ì„
    HYBRID = "hybrid"  # í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼


class NodeType(Enum):
    """ê·¸ë˜í”„ ë…¸ë“œ íƒ€ì…"""

    PAPER = "paper"
    AUTHOR = "author"
    KEYWORD = "keyword"
    JOURNAL = "journal"


@dataclass
class QueryAnalysisResult:
    """ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼"""

    # ê¸°ë³¸ ì •ë³´
    original_query: str
    processed_query: str
    language: str

    # ë¶„ë¥˜ ê²°ê³¼
    complexity: QueryComplexity
    query_type: QueryType
    search_mode: SearchMode

    # í•„ìš” ë¦¬ì†ŒìŠ¤
    required_node_types: Set[NodeType]
    required_edge_types: Set[str]
    estimated_scope: str  # "narrow", "medium", "broad"

    # ì¶”ì¶œëœ ì—”í‹°í‹°
    entities: Dict[str, List[str]]  # íƒ€ì…ë³„ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
    keywords: List[str]
    temporal_indicators: List[str]

    # ë©”íƒ€ë°ì´í„°
    confidence_score: float
    processing_hints: List[str]
    estimated_complexity_score: float
    suggested_timeout: int  # ì´ˆ ë‹¨ìœ„

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™” ìš©)"""
        result = asdict(self)

        # Enumë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        result["complexity"] = self.complexity.value
        result["query_type"] = self.query_type.value
        result["search_mode"] = self.search_mode.value
        result["required_node_types"] = [nt.value for nt in self.required_node_types]

        return result


class QueryAnalyzer:
    """GraphRAG ì¿¼ë¦¬ ë¶„ì„ê¸°"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Args:
            config: ë¶„ì„ê¸° ì„¤ì •
        """
        self.config = config or self._get_default_config()

        # ì–¸ì–´ë³„ íŒ¨í„´ ë¡œë“œ
        self._load_language_patterns()

        # ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ ë¡œë“œ
        self._load_domain_keywords()

        # ë³µì¡ë„ ë¶„ì„ ê°€ì¤‘ì¹˜
        self._load_complexity_weights()

        logger.info("âœ… QueryAnalyzer initialized")

    def _get_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            "supported_languages": ["ko", "en"],
            "default_language": "ko",
            "complexity_threshold": {
                "simple_max": 0.3,
                "medium_max": 0.6,
                "complex_max": 0.8,
            },
            "entity_extraction": {"max_entities_per_type": 10, "min_confidence": 0.5},
            "timeout_settings": {
                "simple": 10,
                "medium": 30,
                "complex": 120,
                "exploratory": 300,
            },
        }

    def _load_language_patterns(self):
        """ì–¸ì–´ë³„ íŒ¨í„´ ì •ì˜"""
        self.language_patterns = {
            "ko": {
                # ì§ˆë¬¸ íŒ¨í„´
                "question_patterns": [
                    r"ë¬´ì—‡",
                    r"ëˆ„êµ¬",
                    r"ì–¸ì œ",
                    r"ì–´ë””",
                    r"ì–´ë–»ê²Œ",
                    r"ì™œ",
                    r"ì–¼ë§ˆë‚˜",
                    r"ì–´ë–¤",
                    r"ëª‡",
                    r"ì–´ëŠ",
                    r"ë­",
                    r"ëˆ„ê°€",
                ],
                # ë³µì¡ë„ ì§€ì‹œì–´
                "complexity_indicators": {
                    "simple": [
                        r"ëˆ„êµ¬",
                        r"ì–¸ì œ",
                        r"ì–´ë””",
                        r"ëª‡",
                        r"ë¦¬ìŠ¤íŠ¸",
                        r"ëª©ë¡",
                        r"ì°¾ì•„",
                        r"ì•Œë ¤",
                        r"ë³´ì—¬",
                    ],
                    "medium": [
                        r"ë™í–¥",
                        r"íŠ¸ë Œë“œ",
                        r"íŒ¨í„´",
                        r"ë³€í™”",
                        r"ë¶„ì„",
                        r"ë¹„êµ",
                        r"ê´€ê³„",
                        r"ì˜í–¥",
                        r"ì°¨ì´",
                    ],
                    "complex": [
                        r"ì¢…í•©",
                        r"ì „ì²´ì ",
                        r"í¬ê´„ì ",
                        r"ìƒê´€ê´€ê³„",
                        r"ì¸ê³¼ê´€ê³„",
                        r"ì˜ˆì¸¡",
                        r"ëª¨ë¸",
                        r"ì‹œë®¬ë ˆì´ì…˜",
                    ],
                    "exploratory": [
                        r"ìˆ¨ê²¨ì§„",
                        r"ë°œê²¬",
                        r"íƒìƒ‰",
                        r"ìƒˆë¡œìš´",
                        r"í˜ì‹ ì ",
                        r"ì˜ˆìƒì¹˜ ëª»í•œ",
                        r"ë†€ë¼ìš´",
                        r"íŠ¹ì´í•œ",
                    ],
                },
                # ì¿¼ë¦¬ íƒ€ì… í‚¤ì›Œë“œ
                "query_type_keywords": {
                    "citation_analysis": [
                        r"ì¸ìš©",
                        r"ì°¸ì¡°",
                        r"ì˜í–¥ë ¥",
                        r"í”¼ì¸ìš©",
                        r"h-index",
                    ],
                    "author_analysis": [
                        r"ì €ì",
                        r"ì—°êµ¬ì",
                        r"êµìˆ˜",
                        r"ë°•ì‚¬",
                        r"ì—°êµ¬ì§„",
                        r"íŒ€",
                    ],
                    "keyword_analysis": [r"í‚¤ì›Œë“œ", r"ì£¼ì œ", r"í† í”½", r"ìš©ì–´", r"ê°œë…"],
                    "collaboration_analysis": [
                        r"í˜‘ì—…",
                        r"ê³µë™ì—°êµ¬",
                        r"í˜‘ë ¥",
                        r"íŒŒíŠ¸ë„ˆì‹­",
                        r"ë„¤íŠ¸ì›Œí¬",
                    ],
                    "trend_analysis": [
                        r"ë™í–¥",
                        r"íŠ¸ë Œë“œ",
                        r"ë³€í™”",
                        r"ë°œì „",
                        r"ì§„í™”",
                        r"ì„±ì¥",
                    ],
                },
            },
            "en": {
                "question_patterns": [
                    r"what",
                    r"who",
                    r"when",
                    r"where",
                    r"how",
                    r"why",
                    r"which",
                    r"whose",
                    r"whom",
                ],
                "complexity_indicators": {
                    "simple": [
                        r"who",
                        r"when",
                        r"where",
                        r"list",
                        r"show",
                        r"find",
                        r"tell",
                        r"give",
                    ],
                    "medium": [
                        r"trend",
                        r"pattern",
                        r"change",
                        r"analyze",
                        r"compare",
                        r"relationship",
                        r"influence",
                        r"impact",
                        r"difference",
                    ],
                    "complex": [
                        r"comprehensive",
                        r"overall",
                        r"correlation",
                        r"causation",
                        r"predict",
                        r"model",
                        r"simulate",
                        r"synthesize",
                    ],
                    "exploratory": [
                        r"hidden",
                        r"discover",
                        r"explore",
                        r"novel",
                        r"innovative",
                        r"unexpected",
                        r"surprising",
                        r"unusual",
                        r"emerging",
                    ],
                },
                "query_type_keywords": {
                    "citation_analysis": [
                        r"citation",
                        r"reference",
                        r"impact",
                        r"cited",
                        r"h-index",
                    ],
                    "author_analysis": [
                        r"author",
                        r"researcher",
                        r"professor",
                        r"scientist",
                        r"team",
                    ],
                    "keyword_analysis": [
                        r"keyword",
                        r"topic",
                        r"subject",
                        r"term",
                        r"concept",
                    ],
                    "collaboration_analysis": [
                        r"collaboration",
                        r"cooperation",
                        r"partnership",
                        r"network",
                    ],
                    "trend_analysis": [
                        r"trend",
                        r"change",
                        r"development",
                        r"evolution",
                        r"growth",
                    ],
                },
            },
        }

    def _load_domain_keywords(self):
        """í•™ìˆ  ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ"""
        self.domain_keywords = {
            # ë°°í„°ë¦¬/ì „ê¸°ì°¨ ë„ë©”ì¸
            "battery_ev": [
                "battery",
                "ë°°í„°ë¦¬",
                "lithium",
                "ë¦¬íŠ¬",
                "soc",
                "ìƒíƒœ",
                "electric vehicle",
                "ì „ê¸°ì°¨",
                "ev",
                "charging",
                "ì¶©ì „",
            ],
            # AI/ML ë„ë©”ì¸
            "ai_ml": [
                "machine learning",
                "ë¨¸ì‹ ëŸ¬ë‹",
                "deep learning",
                "ë”¥ëŸ¬ë‹",
                "artificial intelligence",
                "ì¸ê³µì§€ëŠ¥",
                "neural network",
                "ì‹ ê²½ë§",
            ],
            # í•™ìˆ  ìš©ì–´
            "academic": [
                "research",
                "ì—°êµ¬",
                "paper",
                "ë…¼ë¬¸",
                "journal",
                "ì €ë„",
                "conference",
                "í•™íšŒ",
                "publication",
                "ì¶œíŒ",
            ],
        }

    def _load_complexity_weights(self):
        """ë³µì¡ë„ ê³„ì‚° ê°€ì¤‘ì¹˜"""
        self.complexity_weights = {
            "query_length": 0.1,  # ì¿¼ë¦¬ ê¸¸ì´
            "question_words": 0.15,  # ì˜ë¬¸ì‚¬ ê°œìˆ˜
            "complexity_terms": 0.25,  # ë³µì¡ë„ ì§€ì‹œì–´
            "entity_count": 0.2,  # ì—”í‹°í‹° ê°œìˆ˜
            "logical_operators": 0.15,  # ë…¼ë¦¬ ì—°ì‚°ì (ê·¸ë¦¬ê³ , ë˜ëŠ” ë“±)
            "temporal_scope": 0.15,  # ì‹œê°„ì  ë²”ìœ„
        }

    def detect_language(self, query: str) -> str:
        """ê°œì„ ëœ ì¿¼ë¦¬ ì–¸ì–´ ê°ì§€ (í˜¼ìš© í…ìŠ¤íŠ¸ ì§€ì›)"""
        # 1. ê¸°ë³¸ ë¬¸ì í†µê³„
        korean_chars = len(re.findall(r"[ê°€-í£]", query))
        english_chars = len(re.findall(r"[a-zA-Z]", query))
        total_chars = korean_chars + english_chars

        if total_chars == 0:
            return self.config["default_language"]

        korean_ratio = korean_chars / total_chars

        # 2. ì–¸ì–´ë³„ í•µì‹¬ íŒ¨í„´ ë§¤ì¹­
        korean_patterns = [
            r"[ê°€-í£]+(?:êµìˆ˜|ë°•ì‚¬|ì—°êµ¬ì›|ì €ì)",  # í•œêµ­ì–´ ì§ì±…
            r"[ê°€-í£]+(?:ì˜|ì´|ê°€|ì„|ë¥¼|ì—ì„œ)",  # í•œêµ­ì–´ ì¡°ì‚¬
            r"(?:ë¬´ì—‡|ëˆ„êµ¬|ì–¸ì œ|ì–´ë””|ì–´ë–»ê²Œ|ì™œ)",  # í•œêµ­ì–´ ì˜ë¬¸ì‚¬
            r"(?:ë™í–¥|íŠ¸ë Œë“œ|ë¶„ì„|ì—°êµ¬|ë…¼ë¬¸)",  # í•œêµ­ì–´ í•™ìˆ ìš©ì–´
        ]

        english_patterns = [
            r"\b(?:what|who|when|where|how|why)\b",  # ì˜ì–´ ì˜ë¬¸ì‚¬
            r"\b(?:analysis|research|trend|paper)\b",  # ì˜ì–´ í•™ìˆ ìš©ì–´
            r"\b(?:Dr|Prof|Professor)\s+[A-Z][a-z]+",  # ì˜ì–´ ì§ì±…
            r"\b[A-Z][a-z]+\s+(?:et\s+al|and\s+[A-Z])",  # ì˜ì–´ ì €ì íŒ¨í„´
        ]

        korean_pattern_score = 0
        english_pattern_score = 0

        for pattern in korean_patterns:
            korean_pattern_score += len(re.findall(pattern, query))

        for pattern in english_patterns:
            english_pattern_score += len(re.findall(pattern, query, re.IGNORECASE))

        # 3. ë„ë©”ì¸ ì „ë¬¸ìš©ì–´ ê³ ë ¤
        domain_terms = {
            "technical_english": [
                "machine learning",
                "deep learning",
                "neural network",
                "SoC",
                "IoT",
                "AI",
                "ML",
                "CNN",
                "LSTM",
                "RNN",
                "battery",
                "lithium",
                "charging",
                "electric vehicle",
            ],
            "korean_academic": [
                "ì—°êµ¬",
                "ë…¼ë¬¸",
                "ë¶„ì„",
                "ê°œë°œ",
                "ê¸°ìˆ ",
                "ì‹œìŠ¤í…œ",
                "ì•Œê³ ë¦¬ì¦˜",
                "ëª¨ë¸",
                "ë°ì´í„°",
                "ì„±ëŠ¥",
            ],
        }

        technical_english_count = 0
        for term in domain_terms["technical_english"]:
            if term.lower() in query.lower():
                technical_english_count += 1

        korean_academic_count = 0
        for term in domain_terms["korean_academic"]:
            if term in query:
                korean_academic_count += 1

        # 4. ì¢…í•© íŒë‹¨ ë¡œì§
        # í˜¼ìš© í…ìŠ¤íŠ¸ ê°ì§€
        is_mixed = (
            korean_ratio > 0.1
            and korean_ratio < 0.9
            and technical_english_count > 0
            and korean_academic_count > 0
        )

        if is_mixed:
            # í˜¼ìš©ì¸ ê²½ìš° ì£¼ìš” ì–¸ì–´ íŒ¨í„´ìœ¼ë¡œ ê²°ì •
            if korean_pattern_score > english_pattern_score:
                return "ko"  # í•œêµ­ì–´ ì£¼ë„
            elif english_pattern_score > korean_pattern_score:
                return "en"  # ì˜ì–´ ì£¼ë„
            else:
                # íŒ¨í„´ ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ë¬¸ì ë¹„ìœ¨ë¡œ ê²°ì •
                return "ko" if korean_ratio >= 0.5 else "en"

        # 5. ìˆœìˆ˜ ì–¸ì–´ì¸ ê²½ìš°
        if korean_ratio > 0.7:
            return "ko"
        elif korean_ratio < 0.3:
            return "en"
        else:
            # ì• ë§¤í•œ ê²½ìš° íŒ¨í„´ ì ìˆ˜ë¡œ ê²°ì •
            total_korean_score = korean_pattern_score + korean_academic_count
            total_english_score = english_pattern_score + technical_english_count

            if total_korean_score > total_english_score:
                return "ko"
            elif total_english_score > total_korean_score:
                return "en"
            else:
                # ìµœì¢…ì ìœ¼ë¡œ ë¬¸ì ë¹„ìœ¨ë¡œ ê²°ì •
                return "ko" if korean_ratio >= 0.5 else "en"

    def preprocess_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ ì •ì œ
        processed = query.strip()

        # ì—°ì† ê³µë°± ì œê±°
        processed = re.sub(r"\s+", " ", processed)

        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ì˜ë¯¸ìˆëŠ” ê²ƒë“¤ì€ ë³´ì¡´)
        processed = re.sub(r"[^\w\s\?\!\.\,\(\)\-]", " ", processed)

        return processed

    def extract_entities(self, query: str, language: str) -> Dict[str, List[str]]:
        """í˜¼ìš© í…ìŠ¤íŠ¸ ì§€ì› ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {
            "authors": [],
            "papers": [],
            "keywords": [],
            "journals": [],
            "years": [],
            "institutions": [],
        }

        # ë…„ë„ ì¶”ì¶œ (ì–¸ì–´ ë¬´ê´€)
        years = re.findall(r"\b(19|20)\d{2}\b", query)
        entities["years"] = years

        # í˜¼ìš© í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë‹¤ì¤‘ íŒ¨í„´ ì ìš©
        korean_chars = len(re.findall(r"[ê°€-í£]", query))
        english_chars = len(re.findall(r"[a-zA-Z]", query))
        is_mixed = korean_chars > 0 and english_chars > 0

        # ì €ìëª… íŒ¨í„´ (í˜¼ìš© ê³ ë ¤)
        author_patterns = []

        if language == "ko" or is_mixed:
            # í•œêµ­ì–´ ì €ì íŒ¨í„´
            author_patterns.extend(
                [
                    r"([ê°€-í£]{2,4})\s*(?:êµìˆ˜|ë°•ì‚¬|ì—°êµ¬ì›|ì €ì)",
                    r"([ê°€-í£]{2,4})\s*(?:ë“±|ì™¸)",
                    r"([ê°€-í£]{2,4})\s*(?:ì˜|ì´|ê°€)\s*ì—°êµ¬",
                ]
            )

        if language == "en" or is_mixed:
            # ì˜ì–´ ì €ì íŒ¨í„´
            author_patterns.extend(
                [
                    r"([A-Z][a-z]+\s+[A-Z][a-z]+)",
                    r"([A-Z]\.\s*[A-Z][a-z]+)",
                    r"Dr\.\s*([A-Z][a-z]+\s+[A-Z][a-z]+)",
                    r"Prof\.\s*([A-Z][a-z]+\s+[A-Z][a-z]+)",
                    r"Professor\s+([A-Z][a-z]+\s+[A-Z][a-z]+)",
                ]
            )

        # í˜¼ìš© íŒ¨í„´ (í•œì˜ ì¡°í•©)
        if is_mixed:
            author_patterns.extend(
                [
                    r"([ê°€-í£]{2,4})\s+(?:Dr|Prof|Professor)",  # ê¹€ì² ìˆ˜ Dr
                    r"(?:Dr|Prof|Professor)\s+([ê°€-í£]{2,4})",  # Dr ê¹€ì² ìˆ˜
                    r"([A-Z][a-z]+\s+[A-Z][a-z]+)\s*(?:êµìˆ˜|ë°•ì‚¬)",  # John Smith êµìˆ˜
                ]
            )

        for pattern in author_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities["authors"].extend(matches)

        # ì €ë„ëª… íŒ¨í„´ (í˜¼ìš© ê³ ë ¤)
        journal_patterns = [
            r"(IEEE\s+[A-Za-z\s]+)",
            r"(Nature\s+[A-Za-z\s]+)",
            r"(Journal\s+of\s+[A-Za-z\s]+)",
            r"([ê°€-í£\s]*í•™íšŒì§€)",
            r"([ê°€-í£\s]*ì €ë„)",
        ]

        for pattern in journal_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities["journals"].extend(matches)

        # ê¸°ê´€ëª… íŒ¨í„´
        institution_patterns = [
            r"([ê°€-í£]+ëŒ€í•™êµ?)",
            r"([ê°€-í£]+ì—°êµ¬ì†Œ)",
            r"([A-Z][a-z]+\s+University)",
            r"([A-Z][a-z]+\s+Institute)",
            r"(MIT|Stanford|Harvard|KAIST|ì„œìš¸ëŒ€|ì—°ì„¸ëŒ€|ê³ ë ¤ëŒ€)",
        ]

        for pattern in institution_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities["institutions"].extend(matches)

        # ë„ë©”ì¸ í‚¤ì›Œë“œ ë§¤ì¹­ (ë‹¤êµ­ì–´)
        query_lower = query.lower()
        for domain, keywords in self.domain_keywords.items():
            for keyword in keywords:
                if keyword.lower() in query_lower:
                    entities["keywords"].append(keyword)

        # ì¶”ê°€ ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ (í˜¼ìš© ê³ ë ¤)
        technical_terms = [
            # ì˜ì–´ ê¸°ìˆ ìš©ì–´
            r"\b(machine\s+learning|deep\s+learning|neural\s+network)\b",
            r"\b(artificial\s+intelligence|reinforcement\s+learning)\b",
            r"\b(battery|lithium|charging|electric\s+vehicle)\b",
            r"\b(SoC|IoT|AI|ML|CNN|LSTM|RNN|GPU)\b",
            # í•œêµ­ì–´ ê¸°ìˆ ìš©ì–´
            r"(ë¨¸ì‹ \s*ëŸ¬ë‹|ë”¥\s*ëŸ¬ë‹|ì‹ ê²½ë§)",
            r"(ì¸ê³µì§€ëŠ¥|ê°•í™”í•™ìŠµ|ì „ê¸°ì°¨)",
            r"(ë°°í„°ë¦¬|ë¦¬íŠ¬|ì¶©ì „|ììœ¨ì£¼í–‰)",
        ]

        for pattern in technical_terms:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities["keywords"].extend([match for match in matches if match])

        # ì¤‘ë³µ ì œê±° ë° ì •ì œ
        for key in entities:
            if key == "keywords":
                # í‚¤ì›Œë“œëŠ” ì†Œë¬¸ìë¡œ ì •ê·œí™”
                entities[key] = list(
                    set([kw.lower().strip() for kw in entities[key] if kw.strip()])
                )
            else:
                entities[key] = list(
                    set([item.strip() for item in entities[key] if item.strip()])
                )

        return entities

    def calculate_complexity_score(
        self, query: str, language: str, entities: Dict[str, List[str]]
    ) -> float:
        """í˜¼ìš© í…ìŠ¤íŠ¸ ì§€ì› ì¿¼ë¦¬ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„)"""
        scores = {}

        # í˜¼ìš© í…ìŠ¤íŠ¸ ê°ì§€
        korean_chars = len(re.findall(r"[ê°€-í£]", query))
        english_chars = len(re.findall(r"[a-zA-Z]", query))
        is_mixed = korean_chars > 0 and english_chars > 0

        # 1. ì¿¼ë¦¬ ê¸¸ì´ ì ìˆ˜
        length_score = min(1.0, len(query.split()) / 20)
        scores["query_length"] = length_score

        # 2. ì˜ë¬¸ì‚¬ ê°œìˆ˜ (ë‹¤êµ­ì–´ ì§€ì›)
        question_words = 0

        # ê¸°ë³¸ ì–¸ì–´ íŒ¨í„´
        patterns = self.language_patterns[language]
        for pattern in patterns["question_patterns"]:
            question_words += len(re.findall(pattern, query, re.IGNORECASE))

        # í˜¼ìš©ì¸ ê²½ìš° ë‹¤ë¥¸ ì–¸ì–´ íŒ¨í„´ë„ ì ìš©
        if is_mixed:
            other_language = "en" if language == "ko" else "ko"
            if other_language in self.language_patterns:
                other_patterns = self.language_patterns[other_language]
                for pattern in other_patterns["question_patterns"]:
                    question_words += len(re.findall(pattern, query, re.IGNORECASE))

        scores["question_words"] = min(1.0, question_words / 3)

        # 3. ë³µì¡ë„ ì§€ì‹œì–´ ì ìˆ˜ (ë‹¤êµ­ì–´)
        complexity_score = 0

        # ê¸°ë³¸ ì–¸ì–´ì˜ ë³µì¡ë„ ì§€ì‹œì–´
        for level, terms in patterns["complexity_indicators"].items():
            weight = {"simple": 0.2, "medium": 0.5, "complex": 0.8, "exploratory": 1.0}[
                level
            ]
            for term in terms:
                if re.search(term, query, re.IGNORECASE):
                    complexity_score = max(complexity_score, weight)

        # í˜¼ìš©ì¸ ê²½ìš° ë‹¤ë¥¸ ì–¸ì–´ íŒ¨í„´ë„ í™•ì¸
        if is_mixed:
            other_language = "en" if language == "ko" else "ko"
            if other_language in self.language_patterns:
                other_patterns = self.language_patterns[other_language]
                for level, terms in other_patterns["complexity_indicators"].items():
                    weight = {
                        "simple": 0.2,
                        "medium": 0.5,
                        "complex": 0.8,
                        "exploratory": 1.0,
                    }[level]
                    for term in terms:
                        if re.search(term, query, re.IGNORECASE):
                            complexity_score = max(complexity_score, weight)

        # ì¶”ê°€ ë³µì¡ë„ ì§€ì‹œì–´ (ë„ë©”ì¸ íŠ¹í™”)
        advanced_terms = [
            # ê³ ê¸‰ ë¶„ì„ ìš©ì–´
            r"(?:comprehensive|ì „ì²´ì |overall|ì¢…í•©)",
            r"(?:correlation|ìƒê´€ê´€ê³„|causation|ì¸ê³¼ê´€ê³„)",
            r"(?:prediction|ì˜ˆì¸¡|forecasting|ì „ë§)",
            r"(?:network\s+analysis|ë„¤íŠ¸ì›Œí¬\s*ë¶„ì„)",
            r"(?:hidden\s+pattern|ìˆ¨ê²¨ì§„\s*íŒ¨í„´)",
            r"(?:deep\s+analysis|ì‹¬ì¸µ\s*ë¶„ì„)",
            # ë‹¤ì¤‘ ê°œë… ì¡°í•©
            r"(?:and|ê·¸ë¦¬ê³ |ë˜í•œ|ë¿ë§Œ\s*ì•„ë‹ˆë¼)",
            r"(?:both|ë‘˜\s*ë‹¤|ëª¨ë‘)",
            r"(?:relationship\s+between|ê´€ê³„|ì‚¬ì´)",
        ]

        for term_pattern in advanced_terms:
            if re.search(term_pattern, query, re.IGNORECASE):
                complexity_score = max(complexity_score, 0.7)

        scores["complexity_terms"] = complexity_score

        # 4. ì—”í‹°í‹° ê°œìˆ˜ (ë‹¤ì–‘ì„± ê³ ë ¤)
        total_entities = sum(len(ents) for ents in entities.values())
        entity_types = sum(
            1 for ents in entities.values() if ents
        )  # ì—”í‹°í‹° íƒ€ì… ë‹¤ì–‘ì„±

        entity_score = min(1.0, total_entities / 10)
        # ì—”í‹°í‹° íƒ€ì…ì´ ë‹¤ì–‘í•˜ë©´ ë³µì¡ë„ ì¦ê°€
        if entity_types >= 3:
            entity_score = min(1.0, entity_score * 1.3)

        scores["entity_count"] = entity_score

        # 5. ë…¼ë¦¬ ì—°ì‚°ì (ë‹¤êµ­ì–´)
        logical_ops = [
            # í•œêµ­ì–´
            r"(?:ê·¸ë¦¬ê³ |ë˜í•œ|ë˜|ë”ë¶ˆì–´|ë¿ë§Œ\s*ì•„ë‹ˆë¼)",
            r"(?:ë˜ëŠ”|í˜¹ì€|ì•„ë‹ˆë©´)",
            r"(?:í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ê·¸ëŸ°ë°|ë°˜ë©´)",
            r"(?:ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ|ê²°ê³¼ì ìœ¼ë¡œ)",
            # ì˜ì–´
            r"\b(?:and|also|furthermore|moreover)\b",
            r"\b(?:or|either|alternatively)\b",
            r"\b(?:but|however|nevertheless|whereas)\b",
            r"\b(?:therefore|thus|consequently)\b",
        ]

        logical_count = 0
        for op_pattern in logical_ops:
            logical_count += len(re.findall(op_pattern, query, re.IGNORECASE))

        scores["logical_operators"] = min(1.0, logical_count / 3)

        # 6. ì‹œê°„ì  ë²”ìœ„ (ë‹¤êµ­ì–´)
        temporal_indicators = [
            # í•œêµ­ì–´ ì‹œê°„ ì§€ì‹œì–´
            r"(?:ìµœê·¼|ìš”ì¦˜|ê·¼ë˜|ì§€ê¸ˆ)",
            r"(?:ê³¼ê±°|ì˜ˆì „|ì´ì „|ì „ì—)",
            r"(?:ë¯¸ë˜|ì•ìœ¼ë¡œ|í–¥í›„|ì¥ë˜)",
            r"(?:ë³€í™”|ë°œì „|ì§„í™”|ì¶”ì´)",
            r"(?:ë™í–¥|íŠ¸ë Œë“œ|ê²½í–¥)",
            r"(?:ì—­ì‚¬|ë°œë‹¬ê³¼ì •|ë³€ì²œ)",
            # ì˜ì–´ ì‹œê°„ ì§€ì‹œì–´
            r"\b(?:recent|recently|current|now|today)\b",
            r"\b(?:past|previous|former|before|ago)\b",
            r"\b(?:future|upcoming|next|coming)\b",
            r"\b(?:change|development|evolution|progress)\b",
            r"\b(?:trend|tendency|pattern)\b",
            r"\b(?:history|historical|timeline)\b",
        ]

        temporal_score = 0
        temporal_count = 0
        for indicator_pattern in temporal_indicators:
            matches = re.findall(indicator_pattern, query, re.IGNORECASE)
            if matches:
                temporal_count += len(matches)

        if temporal_count > 0:
            temporal_score = min(1.0, 0.4 + (temporal_count * 0.2))

        scores["temporal_scope"] = temporal_score

        # 7. í˜¼ìš© í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ (í˜¼ìš© ìì²´ê°€ ë³µì¡ì„±ì„ ë‚˜íƒ€ëƒ„)
        if is_mixed:
            # í˜¼ìš© í…ìŠ¤íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë” ë³µì¡í•œ ê°œë…ì„ ë‹¤ë£¸
            mixed_bonus = 0.1
            for key in scores:
                scores[key] = min(1.0, scores[key] + mixed_bonus)

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_score = sum(scores[key] * self.complexity_weights[key] for key in scores)

        return min(1.0, total_score)

    def classify_query_type(
        self, query: str, language: str, entities: Dict[str, List[str]]
    ) -> QueryType:
        """ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜"""
        query_lower = query.lower()
        patterns = self.language_patterns[language]

        # ê° íƒ€ì…ë³„ ì ìˆ˜ ê³„ì‚°
        type_scores = {}

        for query_type, keywords in patterns["query_type_keywords"].items():
            score = 0
            for keyword in keywords:
                if re.search(keyword, query_lower):
                    score += 1
            type_scores[query_type] = score

        # ì—”í‹°í‹° ê¸°ë°˜ ì¶”ê°€ ì ìˆ˜
        if entities["authors"]:
            type_scores["author_analysis"] = type_scores.get("author_analysis", 0) + 2

        if entities["keywords"]:
            type_scores["keyword_analysis"] = type_scores.get("keyword_analysis", 0) + 2

        if entities["years"]:
            type_scores["trend_analysis"] = type_scores.get("trend_analysis", 0) + 1

        # íŠ¹ë³„ íŒ¨í„´ ê²€ì‚¬
        comprehensive_patterns = [
            r"ì¢…í•©",
            r"ì „ì²´",
            r"ëª¨ë“ ",
            r"ì „ë°˜ì ",
            r"overall",
            r"comprehensive",
            r"all",
        ]

        for pattern in comprehensive_patterns:
            if re.search(pattern, query_lower):
                type_scores["comprehensive_analysis"] = (
                    type_scores.get("comprehensive_analysis", 0) + 3
                )

        # ìµœê³  ì ìˆ˜ íƒ€ì… ì„ íƒ
        if not type_scores or max(type_scores.values()) == 0:
            return QueryType.FACTUAL_LOOKUP

        best_type = max(type_scores.items(), key=lambda x: x[1])[0]
        return QueryType(best_type)

    def determine_complexity(self, complexity_score: float) -> QueryComplexity:
        """ë³µì¡ë„ ì ìˆ˜ë¡œë¶€í„° ë³µì¡ë„ ë ˆë²¨ ê²°ì •"""
        thresholds = self.config["complexity_threshold"]

        if complexity_score <= thresholds["simple_max"]:
            return QueryComplexity.SIMPLE
        elif complexity_score <= thresholds["medium_max"]:
            return QueryComplexity.MEDIUM
        elif complexity_score <= thresholds["complex_max"]:
            return QueryComplexity.COMPLEX
        else:
            return QueryComplexity.EXPLORATORY

    def determine_required_resources(
        self, query_type: QueryType, entities: Dict[str, List[str]]
    ) -> Tuple[Set[NodeType], Set[str]]:
        """í•„ìš”í•œ ë…¸ë“œ/ì—£ì§€ íƒ€ì… ê²°ì •"""

        # ì¿¼ë¦¬ íƒ€ì…ë³„ ê¸°ë³¸ ë¦¬ì†ŒìŠ¤
        type_resources = {
            QueryType.CITATION_ANALYSIS: {
                "nodes": {NodeType.PAPER},
                "edges": {"cites", "semantically_similar_to"},
            },
            QueryType.AUTHOR_ANALYSIS: {
                "nodes": {NodeType.AUTHOR, NodeType.PAPER},
                "edges": {"authored_by", "collaborates_with"},
            },
            QueryType.KEYWORD_ANALYSIS: {
                "nodes": {NodeType.KEYWORD, NodeType.PAPER},
                "edges": {"has_keyword", "co_occurs_with"},
            },
            QueryType.COLLABORATION_ANALYSIS: {
                "nodes": {NodeType.AUTHOR, NodeType.PAPER},
                "edges": {"collaborates_with", "authored_by"},
            },
            QueryType.TREND_ANALYSIS: {
                "nodes": {NodeType.KEYWORD, NodeType.PAPER, NodeType.AUTHOR},
                "edges": {"has_keyword", "temporal_proximity", "authored_by"},
            },
            QueryType.COMPREHENSIVE_ANALYSIS: {
                "nodes": {
                    NodeType.PAPER,
                    NodeType.AUTHOR,
                    NodeType.KEYWORD,
                    NodeType.JOURNAL,
                },
                "edges": {
                    "cites",
                    "authored_by",
                    "has_keyword",
                    "collaborates_with",
                    "co_occurs_with",
                    "published_in",
                },
            },
        }

        # ê¸°ë³¸ ë¦¬ì†ŒìŠ¤
        resources = type_resources.get(
            query_type, {"nodes": {NodeType.PAPER}, "edges": {"cites"}}
        )

        required_nodes = set(resources["nodes"])
        required_edges = set(resources["edges"])

        # ì—”í‹°í‹° ê¸°ë°˜ ì¶”ê°€ ë¦¬ì†ŒìŠ¤
        if entities["authors"]:
            required_nodes.add(NodeType.AUTHOR)
            required_edges.update(["authored_by", "collaborates_with"])

        if entities["keywords"]:
            required_nodes.add(NodeType.KEYWORD)
            required_edges.update(["has_keyword", "co_occurs_with"])

        if entities["years"]:
            required_edges.add("temporal_proximity")

        return required_nodes, required_edges

    def determine_search_mode(
        self,
        complexity: QueryComplexity,
        query_type: QueryType,
        entities: Dict[str, List[str]],
    ) -> SearchMode:
        """ê²€ìƒ‰ ëª¨ë“œ ê²°ì •"""

        # íŠ¹ì • ì—”í‹°í‹°ê°€ ë§ì´ ì–¸ê¸‰ë˜ë©´ LOCAL
        total_specific_entities = len(entities["authors"]) + len(entities["papers"])

        if total_specific_entities >= 2:
            return SearchMode.LOCAL

        # íƒìƒ‰ì ì´ê±°ë‚˜ ì¢…í•©ì  ë¶„ì„ì´ë©´ GLOBAL
        if (
            complexity == QueryComplexity.EXPLORATORY
            or query_type == QueryType.COMPREHENSIVE_ANALYSIS
        ):
            return SearchMode.GLOBAL

        # íŠ¸ë Œë“œë‚˜ íŒ¨í„´ ë¶„ì„ì´ë©´ HYBRID
        if query_type in [QueryType.TREND_ANALYSIS, QueryType.COLLABORATION_ANALYSIS]:
            return SearchMode.HYBRID

        # ê¸°ë³¸ê°’
        return SearchMode.LOCAL if total_specific_entities > 0 else SearchMode.GLOBAL

    def generate_processing_hints(self, result: QueryAnalysisResult) -> List[str]:
        """ì²˜ë¦¬ íŒíŠ¸ ìƒì„±"""
        hints = []

        # ë³µì¡ë„ë³„ íŒíŠ¸
        if result.complexity == QueryComplexity.SIMPLE:
            hints.append("Direct entity lookup recommended")
        elif result.complexity == QueryComplexity.EXPLORATORY:
            hints.append("Consider community detection algorithms")
            hints.append("Enable broad graph traversal")

        # ì¿¼ë¦¬ íƒ€ì…ë³„ íŒíŠ¸
        if result.query_type == QueryType.TREND_ANALYSIS:
            hints.append("Include temporal analysis")
            hints.append("Consider time-series aggregation")

        if result.query_type == QueryType.COLLABORATION_ANALYSIS:
            hints.append("Focus on author-author relationships")
            hints.append("Calculate network centrality metrics")

        # ë¦¬ì†ŒìŠ¤ë³„ íŒíŠ¸
        if NodeType.AUTHOR in result.required_node_types:
            hints.append("Include author disambiguation")

        if len(result.required_node_types) > 2:
            hints.append("Use multi-type graph traversal")

        return hints

    def analyze(self, query: str) -> QueryAnalysisResult:
        """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
        logger.info(f"ğŸ” Analyzing query: {query[:50]}...")

        # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
        language = self.detect_language(query)
        processed_query = self.preprocess_query(query)

        # 2. ì—”í‹°í‹° ì¶”ì¶œ
        entities = self.extract_entities(processed_query, language)

        # 3. ë³µì¡ë„ ê³„ì‚°
        complexity_score = self.calculate_complexity_score(
            processed_query, language, entities
        )
        complexity = self.determine_complexity(complexity_score)

        # 4. ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜
        query_type = self.classify_query_type(processed_query, language, entities)

        # 5. í•„ìš” ë¦¬ì†ŒìŠ¤ ê²°ì •
        required_nodes, required_edges = self.determine_required_resources(
            query_type, entities
        )

        # 6. ê²€ìƒ‰ ëª¨ë“œ ê²°ì •
        search_mode = self.determine_search_mode(complexity, query_type, entities)

        # 7. ë²”ìœ„ ì¶”ì •
        if len(entities["authors"]) + len(entities["papers"]) > 3:
            estimated_scope = "narrow"
        elif complexity in [QueryComplexity.COMPLEX, QueryComplexity.EXPLORATORY]:
            estimated_scope = "broad"
        else:
            estimated_scope = "medium"

        # 8. í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœí™”)
        keywords = (
            entities["keywords"]
            + [
                word
                for word in processed_query.split()
                if len(word) > 3 and word.isalpha()
            ][:10]
        )  # ìµœëŒ€ 10ê°œ

        # 9. ì‹œê°„ ì§€ì‹œì–´
        temporal_indicators = entities["years"] + [
            word
            for word in ["ìµœê·¼", "ê³¼ê±°", "ë¯¸ë˜", "recent", "past", "future"]
            if word in processed_query.lower()
        ]

        # 10. ì‹ ë¢°ë„ ë° íƒ€ì„ì•„ì›ƒ ê³„ì‚°
        confidence_score = min(1.0, 0.5 + complexity_score * 0.5)
        suggested_timeout = self.config["timeout_settings"][complexity.value]

        # 11. ê²°ê³¼ êµ¬ì„±
        result = QueryAnalysisResult(
            original_query=query,
            processed_query=processed_query,
            language=language,
            complexity=complexity,
            query_type=query_type,
            search_mode=search_mode,
            required_node_types=required_nodes,
            required_edge_types=required_edges,
            estimated_scope=estimated_scope,
            entities=entities,
            keywords=keywords,
            temporal_indicators=temporal_indicators,
            confidence_score=confidence_score,
            processing_hints=[],
            estimated_complexity_score=complexity_score,
            suggested_timeout=suggested_timeout,
        )

        # 12. ì²˜ë¦¬ íŒíŠ¸ ìƒì„±
        result.processing_hints = self.generate_processing_hints(result)

        logger.info(f"âœ… Analysis complete: {complexity.value} {query_type.value}")
        return result

    def batch_analyze(self, queries: List[str]) -> List[QueryAnalysisResult]:
        """ì—¬ëŸ¬ ì¿¼ë¦¬ ì¼ê´„ ë¶„ì„"""
        logger.info(f"ğŸ“Š Batch analyzing {len(queries)} queries...")

        results = []
        for i, query in enumerate(queries):
            logger.info(f"Processing query {i+1}/{len(queries)}")
            result = self.analyze(query)
            results.append(result)

        return results


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í˜¼ìš© í…ìŠ¤íŠ¸ í¬í•¨)"""
    analyzer = QueryAnalyzer()

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤ (í˜¼ìš© í…ìŠ¤íŠ¸ í¬í•¨)
    test_queries = [
        # ìˆœìˆ˜ í•œêµ­ì–´
        "ê¹€ì² ìˆ˜ êµìˆ˜ì˜ ì—°êµ¬ ë…¼ë¬¸ ëª©ë¡ì„ ë³´ì—¬ì¤˜",
        "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì˜ ë™í–¥ì€?",
        # ìˆœìˆ˜ ì˜ì–´
        "Who are the most cited authors in battery research?",
        "What are the recent trends in electric vehicle charging technology?",
        # í˜¼ìš© í…ìŠ¤íŠ¸ (í•œê¸€ ì£¼ë„)
        "ê¹€ì² ìˆ˜ êµìˆ˜ì˜ machine learning ì—°êµ¬ ì‹¤ì ì€?",
        "battery SoC predictionì— ëŒ€í•œ í•œêµ­ ì—°êµ¬ìë“¤ì˜ ë™í–¥ì€?",
        "IEEE journalì— ë°œí‘œëœ ë”¥ëŸ¬ë‹ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•´ì¤˜",
        # í˜¼ìš© í…ìŠ¤íŠ¸ (ì˜ì–´ ì£¼ë„)
        "What are the trends in ë°°í„°ë¦¬ ì—°êµ¬ by Korean researchers?",
        "Machine learning applications in ì „ê¸°ì°¨ charging optimization",
        "Dr. Smith and ê¹€ì² ìˆ˜ êµìˆ˜ì˜ collaboration network",
        # ë³µì¡í•œ í˜¼ìš© ë¶„ì„
        "ì „ê¸°ì°¨ ë°°í„°ë¦¬ ë¶„ì•¼ì—ì„œ ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” international researchersë“¤ê³¼ ê·¸ë“¤ì˜ collaboration networkë¥¼ comprehensiveí•˜ê²Œ ë¶„ì„í•´ì¤˜",
    ]

    print("ğŸ§ª Testing QueryAnalyzer with Mixed Language Support...")
    print("=" * 70)

    for i, query in enumerate(test_queries):
        print(f"\n{i+1}. Query: {query}")
        print("-" * 50)

        result = analyzer.analyze(query)

        # ì–¸ì–´ ê°ì§€ ê²°ê³¼
        korean_chars = len(re.findall(r"[ê°€-í£]", query))
        english_chars = len(re.findall(r"[a-zA-Z]", query))
        total_chars = korean_chars + english_chars
        korean_ratio = korean_chars / total_chars if total_chars > 0 else 0

        print(f"ğŸ“ Language Analysis:")
        print(f"   Detected: {result.language}")
        print(f"   Korean ratio: {korean_ratio:.1%} ({korean_chars}/{total_chars})")
        print(
            f"   Mixed text: {'Yes' if korean_chars > 0 and english_chars > 0 else 'No'}"
        )

        print(f"ğŸ” Analysis Results:")
        print(
            f"   Complexity: {result.complexity.value} (score: {result.estimated_complexity_score:.3f})"
        )
        print(f"   Type: {result.query_type.value}")
        print(f"   Search Mode: {result.search_mode.value}")
        print(f"   Required Nodes: {[nt.value for nt in result.required_node_types]}")

        # ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ (í˜¼ìš© í…ìŠ¤íŠ¸ íŠ¹ë³„íˆ í™•ì¸)
        if any(result.entities.values()):
            print(f"ğŸ¯ Extracted Entities:")
            for entity_type, entities in result.entities.items():
                if entities:
                    print(f"   {entity_type}: {entities}")

        print(f"âš¡ Performance:")
        print(f"   Confidence: {result.confidence_score:.3f}")
        print(f"   Timeout: {result.suggested_timeout}s")

        # í˜¼ìš© í…ìŠ¤íŠ¸ íŠ¹ë³„ íŒíŠ¸
        if korean_chars > 0 and english_chars > 0:
            print(f"ğŸŒ Mixed Language Hints:")
            if "machine learning" in query.lower() or "deep learning" in query.lower():
                print(f"   - Technical English terms detected")
            if any(term in query for term in ["êµìˆ˜", "ì—°êµ¬", "ë¶„ì„"]):
                print(f"   - Korean academic terms detected")
            print(f"   - Applied multi-language pattern matching")

    print(f"\nâœ… QueryAnalyzer testing completed!")
    print(f"ğŸŒ Mixed language support validated!")


if __name__ == "__main__":
    main()
