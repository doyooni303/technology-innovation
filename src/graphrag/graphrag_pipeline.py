"""
GraphRAG ë©”ì¸ íŒŒì´í”„ë¼ì¸
Main GraphRAG Pipeline Integration

ì „ì²´ GraphRAG ì‹œìŠ¤í…œì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤
- ì„¤ì • ê´€ë¦¬ ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
- ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
- ì¿¼ë¦¬ ë¶„ì„ ë° ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
- ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™” ë° LLM ì—°ë™
- í†µí•© QA ì¸í„°í˜ì´ìŠ¤ ì œê³µ
"""

import os
import json
import time
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np

# GraphRAG ì»´í¬ë„ŒíŠ¸ë“¤
try:
    from .config_manager import GraphRAGConfigManager
    from .query_analyzer import QueryAnalyzer, QueryAnalysisResult
    from .embeddings import (
        MultiNodeEmbedder,
        VectorStoreManager,
        create_embedding_model,
        is_ready as embeddings_ready,
    )
    from .embeddings.subgraph_extractor import SubgraphExtractor, SubgraphResult
    from .embeddings.context_serializer import ContextSerializer, SerializedContext
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# LLM ê´€ë ¨
try:
    import torch
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        GenerationConfig,
        BitsAndBytesConfig,
    )

    _transformers_available = True
except ImportError:
    _transformers_available = False
    warnings.warn("Transformers not available. Local LLM will not work.")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class PipelineStatus(Enum):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""

    UNINITIALIZED = "uninitialized"
    INITIALIZING = "initializing"
    READY = "ready"
    PROCESSING = "processing"
    ERROR = "error"


@dataclass
class PipelineState:
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´"""

    status: PipelineStatus
    components_loaded: Dict[str, bool]
    last_error: Optional[str]
    initialization_time: Optional[float]
    total_queries_processed: int
    last_query_time: Optional[float]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {**asdict(self), "status": self.status.value}


@dataclass
class QAResult:
    """QA ê²°ê³¼ í´ë˜ìŠ¤"""

    query: str
    answer: str
    subgraph_result: Optional[SubgraphResult]
    serialized_context: Optional[SerializedContext]
    query_analysis: Optional[QueryAnalysisResult]
    processing_time: float
    confidence_score: float
    source_nodes: List[str]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "query": self.query,
            "answer": self.answer,
            "processing_time": self.processing_time,
            "confidence_score": self.confidence_score,
            "source_nodes": self.source_nodes,
            "subgraph_nodes": (
                self.subgraph_result.total_nodes if self.subgraph_result else 0
            ),
            "subgraph_edges": (
                self.subgraph_result.total_edges if self.subgraph_result else 0
            ),
            "query_complexity": (
                self.query_analysis.complexity.value
                if self.query_analysis
                else "unknown"
            ),
            "query_type": (
                self.query_analysis.query_type.value
                if self.query_analysis
                else "unknown"
            ),
        }


class LocalLLMManager:
    """ë¡œì»¬ LLM ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: LLM ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.model = None
        self.tokenizer = None
        self.generation_config = None
        self.is_loaded = False

    def load_model(self) -> None:
        """ëª¨ë¸ ë¡œë“œ"""
        if not _transformers_available:
            raise ImportError("Transformers not available for local LLM")

        model_path = self.config.get("model_path")
        # if not model_path or not Path(model_path).exists():
        #     raise FileNotFoundError(f"Model not found: {model_path}")

        logger.info(f"ğŸ¤– Loading local LLM: {model_path}")

        # try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=self.config.get("trust_remote_code", True)
        )

        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
        quantization_config = None
        if self.config.get("load_in_4bit", False):
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
        elif self.config.get("load_in_8bit", False):
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)

        # ëª¨ë¸ ë¡œë“œ
        model_kwargs = {
            "trust_remote_code": self.config.get("trust_remote_code", True),
            "device_map": self.config.get("device_map", "auto"),
            "torch_dtype": getattr(torch, self.config.get("torch_dtype", "bfloat16")),
        }

        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config

        self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)

        # ìƒì„± ì„¤ì •
        self.generation_config = GenerationConfig(
            temperature=self.config.get("temperature", 0.1),
            max_new_tokens=self.config.get("max_new_tokens", 2048),
            do_sample=self.config.get("do_sample", True),
            top_p=self.config.get("top_p", 0.9),
            top_k=self.config.get("top_k", 50),
            repetition_penalty=self.config.get("repetition_penalty", 1.1),
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )

        self.is_loaded = True
        logger.info("âœ… Local LLM loaded successfully")

    # except Exception as e:
    #     logger.error(f"âŒ Failed to load local LLM: {e}")
    #     raise

    def generate(self, prompt: str, max_length: Optional[int] = None) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.is_loaded:
            self.load_model()

        # try:
        # í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096,
            padding=True,
        )

        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # attention_mask í™•ì¸
        if "attention_mask" not in inputs:
            inputs["attention_mask"] = torch.ones_like(inputs["input_ids"])

        # YAML ì„¤ì • ê¸°ë°˜ ìƒì„± ì„¤ì • (ê¸°ë³¸ê°’ì€ ì•ˆì „í•˜ê²Œ)
        generation_config = GenerationConfig(
            temperature=max(
                0.01, min(2.0, self.config.get("temperature", 0.1))
            ),  # ì•ˆì „ ë²”ìœ„
            max_new_tokens=min(
                max_length or self.config.get("max_new_tokens", 512),
                self.config.get("max_new_tokens", 512),
            ),
            do_sample=self.config.get("do_sample", True),  # YAML ì„¤ì • ìš°ì„ 
            top_p=max(0.1, min(1.0, self.config.get("top_p", 0.9))),  # ì•ˆì „ ë²”ìœ„
            top_k=max(1, min(100, self.config.get("top_k", 50))),  # ì•ˆì „ ë²”ìœ„
            repetition_penalty=max(
                1.0, min(2.0, self.config.get("repetition_penalty", 1.1))
            ),  # ì•ˆì „ ë²”ìœ„
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            use_cache=True,
        )

        logger.info(
            f"ğŸ” Generation config: temp={generation_config.temperature}, "
            f"do_sample={generation_config.do_sample}, "
            f"max_tokens={generation_config.max_new_tokens}"
        )

        # ì²« ë²ˆì§¸ ì‹œë„: YAML ì„¤ì •ëŒ€ë¡œ
        with torch.no_grad():
            # try:
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                generation_config=generation_config,
                use_cache=True,
            )

            # except RuntimeError as cuda_error:
            #     if "CUDA" in str(cuda_error) or "device-side assert" in str(cuda_error):
            #         logger.warning(f"âš ï¸ CUDA error with YAML settings: {cuda_error}")
            #         logger.info("ğŸ”„ Retrying with safer settings...")

            #         # ë‘ ë²ˆì§¸ ì‹œë„: ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
            #         return self._retry_with_safe_settings(inputs, max_length)
            #     else:
            #         raise

        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_tokens = outputs[0][inputs["input_ids"].shape[1] :]
        generated_text = self.tokenizer.decode(
            generated_tokens,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        )

        result = generated_text.strip()
        if not result:
            result = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

        logger.info(f"âœ… Generated {len(result)} characters with YAML settings")
        return result

    # except Exception as e:
    #     logger.error(f"âŒ Text generation failed: {e}")
    #     return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def unload_model(self) -> None:
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.model:
            del self.model
            self.model = None
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.is_loaded = False
        logger.info("ğŸ—‘ï¸ Local LLM unloaded")


class GraphRAGPipeline:
    """GraphRAG ë©”ì¸ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        config_file: str = "graphrag_config.yaml",
        env_file: Optional[str] = None,
        auto_setup: bool = False,
    ):
        """
        Args:
            config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            env_file: í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ
            auto_setup: ìë™ ì´ˆê¸°í™” ì—¬ë¶€
        """
        self.config_file = Path(config_file)
        self.env_file = Path(env_file) if env_file else None

        # ìƒíƒœ ê´€ë¦¬
        self.state = PipelineState(
            status=PipelineStatus.UNINITIALIZED,
            components_loaded={},
            last_error=None,
            initialization_time=None,
            total_queries_processed=0,
            last_query_time=None,
        )

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.config_manager = None
        self.query_analyzer = None
        self.embedder = None
        self.vector_store = None
        self.subgraph_extractor = None
        self.context_serializer = None
        self.llm_manager = None

        # ìºì‹œ
        self.query_cache = {}
        self.embeddings_loaded = False

        logger.info("ğŸš€ GraphRAG Pipeline initialized")

        if auto_setup:
            self.setup()

    def setup(self) -> None:
        """ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ Setting up GraphRAG Pipeline...")
        self.state.status = PipelineStatus.INITIALIZING

        start_time = time.time()

        try:
            # 1. ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”
            self._setup_config_manager()

            # 2. ì¿¼ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
            self._setup_query_analyzer()

            # 3. ì„ë² ë”© ì‹œìŠ¤í…œ í™•ì¸
            self._check_embeddings_system()

            # 4. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”ê¸° ì´ˆê¸°í™”
            self._setup_context_serializer()

            # 5. LLM ê´€ë¦¬ì ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
            self._setup_llm_manager()

            # 6. ì‹œìŠ¤í…œ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.state.initialization_time = time.time() - start_time
            self.state.status = PipelineStatus.READY
            self.state.last_error = None

            logger.info(
                f"âœ… GraphRAG Pipeline setup completed ({self.state.initialization_time:.2f}s)"
            )

        except Exception as e:
            self.state.status = PipelineStatus.ERROR
            self.state.last_error = str(e)
            logger.error(f"âŒ Pipeline setup failed: {e}")
            raise

    def _setup_config_manager(self) -> None:
        """ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”"""
        logger.info("ğŸ“‹ Setting up config manager...")

        self.config_manager = GraphRAGConfigManager(
            config_file=str(self.config_file),
            env_file=str(self.env_file) if self.env_file else None,
        )

        self.state.components_loaded["config_manager"] = True
        logger.info("âœ… Config manager ready")

    def _setup_query_analyzer(self) -> None:
        """ì¿¼ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ” Setting up query analyzer...")

        self.query_analyzer = QueryAnalyzer()
        self.state.components_loaded["query_analyzer"] = True
        logger.info("âœ… Query analyzer ready")

    def _check_embeddings_system(self) -> None:
        """ì„ë² ë”© ì‹œìŠ¤í…œ í™•ì¸"""
        logger.info("ğŸ” Checking embeddings system...")

        if not embeddings_ready():
            logger.warning("âš ï¸ Embeddings system not fully ready")
            self.state.components_loaded["embeddings"] = False
        else:
            self.state.components_loaded["embeddings"] = True
            logger.info("âœ… Embeddings system ready")

    def _setup_context_serializer(self) -> None:
        """ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ“ Setting up context serializer...")

        self.context_serializer = ContextSerializer()
        self.state.components_loaded["context_serializer"] = True
        logger.info("âœ… Context serializer ready")

    def _setup_llm_manager(self) -> None:
        """LLM ê´€ë¦¬ì ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        logger.info("ğŸ¤– Setting up LLM manager...")

        llm_config = self.config_manager.get_llm_config()

        if llm_config.get("model_path"):  # ë¡œì»¬ ëª¨ë¸
            self.llm_manager = LocalLLMManager(llm_config)
            self.state.components_loaded["llm_manager"] = True
            logger.info("âœ… Local LLM manager ready (model will be loaded on demand)")
        else:
            logger.warning("âš ï¸ No local LLM configuration found")
            self.state.components_loaded["llm_manager"] = False

    # def _ensure_embeddings_loaded(self) -> None:
    #     """ì„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ í™•ì¸ - ë²¡í„° ì €ì¥ì†Œ ì—°ë™ ê°œì„ """
    #     if self.embeddings_loaded:
    #         return

    #     logger.info("ğŸ“¥ Loading embeddings system...")

    #     # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    #     config = self.config_manager.config

    #     # í†µí•© ê·¸ë˜í”„ íŒŒì¼ í™•ì¸
    #     unified_graph_path = config.graph.unified_graph_path
    #     if not Path(unified_graph_path).exists():
    #         raise FileNotFoundError(f"Unified graph not found: {unified_graph_path}")

    #     # ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ í™•ì¸
    #     vector_store_root = config.graph.vector_store_path
    #     if not Path(vector_store_root).exists():
    #         logger.warning(f"Vector store root not found: {vector_store_root}")
    #         logger.info("ğŸ’¡ Run build_embeddings() first to create vector store")
    #         return

    #     # ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    #     vector_store_config = self.config_manager.get_vector_store_config()
    #     store_directory = vector_store_config["persist_directory"]

    #     logger.info(f"ğŸ“‚ Vector store directory: {store_directory}")
    #     logger.info(f"ğŸ“‚ Store type: {vector_store_config['store_type']}")

    #     # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ë˜ëŠ” ìƒì„±
    #     try:
    #         from .embeddings.vector_store_manager import create_vector_store_from_config

    #         self.vector_store = create_vector_store_from_config(
    #             config_manager=self.config_manager
    #         )

    #         # ë²¡í„° ì €ì¥ì†Œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì„ë² ë”©ì—ì„œ ë¡œë“œ ì‹œë„
    #         if self.vector_store.store.total_vectors == 0:
    #             embeddings_dir = config.paths.vector_store.embeddings

    #             if (
    #                 Path(embeddings_dir).exists()
    #                 and (Path(embeddings_dir) / "embeddings.npy").exists()
    #             ):
    #                 logger.info("ğŸ”„ Loading from saved embeddings...")
    #                 self.vector_store.load_from_saved_embeddings(vector_store_root)
    #             else:
    #                 logger.warning(f"No vector data found in: {store_directory}")
    #                 logger.warning(f"No embeddings found in: {embeddings_dir}")
    #                 logger.info("ğŸ’¡ Run build_embeddings() first")
    #                 return

    #         logger.info(
    #             f"âœ… Vector store loaded: {self.vector_store.store.total_vectors:,} vectors"
    #         )

    #         # SubgraphExtractor ì´ˆê¸°í™” - VectorStoreManager ì¸ìŠ¤í„´ìŠ¤ ì§ì ‘ ì „ë‹¬
    #         self.subgraph_extractor = SubgraphExtractor(
    #             unified_graph_path=unified_graph_path,
    #             vector_store_path=store_directory,  # ê²½ë¡œë§Œ ì „ë‹¬
    #             embedding_model=config.embeddings.model_name,
    #             device=config.embeddings.device,
    #         )

    #         # SubgraphExtractorì˜ ë²¡í„° ì €ì¥ì†Œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •
    #         self.subgraph_extractor.vector_store = self.vector_store

    #         self.embeddings_loaded = True
    #         logger.info("âœ… Embeddings system loaded successfully")

    #     except Exception as e:
    #         logger.error(f"âŒ Failed to load embeddings system: {e}")
    #         logger.error(f"   Store directory: {store_directory}")
    #         logger.error(f"   Store exists: {Path(store_directory).exists()}")

    #         # ë””ë²„ê¹… ì •ë³´
    #         if Path(store_directory).exists():
    #             files = list(Path(store_directory).glob("*"))
    #             logger.error(f"   Files in store: {[f.name for f in files]}")

    #         raise
    def _ensure_embeddings_loaded(self) -> None:
        """ì„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ í™•ì¸ - ëª¨ë“  model_type ì§€ì›"""
        if self.embeddings_loaded:
            return

        logger.info("ğŸ“¥ Loading embeddings system...")

        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        config = self.config_manager.config

        # í†µí•© ê·¸ë˜í”„ íŒŒì¼ í™•ì¸
        unified_graph_path = config.graph.unified_graph_path
        if not Path(unified_graph_path).exists():
            raise FileNotFoundError(f"Unified graph not found: {unified_graph_path}")

        # ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ í™•ì¸
        vector_store_root = config.paths.vector_store_root
        if not Path(vector_store_root).exists():
            Path(vector_store_root).mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ Created vector store directory: {vector_store_root}")

        # ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        vector_config = self.config_manager.get_vector_store_config()

        # VectorStoreManager ì´ˆê¸°í™”
        from .embeddings.vector_store_manager import VectorStoreManager

        self.vector_store = VectorStoreManager(
            store_type=vector_config["store_type"],
            persist_directory=vector_config["persist_directory"],
            **{
                k: v
                for k, v in vector_config.items()
                if k not in ["store_type", "persist_directory"]
            },
        )

        # ì„ë² ë”© ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  íƒ€ì… ì§€ì›)
        embedding_config = self.config_manager.get_embeddings_config()  # âœ… ìœ ì—°í•¨

        from .embeddings import create_embedding_model

        embedder_model = create_embedding_model(
            model_name=embedding_config["model_name"],  # âœ… íƒ€ì… ë¬´ê´€
            device=embedding_config["device"],  # âœ… íƒ€ì… ë¬´ê´€
            cache_dir=embedding_config["cache_dir"],  # âœ… íƒ€ì… ë¬´ê´€
        )

        # MultiNodeEmbedder ì´ˆê¸°í™”
        from .embeddings.multi_node_embedder import MultiNodeEmbedder

        self.embedder = MultiNodeEmbedder(
            unified_graph_path=config.graph.unified_graph_path,
            embedding_model=embedder_model,
            vector_store=self.vector_store,
            batch_size=embedding_config["batch_size"],  # âœ… íƒ€ì… ë¬´ê´€
        )

        # SubgraphExtractor ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì„¤ì • êµ¬ì¡°)
        embedding_config = self.config_manager.get_embeddings_config()
        from .embeddings.subgraph_extractor import SubgraphExtractor

        self.subgraph_extractor = SubgraphExtractor(
            unified_graph_path=config.graph.unified_graph_path,
            vector_store_path=config.paths.vector_store_root,  # âœ… ê²½ë¡œ ì „ë‹¬
            embedding_model=embedding_config["model_name"],
            device=embedding_config["device"],
        )

        self.embeddings_loaded = True
        logger.info("âœ… Embeddings system loaded with flexible config structure")

    def build_embeddings(self, force_rebuild: bool = False) -> Dict[str, Any]:
        """ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• - ìƒˆë¡œìš´ ê²½ë¡œ êµ¬ì¡° ì‚¬ìš©"""
        logger.info("ğŸ—ï¸ Building embeddings and vector store...")

        if self.state.status != PipelineStatus.READY:
            raise RuntimeError("Pipeline not ready. Call setup() first.")

        config = self.config_manager.config

        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.config_manager._create_directories()

        # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™” (ì„¤ì • ê´€ë¦¬ì ì‚¬ìš©)
        from .embeddings.multi_node_embedder import create_embedder_with_config

        embedding_config = self.config_manager.get_embeddings_config()
        self.embedder = create_embedder_with_config(
            unified_graph_path=config.graph.unified_graph_path,
            config_manager=self.config_manager,
            device=embedding_config["device"],
        )

        # ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        vector_store_config = self.config_manager.get_vector_store_config()

        # ì„ë² ë”© ìƒì„± (ìƒˆë¡œìš´ ê²½ë¡œ êµ¬ì¡°ë¡œ)
        embedding_results, saved_files = self.embedder.run_full_pipeline(
            output_dir=config.graph.vector_store_path,  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            use_cache=not force_rebuild,
            show_progress=True,
            vector_store_config=vector_store_config,
        )

        # ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• (ì„¤ì • ê´€ë¦¬ì ì‚¬ìš©)
        from .embeddings.vector_store_manager import create_vector_store_from_config

        self.vector_store = create_vector_store_from_config(
            config_manager=self.config_manager,
            store_type=vector_store_config["store_type"],
        )

        # ì„ë² ë”© ê²°ê³¼ë¡œë¶€í„° ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
        self.vector_store.load_from_embeddings(
            embedding_results,
            embeddings_dir=config.paths.vector_store.embeddings,
        )

        # í†µê³„ ë°˜í™˜
        total_nodes = sum(len(results) for results in embedding_results.values())

        result = {
            "total_embeddings": total_nodes,
            "embeddings_by_type": {k: len(v) for k, v in embedding_results.items()},
            "saved_files": {k: str(v) for k, v in saved_files.items()},
            "vector_store_info": self.vector_store.get_store_info(),
            "path_structure": {
                "vector_store_root": config.graph.vector_store_path,
                "embeddings_dir": config.paths.vector_store.embeddings,
                "store_specific_dir": vector_store_config["persist_directory"],
            },
        }

        logger.info(f"âœ… Embeddings built: {total_nodes:,} nodes")
        logger.info(f"ğŸ“‚ Structure created:")
        logger.info(f"   Root: {config.graph.vector_store_path}")
        logger.info(f"   Embeddings: {config.paths.vector_store.embeddings}")
        logger.info(f"   Vector Store: {vector_store_config['persist_directory']}")

        return result

    def ask(self, query: str, return_context: bool = False) -> Union[str, QAResult]:
        """ë©”ì¸ QA ì¸í„°í˜ì´ìŠ¤

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            return_context: ìƒì„¸ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ ì—¬ë¶€

        Returns:
            ë‹µë³€ ë¬¸ìì—´ ë˜ëŠ” QAResult ê°ì²´
        """
        if self.state.status != PipelineStatus.READY:
            raise RuntimeError("Pipeline not ready. Call setup() first.")

        start_time = time.time()
        self.state.status = PipelineStatus.PROCESSING

        # try:
        logger.info(f"â“ Processing query: '{query[:50]}...'")

        # 1. ìºì‹œ í™•ì¸
        if query in self.query_cache:
            logger.info("âœ… Cache hit")
            cached_result = self.query_cache[query]
            if return_context:
                return cached_result
            else:
                return cached_result.answer

        # 2. ì„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ
        self._ensure_embeddings_loaded()

        # 3. ì¿¼ë¦¬ ë¶„ì„
        query_analysis = self.query_analyzer.analyze(query)

        # 4. ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
        subgraph_result = self.subgraph_extractor.extract_subgraph(
            query=query, query_analysis=query_analysis
        )

        # 5. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”
        serialized_context = self.context_serializer.serialize(
            subgraph_result=subgraph_result, query_analysis=query_analysis
        )

        # 6. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        context_text = getattr(serialized_context, "main_text", "") or ""
        if not isinstance(context_text, str):
            context_text = str(context_text) if context_text else "No context available"

        answer = self._generate_answer(query, context_text)
        # answer = self._generate_answer(query, serialized_context.main_text)

        # 7. ê²°ê³¼ êµ¬ì„±
        processing_time = time.time() - start_time

        qa_result = QAResult(
            query=query,
            answer=answer,
            subgraph_result=subgraph_result,
            serialized_context=serialized_context,
            query_analysis=query_analysis,
            processing_time=processing_time,
            confidence_score=subgraph_result.confidence_score,
            source_nodes=list(subgraph_result.nodes.keys()),
        )

        # 8. ìºì‹œ ì €ì¥
        self.query_cache[query] = qa_result

        # 9. ìƒíƒœ ì—…ë°ì´íŠ¸
        self.state.total_queries_processed += 1
        self.state.last_query_time = processing_time
        self.state.status = PipelineStatus.READY

        logger.info(f"âœ… Query processed ({processing_time:.2f}s)")

        if return_context:
            return qa_result
        else:
            return answer

    # except Exception as e:
    #     self.state.status = PipelineStatus.ERROR
    #     self.state.last_error = str(e)
    #     logger.error(f"âŒ Query processing failed: {e}")

    #     # ê°„ë‹¨í•œ ë‹µë³€ ë°˜í™˜
    #     fallback_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    #     if return_context:
    #         return QAResult(
    #             query=query,
    #             answer=fallback_answer,
    #             subgraph_result=None,
    #             serialized_context=None,
    #             query_analysis=None,
    #             processing_time=time.time() - start_time,
    #             confidence_score=0.0,
    #             source_nodes=[],
    #         )
    #     else:
    #         return fallback_answer

    def _generate_answer(self, query: str, context: str) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì•ˆì „í•œ ë²„ì „)"""

        # ì…ë ¥ ê²€ì¦
        if not isinstance(query, str):
            logger.error(f"âŒ Query is not a string: {type(query)} - {query}")
            query = str(query) if query is not None else "Unknown query"

        if not isinstance(context, str):
            logger.error(f"âŒ Context is not a string: {type(context)} - {context}")
            context = str(context) if context is not None else "No context available"

        # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        if not query.strip():
            query = "Unknown query"

        if not context.strip():
            context = "No context available"

        # try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_qa_prompt(query, context)

        # í”„ë¡¬í”„íŠ¸ ê²€ì¦
        if not isinstance(prompt, str):
            logger.error(f"âŒ Prompt is not a string: {type(prompt)}")
            prompt = f"ì§ˆë¬¸: {query}\në‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."

        logger.debug(f"ğŸ” Generated prompt length: {len(prompt)}")

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        if self.llm_manager and self.llm_manager.config.get("model_path"):
            # ë¡œì»¬ LLM ì‚¬ìš©
            # try:
            logger.info("ğŸ¤– Generating answer with local LLM...")
            answer = self.llm_manager.generate(prompt, max_length=1000)

            # ë‹µë³€ ê²€ì¦
            if not isinstance(answer, str):
                logger.error(f"âŒ LLM returned non-string: {type(answer)}")
                return f"LLM ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            if not answer.strip():
                logger.warning("âš ï¸ LLM returned empty response")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

            return answer.strip()

        # except Exception as e:
        #     logger.error(f"âŒ Local LLM generation failed: {e}")
        #     logger.debug(f"âŒ Prompt that caused error: {prompt[:200]}...")
        #     return f"ë¡œì»¬ LLM ì˜¤ë¥˜: {str(e)}"
        else:
            # API LLM í´ë°± ë˜ëŠ” ê¸°ë³¸ ë‹µë³€
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤. LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

    # except Exception as e:
    #     logger.error(f"âŒ Answer generation failed: {e}")
    #     return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _build_qa_prompt(self, query: str, context: str) -> str:
        """QA í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì•ˆì „í•œ ë²„ì „)"""

        # ì…ë ¥ ê²€ì¦ ë° ì •ë¦¬
        query = str(query).strip() if query else "Unknown query"
        context = str(context).strip() if context else "No context available"

        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í¬ë‚˜ì´ì € í•œê³„ ê³ ë ¤)
        max_context_length = 3000  # ì•ˆì „í•œ ê¸¸ì´
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
            logger.warning(f"âš ï¸ Context truncated to {max_context_length} chars")

        prompt_template = """ë‹¤ìŒì€ í•™ìˆ  ì—°êµ¬ ë¬¸í—Œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

    **ì»¨í…ìŠ¤íŠ¸:**
    {context}

    **ì§ˆë¬¸:** {query}

    **ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
    1. ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
    2. í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ì ì ˆíˆ í˜¼ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
    3. êµ¬ì²´ì ì¸ ë…¼ë¬¸, ì €ì, ì—°êµ¬ ê²°ê³¼ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”
    4. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œí•˜ì„¸ìš”
    5. ë‹µë³€ì€ 500ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”

    **ë‹µë³€:**"""

        try:
            formatted_prompt = prompt_template.format(context=context, query=query)

            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¸
            if len(formatted_prompt) > 8000:  # í† í¬ë‚˜ì´ì € í•œê³„ ê³ ë ¤
                logger.warning(
                    f"âš ï¸ Prompt too long: {len(formatted_prompt)} chars, truncating..."
                )
                # ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ì¤„ì„
                shorter_context = context[:1500] + "..."
                formatted_prompt = prompt_template.format(
                    context=shorter_context, query=query
                )

            return formatted_prompt

        except Exception as e:
            logger.error(f"âŒ Prompt formatting failed: {e}")
            # ìµœì†Œí•œì˜ ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸
            return f"ì§ˆë¬¸: {query}\n\nìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”."

    def get_subgraph(self, query: str) -> Optional[SubgraphResult]:
        """ì„œë¸Œê·¸ë˜í”„ë§Œ ì¶”ì¶œ (LLM ì—†ì´)"""
        try:
            self._ensure_embeddings_loaded()

            query_analysis = self.query_analyzer.analyze(query)
            subgraph_result = self.subgraph_extractor.extract_subgraph(
                query=query, query_analysis=query_analysis
            )

            return subgraph_result

        except Exception as e:
            logger.error(f"âŒ Subgraph extraction failed: {e}")
            return None

    def batch_process(self, queries: List[str], max_workers: int = 2) -> List[QAResult]:
        """ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬"""
        logger.info(f"ğŸ“‹ Processing {len(queries)} queries in batch...")

        results = []
        for i, query in enumerate(queries):
            logger.info(f"Processing query {i+1}/{len(queries)}")
            result = self.ask(query, return_context=True)
            results.append(result)

        logger.info(f"âœ… Batch processing completed: {len(results)} results")
        return results

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ - ê²½ë¡œ ì •ë³´ í¬í•¨"""
        status = {
            "pipeline_state": self.state.to_dict(),
            "components": self.state.components_loaded,
            "embeddings_loaded": self.embeddings_loaded,
            "cache_size": len(self.query_cache),
            "memory_usage": self._get_memory_usage(),
        }

        # ì„¤ì • ê´€ë¦¬ì ì •ë³´
        if self.config_manager:
            config = self.config_manager.config
            embedding_config = self.config_manager.get_embeddings_config()
            status["configuration"] = {
                "llm_provider": config.llm.provider,
                "embedding_model": embedding_config["model_name"],
                "vector_store_type": config.vector_store.store_type,
                "paths": {
                    "unified_graph": config.graph.unified_graph_path,
                    "vector_store_root": config.paths.vector_store_root,
                    "embeddings_dir": config.paths.vector_store.embeddings,
                    "store_directory": self.config_manager.get_vector_store_config()[
                        "persist_directory"
                    ],
                },
            }

        # ë²¡í„° ì €ì¥ì†Œ ì •ë³´
        if self.vector_store:
            status["vector_store"] = self.vector_store.get_store_info()

        # LLM ìƒíƒœ
        if self.llm_manager:
            status["llm_loaded"] = self.llm_manager.is_loaded
            status["llm_model_path"] = self.llm_manager.config.get("model_path")

        return status

    def setup_from_config(
        self,
        config_file: str = "graphrag_config.yaml",
        auto_build_embeddings: bool = False,
    ) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ë¡œë¶€í„° ì™„ì „ ìë™ ì„¤ì •"""

        logger.info(f"ğŸš€ Setting up GraphRAG from config: {config_file}")

        # ì„¤ì • íŒŒì¼ë¡œ ì´ˆê¸°í™”
        self.config_file = Path(config_file)

        # ì„¤ì • ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.setup()

        setup_result = {
            "setup_completed": True,
            "config_loaded": True,
            "directories_created": True,
        }

        # ìë™ ì„ë² ë”© êµ¬ì¶•
        if auto_build_embeddings:
            try:
                build_result = self.build_embeddings()
                setup_result.update(
                    {
                        "embeddings_built": True,
                        "embedding_stats": build_result,
                    }
                )
            except Exception as e:
                logger.warning(f"âš ï¸ Auto embedding build failed: {e}")
                setup_result["embeddings_built"] = False
                setup_result["embedding_error"] = str(e)

        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        status = self.get_system_status()
        setup_result["system_status"] = status

        logger.info("âœ… GraphRAG setup completed!")
        return setup_result

    def rebuild_vector_store(
        self,
        new_store_type: Optional[str] = None,
        force_rebuild_embeddings: bool = False,
    ) -> Dict[str, Any]:
        """ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì¶•"""

        logger.info("ğŸ”„ Rebuilding vector store...")

        config = self.config_manager.config
        current_store_type = config.vector_store.store_type
        target_store_type = new_store_type or current_store_type

        result = {
            "previous_store_type": current_store_type,
            "new_store_type": target_store_type,
            "embeddings_rebuilt": False,
            "store_migrated": False,
        }

        # ì„ë² ë”© ì¬êµ¬ì¶• (í•„ìš”ì‹œ)
        if force_rebuild_embeddings:
            logger.info("ğŸ—ï¸ Rebuilding embeddings...")
            build_result = self.build_embeddings(force_rebuild=True)
            result["embeddings_rebuilt"] = True
            result["embedding_stats"] = build_result

        # ë²¡í„° ì €ì¥ì†Œ íƒ€ì… ë³€ê²½ (í•„ìš”ì‹œ)
        if new_store_type and new_store_type != current_store_type:
            logger.info(f"ğŸ”„ Migrating from {current_store_type} to {new_store_type}")

            # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
            self._ensure_embeddings_loaded()

            if self.vector_store:
                # ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰
                new_vector_store = self.vector_store.migrate_store_type(new_store_type)
                self.vector_store = new_vector_store

                # ì„¤ì • ì—…ë°ì´íŠ¸
                self.config_manager.update_config(
                    **{"vector_store.store_type": new_store_type}
                )

                result["store_migrated"] = True
                result["new_store_info"] = self.vector_store.get_store_info()

        logger.info("âœ… Vector store rebuild completed!")
        return result

    @classmethod
    def from_config_file(
        cls,
        config_file: str = "graphrag_config.yaml",
        auto_setup: bool = True,
        auto_build_embeddings: bool = False,
    ) -> "GraphRAGPipeline":
        """ì„¤ì • íŒŒì¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ìƒì„± (í´ë˜ìŠ¤ ë©”ì„œë“œ)"""

        pipeline = cls(config_file=config_file, auto_setup=False)

        if auto_setup:
            pipeline.setup_from_config(
                config_file=config_file,
                auto_build_embeddings=auto_build_embeddings,
            )

        return pipeline

    def _get_memory_usage(self) -> Dict[str, str]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            import psutil

            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
            memory = psutil.virtual_memory()

            usage = {
                "system_total": f"{memory.total / 1024**3:.1f}GB",
                "system_available": f"{memory.available / 1024**3:.1f}GB",
                "system_percent": f"{memory.percent:.1f}%",
            }

            # GPU ë©”ëª¨ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                gpu_allocated = torch.cuda.memory_allocated(0)
                gpu_cached = torch.cuda.memory_reserved(0)

                usage.update(
                    {
                        "gpu_total": f"{gpu_memory / 1024**3:.1f}GB",
                        "gpu_allocated": f"{gpu_allocated / 1024**3:.1f}GB",
                        "gpu_cached": f"{gpu_cached / 1024**3:.1f}GB",
                        "gpu_percent": f"{(gpu_allocated / gpu_memory) * 100:.1f}%",
                    }
                )

            return usage

        except ImportError:
            return {"status": "psutil not available"}
        except Exception as e:
            return {"error": str(e)}

    def clear_cache(self) -> None:
        """ìºì‹œ ì •ë¦¬"""
        self.query_cache.clear()
        logger.info("ğŸ—‘ï¸ Query cache cleared")

    def shutdown(self) -> None:
        """ì‹œìŠ¤í…œ ì¢…ë£Œ ë° ì •ë¦¬"""
        logger.info("ğŸ”Œ Shutting down GraphRAG Pipeline...")

        # LLM ì–¸ë¡œë“œ
        if self.llm_manager:
            self.llm_manager.unload_model()

        # ìºì‹œ ì •ë¦¬
        self.clear_cache()

        # ìƒíƒœ ë¦¬ì…‹
        self.state.status = PipelineStatus.UNINITIALIZED
        self.embeddings_loaded = False

        logger.info("âœ… Pipeline shutdown completed")


def create_graphrag_pipeline(
    config_file: str = "graphrag_config.yaml",
    auto_setup: bool = True,
    auto_build_embeddings: bool = False,
) -> GraphRAGPipeline:
    """GraphRAG íŒŒì´í”„ë¼ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""

    return GraphRAGPipeline.from_config_file(
        config_file=config_file,
        auto_setup=auto_setup,
        auto_build_embeddings=auto_build_embeddings,
    )


def main():
    """GraphRAG Pipeline í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Testing GraphRAG Pipeline...")

    try:
        # 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = GraphRAGPipeline(config_file="graphrag_config.yaml", auto_setup=True)

        # 2. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        status = pipeline.get_system_status()
        print(f"ğŸ“Š System Status:")
        print(f"   Pipeline: {status['pipeline_state']['status']}")
        print(f"   Components: {status['components']}")

        # 3. ì„ë² ë”© êµ¬ì¶• (í•„ìš”í•œ ê²½ìš°)
        if not status["embeddings_loaded"]:
            print(f"\nğŸ—ï¸ Building embeddings...")
            build_result = pipeline.build_embeddings()
            print(f"âœ… Built {build_result['total_embeddings']} embeddings")

        # 4. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        test_queries = [
            "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì€?",
            "AI ë° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì´ ì ìš©ëœ ì£¼ìš” taskëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            "ë°°í„°ë¦¬ ì „ê·¹ ê³µì •ì—ì„œ AIê°€ ì ìš©ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œìš”?",
        ]

        print(f"\nâ“ Testing queries...")
        for i, query in enumerate(test_queries[:1]):  # ì²« ë²ˆì§¸ë§Œ í…ŒìŠ¤íŠ¸
            print(f"\n{i+1}. {query}")

            result = pipeline.ask(query, return_context=True)

            print(f"âœ… Answer: {result.answer[:200]}...")
            print(
                f"ğŸ“Š Stats: {result.processing_time:.2f}s, {result.confidence_score:.3f} confidence"
            )
            print(f"ğŸ“„ Sources: {len(result.source_nodes)} nodes")

        # 5. ìµœì¢… ìƒíƒœ
        final_status = pipeline.get_system_status()
        print(f"\nğŸ“ˆ Final Stats:")
        print(
            f"   Queries processed: {final_status['pipeline_state']['total_queries_processed']}"
        )
        print(f"   Cache size: {final_status['cache_size']}")

        print(f"\nâœ… GraphRAG Pipeline test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
