"""
GraphRAG ë©”ì¸ íŒŒì´í”„ë¼ì¸
Main GraphRAG Pipeline Integration

ì „ì²´ GraphRAG ì‹œìŠ¤í…œì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤
- ì„¤ì • ê´€ë¦¬ ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
- ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
- ì¿¼ë¦¬ ë¶„ì„ ë° ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
- ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™” ë° LLM ì—°ë™
- í†µí•© QA ì¸í„°í˜ì´ìŠ¤ ì œê³µ
"""

import os
import json
import time
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np

# GraphRAG ì»´í¬ë„ŒíŠ¸ë“¤
try:
    from .config_manager import GraphRAGConfigManager
    from .query_analyzer import QueryAnalyzer, QueryAnalysisResult
    from .embeddings import (
        MultiNodeEmbedder,
        VectorStoreManager,
        create_embedding_model,
        is_ready as embeddings_ready,
    )
    from .embeddings.subgraph_extractor import SubgraphExtractor, SubgraphResult
    from .embeddings.context_serializer import ContextSerializer, SerializedContext
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# LLM ê´€ë ¨
try:
    import torch
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        GenerationConfig,
        BitsAndBytesConfig,
    )

    _transformers_available = True
except ImportError:
    _transformers_available = False
    warnings.warn("Transformers not available. Local LLM will not work.")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class PipelineStatus(Enum):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""

    UNINITIALIZED = "uninitialized"
    INITIALIZING = "initializing"
    READY = "ready"
    PROCESSING = "processing"
    ERROR = "error"


@dataclass
class PipelineState:
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´"""

    status: PipelineStatus
    components_loaded: Dict[str, bool]
    last_error: Optional[str]
    initialization_time: Optional[float]
    total_queries_processed: int
    last_query_time: Optional[float]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {**asdict(self), "status": self.status.value}


@dataclass
class QAResult:
    """QA ê²°ê³¼ í´ë˜ìŠ¤"""

    query: str
    answer: str
    subgraph_result: Optional[SubgraphResult]
    serialized_context: Optional[SerializedContext]
    query_analysis: Optional[QueryAnalysisResult]
    processing_time: float
    confidence_score: float
    source_nodes: List[str]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "query": self.query,
            "answer": self.answer,
            "processing_time": self.processing_time,
            "confidence_score": self.confidence_score,
            "source_nodes": self.source_nodes,
            "subgraph_nodes": (
                self.subgraph_result.total_nodes if self.subgraph_result else 0
            ),
            "subgraph_edges": (
                self.subgraph_result.total_edges if self.subgraph_result else 0
            ),
            "query_complexity": (
                self.query_analysis.complexity.value
                if self.query_analysis
                else "unknown"
            ),
            "query_type": (
                self.query_analysis.query_type.value
                if self.query_analysis
                else "unknown"
            ),
        }


class LocalLLMManager:
    """ë¡œì»¬ LLM ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: LLM ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.model = None
        self.tokenizer = None
        self.generation_config = None
        self.is_loaded = False

    def load_model(self) -> None:
        """ëª¨ë¸ ë¡œë“œ"""
        if not _transformers_available:
            raise ImportError("Transformers not available for local LLM")

        model_path = self.config.get("model_path")
        if not model_path or not Path(model_path).exists():
            raise FileNotFoundError(f"Model not found: {model_path}")

        logger.info(f"ğŸ¤– Loading local LLM: {model_path}")

        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=self.config.get("trust_remote_code", True)
            )

            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
            quantization_config = None
            if self.config.get("load_in_4bit", False):
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                )
            elif self.config.get("load_in_8bit", False):
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)

            # ëª¨ë¸ ë¡œë“œ
            model_kwargs = {
                "trust_remote_code": self.config.get("trust_remote_code", True),
                "device_map": self.config.get("device_map", "auto"),
                "torch_dtype": getattr(
                    torch, self.config.get("torch_dtype", "bfloat16")
                ),
            }

            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config

            self.model = AutoModelForCausalLM.from_pretrained(
                model_path, **model_kwargs
            )

            # ìƒì„± ì„¤ì •
            self.generation_config = GenerationConfig(
                temperature=self.config.get("temperature", 0.1),
                max_new_tokens=self.config.get("max_new_tokens", 2048),
                do_sample=self.config.get("do_sample", True),
                top_p=self.config.get("top_p", 0.9),
                top_k=self.config.get("top_k", 50),
                repetition_penalty=self.config.get("repetition_penalty", 1.1),
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

            self.is_loaded = True
            logger.info("âœ… Local LLM loaded successfully")

        except Exception as e:
            logger.error(f"âŒ Failed to load local LLM: {e}")
            raise

    def generate(self, prompt: str, max_length: Optional[int] = None) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.is_loaded:
            self.load_model()

        try:
            # í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=4096,  # ì…ë ¥ ê¸¸ì´ ì œí•œ
            )

            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
            if max_length:
                generation_config = GenerationConfig.from_dict(
                    {**self.generation_config.to_dict(), "max_new_tokens": max_length}
                )
            else:
                generation_config = self.generation_config

            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs, generation_config=generation_config, use_cache=True
                )

            # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œê±°)
            generated_tokens = outputs[0][inputs["input_ids"].shape[1] :]
            generated_text = self.tokenizer.decode(
                generated_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True,
            )

            return generated_text.strip()

        except Exception as e:
            logger.error(f"âŒ Text generation failed: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def unload_model(self) -> None:
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.model:
            del self.model
            self.model = None
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.is_loaded = False
        logger.info("ğŸ—‘ï¸ Local LLM unloaded")


class GraphRAGPipeline:
    """GraphRAG ë©”ì¸ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        config_file: str = "graphrag_config.yaml",
        env_file: Optional[str] = None,
        auto_setup: bool = False,
    ):
        """
        Args:
            config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            env_file: í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ
            auto_setup: ìë™ ì´ˆê¸°í™” ì—¬ë¶€
        """
        self.config_file = Path(config_file)
        self.env_file = Path(env_file) if env_file else None

        # ìƒíƒœ ê´€ë¦¬
        self.state = PipelineState(
            status=PipelineStatus.UNINITIALIZED,
            components_loaded={},
            last_error=None,
            initialization_time=None,
            total_queries_processed=0,
            last_query_time=None,
        )

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.config_manager = None
        self.query_analyzer = None
        self.embedder = None
        self.vector_store = None
        self.subgraph_extractor = None
        self.context_serializer = None
        self.llm_manager = None

        # ìºì‹œ
        self.query_cache = {}
        self.embeddings_loaded = False

        logger.info("ğŸš€ GraphRAG Pipeline initialized")

        if auto_setup:
            self.setup()

    def setup(self) -> None:
        """ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ Setting up GraphRAG Pipeline...")
        self.state.status = PipelineStatus.INITIALIZING

        start_time = time.time()

        try:
            # 1. ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”
            self._setup_config_manager()

            # 2. ì¿¼ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
            self._setup_query_analyzer()

            # 3. ì„ë² ë”© ì‹œìŠ¤í…œ í™•ì¸
            self._check_embeddings_system()

            # 4. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”ê¸° ì´ˆê¸°í™”
            self._setup_context_serializer()

            # 5. LLM ê´€ë¦¬ì ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
            self._setup_llm_manager()

            # 6. ì‹œìŠ¤í…œ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.state.initialization_time = time.time() - start_time
            self.state.status = PipelineStatus.READY
            self.state.last_error = None

            logger.info(
                f"âœ… GraphRAG Pipeline setup completed ({self.state.initialization_time:.2f}s)"
            )

        except Exception as e:
            self.state.status = PipelineStatus.ERROR
            self.state.last_error = str(e)
            logger.error(f"âŒ Pipeline setup failed: {e}")
            raise

    def _setup_config_manager(self) -> None:
        """ì„¤ì • ê´€ë¦¬ì ì´ˆê¸°í™”"""
        logger.info("ğŸ“‹ Setting up config manager...")

        self.config_manager = GraphRAGConfigManager(
            config_file=str(self.config_file),
            env_file=str(self.env_file) if self.env_file else None,
        )

        self.state.components_loaded["config_manager"] = True
        logger.info("âœ… Config manager ready")

    def _setup_query_analyzer(self) -> None:
        """ì¿¼ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ” Setting up query analyzer...")

        self.query_analyzer = QueryAnalyzer()
        self.state.components_loaded["query_analyzer"] = True
        logger.info("âœ… Query analyzer ready")

    def _check_embeddings_system(self) -> None:
        """ì„ë² ë”© ì‹œìŠ¤í…œ í™•ì¸"""
        logger.info("ğŸ” Checking embeddings system...")

        if not embeddings_ready():
            logger.warning("âš ï¸ Embeddings system not fully ready")
            self.state.components_loaded["embeddings"] = False
        else:
            self.state.components_loaded["embeddings"] = True
            logger.info("âœ… Embeddings system ready")

    def _setup_context_serializer(self) -> None:
        """ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ“ Setting up context serializer...")

        self.context_serializer = ContextSerializer()
        self.state.components_loaded["context_serializer"] = True
        logger.info("âœ… Context serializer ready")

    def _setup_llm_manager(self) -> None:
        """LLM ê´€ë¦¬ì ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        logger.info("ğŸ¤– Setting up LLM manager...")

        llm_config = self.config_manager.get_llm_config()

        if llm_config.get("model_path"):  # ë¡œì»¬ ëª¨ë¸
            self.llm_manager = LocalLLMManager(llm_config)
            self.state.components_loaded["llm_manager"] = True
            logger.info("âœ… Local LLM manager ready (model will be loaded on demand)")
        else:
            logger.warning("âš ï¸ No local LLM configuration found")
            self.state.components_loaded["llm_manager"] = False

    def _ensure_embeddings_loaded(self) -> None:
        """ì„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ í™•ì¸"""
        if self.embeddings_loaded:
            return

        logger.info("ğŸ“¥ Loading embeddings system...")

        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        config = self.config_manager.config

        # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        unified_graph_path = config.graph.unified_graph_path
        if not Path(unified_graph_path).exists():
            raise FileNotFoundError(f"Unified graph not found: {unified_graph_path}")

        # ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ í™•ì¸
        vector_store_path = config.graph.vector_store_path
        if not Path(vector_store_path).exists():
            logger.warning(f"Vector store not found: {vector_store_path}")
            logger.info("ğŸ’¡ Run build_embeddings() first to create vector store")
            return

        # ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        self.subgraph_extractor = SubgraphExtractor(
            unified_graph_path=unified_graph_path,
            vector_store_path=vector_store_path,
            embedding_model=config.embedding.model_name,
            device=config.embedding.device,
        )

        self.embeddings_loaded = True
        logger.info("âœ… Embeddings system loaded")

    def build_embeddings(self, force_rebuild: bool = False) -> Dict[str, Any]:
        """ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
        logger.info("ğŸ—ï¸ Building embeddings and vector store...")

        if self.state.status != PipelineStatus.READY:
            raise RuntimeError("Pipeline not ready. Call setup() first.")

        config = self.config_manager.config

        # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        self.embedder = MultiNodeEmbedder(
            unified_graph_path=config.graph.unified_graph_path,
            embedding_model=config.embedding.model_name,
            batch_size=config.embedding.batch_size,
            max_text_length=config.embedding.max_length,
            language="mixed",
            cache_dir=(
                config.paths.embeddings_cache if hasattr(config, "paths") else None
            ),
            device=config.embedding.device,
        )

        # ì„ë² ë”© ìƒì„±
        embedding_results, saved_files = self.embedder.run_full_pipeline(
            output_dir=config.graph.vector_store_path,
            use_cache=not force_rebuild,
            show_progress=True,
        )

        # ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
        self.vector_store = VectorStoreManager(
            store_type=getattr(config, "vector_store", {}).get("store_type", "auto"),
            persist_directory=config.graph.vector_store_path,
        )

        self.vector_store.load_from_embeddings(embedding_results)

        # í†µê³„ ë°˜í™˜
        total_nodes = sum(len(results) for results in embedding_results.values())

        result = {
            "total_embeddings": total_nodes,
            "embeddings_by_type": {k: len(v) for k, v in embedding_results.items()},
            "saved_files": {k: str(v) for k, v in saved_files.items()},
            "vector_store_path": config.graph.vector_store_path,
        }

        logger.info(f"âœ… Embeddings built: {total_nodes:,} nodes")
        return result

    def ask(self, query: str, return_context: bool = False) -> Union[str, QAResult]:
        """ë©”ì¸ QA ì¸í„°í˜ì´ìŠ¤

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            return_context: ìƒì„¸ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ ì—¬ë¶€

        Returns:
            ë‹µë³€ ë¬¸ìì—´ ë˜ëŠ” QAResult ê°ì²´
        """
        if self.state.status != PipelineStatus.READY:
            raise RuntimeError("Pipeline not ready. Call setup() first.")

        start_time = time.time()
        self.state.status = PipelineStatus.PROCESSING

        try:
            logger.info(f"â“ Processing query: '{query[:50]}...'")

            # 1. ìºì‹œ í™•ì¸
            if query in self.query_cache:
                logger.info("âœ… Cache hit")
                cached_result = self.query_cache[query]
                if return_context:
                    return cached_result
                else:
                    return cached_result.answer

            # 2. ì„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ
            self._ensure_embeddings_loaded()

            # 3. ì¿¼ë¦¬ ë¶„ì„
            query_analysis = self.query_analyzer.analyze(query)

            # 4. ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
            subgraph_result = self.subgraph_extractor.extract_subgraph(
                query=query, query_analysis=query_analysis
            )

            # 5. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”
            serialized_context = self.context_serializer.serialize(
                subgraph_result=subgraph_result, query_analysis=query_analysis
            )

            # 6. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            answer = self._generate_answer(query, serialized_context.main_text)

            # 7. ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time

            qa_result = QAResult(
                query=query,
                answer=answer,
                subgraph_result=subgraph_result,
                serialized_context=serialized_context,
                query_analysis=query_analysis,
                processing_time=processing_time,
                confidence_score=subgraph_result.confidence_score,
                source_nodes=list(subgraph_result.nodes.keys()),
            )

            # 8. ìºì‹œ ì €ì¥
            self.query_cache[query] = qa_result

            # 9. ìƒíƒœ ì—…ë°ì´íŠ¸
            self.state.total_queries_processed += 1
            self.state.last_query_time = processing_time
            self.state.status = PipelineStatus.READY

            logger.info(f"âœ… Query processed ({processing_time:.2f}s)")

            if return_context:
                return qa_result
            else:
                return answer

        except Exception as e:
            self.state.status = PipelineStatus.ERROR
            self.state.last_error = str(e)
            logger.error(f"âŒ Query processing failed: {e}")

            # ê°„ë‹¨í•œ ë‹µë³€ ë°˜í™˜
            fallback_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

            if return_context:
                return QAResult(
                    query=query,
                    answer=fallback_answer,
                    subgraph_result=None,
                    serialized_context=None,
                    query_analysis=None,
                    processing_time=time.time() - start_time,
                    confidence_score=0.0,
                    source_nodes=[],
                )
            else:
                return fallback_answer

    def _generate_answer(self, query: str, context: str) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_qa_prompt(query, context)

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        if self.llm_manager and self.llm_manager.config.get("model_path"):
            # ë¡œì»¬ LLM ì‚¬ìš©
            try:
                answer = self.llm_manager.generate(prompt, max_length=1000)
                return answer
            except Exception as e:
                logger.error(f"âŒ Local LLM generation failed: {e}")
                return f"ë¡œì»¬ LLM ì˜¤ë¥˜: {str(e)}"
        else:
            # API LLM í´ë°± ë˜ëŠ” ê¸°ë³¸ ë‹µë³€
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤. LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

    def _build_qa_prompt(self, query: str, context: str) -> str:
        """QA í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        prompt_template = """ë‹¤ìŒì€ í•™ìˆ  ì—°êµ¬ ë¬¸í—Œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

**ì»¨í…ìŠ¤íŠ¸:**
{context}

**ì§ˆë¬¸:** {query}

**ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
1. ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
2. í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ì ì ˆíˆ í˜¼ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
3. êµ¬ì²´ì ì¸ ë…¼ë¬¸, ì €ì, ì—°êµ¬ ê²°ê³¼ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”
4. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œí•˜ì„¸ìš”
5. ë‹µë³€ì€ 500ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”

**ë‹µë³€:**"""

        return prompt_template.format(context=context, query=query)

    def get_subgraph(self, query: str) -> Optional[SubgraphResult]:
        """ì„œë¸Œê·¸ë˜í”„ë§Œ ì¶”ì¶œ (LLM ì—†ì´)"""
        try:
            self._ensure_embeddings_loaded()

            query_analysis = self.query_analyzer.analyze(query)
            subgraph_result = self.subgraph_extractor.extract_subgraph(
                query=query, query_analysis=query_analysis
            )

            return subgraph_result

        except Exception as e:
            logger.error(f"âŒ Subgraph extraction failed: {e}")
            return None

    def batch_process(self, queries: List[str], max_workers: int = 2) -> List[QAResult]:
        """ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬"""
        logger.info(f"ğŸ“‹ Processing {len(queries)} queries in batch...")

        results = []
        for i, query in enumerate(queries):
            logger.info(f"Processing query {i+1}/{len(queries)}")
            result = self.ask(query, return_context=True)
            results.append(result)

        logger.info(f"âœ… Batch processing completed: {len(results)} results")
        return results

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        status = {
            "pipeline_state": self.state.to_dict(),
            "components": self.state.components_loaded,
            "embeddings_loaded": self.embeddings_loaded,
            "cache_size": len(self.query_cache),
            "memory_usage": self._get_memory_usage(),
        }

        # LLM ìƒíƒœ
        if self.llm_manager:
            status["llm_loaded"] = self.llm_manager.is_loaded
            status["llm_model_path"] = self.llm_manager.config.get("model_path")

        return status

    def _get_memory_usage(self) -> Dict[str, str]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            import psutil

            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
            memory = psutil.virtual_memory()

            usage = {
                "system_total": f"{memory.total / 1024**3:.1f}GB",
                "system_available": f"{memory.available / 1024**3:.1f}GB",
                "system_percent": f"{memory.percent:.1f}%",
            }

            # GPU ë©”ëª¨ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                gpu_allocated = torch.cuda.memory_allocated(0)
                gpu_cached = torch.cuda.memory_reserved(0)

                usage.update(
                    {
                        "gpu_total": f"{gpu_memory / 1024**3:.1f}GB",
                        "gpu_allocated": f"{gpu_allocated / 1024**3:.1f}GB",
                        "gpu_cached": f"{gpu_cached / 1024**3:.1f}GB",
                        "gpu_percent": f"{(gpu_allocated / gpu_memory) * 100:.1f}%",
                    }
                )

            return usage

        except ImportError:
            return {"status": "psutil not available"}
        except Exception as e:
            return {"error": str(e)}

    def clear_cache(self) -> None:
        """ìºì‹œ ì •ë¦¬"""
        self.query_cache.clear()
        logger.info("ğŸ—‘ï¸ Query cache cleared")

    def shutdown(self) -> None:
        """ì‹œìŠ¤í…œ ì¢…ë£Œ ë° ì •ë¦¬"""
        logger.info("ğŸ”Œ Shutting down GraphRAG Pipeline...")

        # LLM ì–¸ë¡œë“œ
        if self.llm_manager:
            self.llm_manager.unload_model()

        # ìºì‹œ ì •ë¦¬
        self.clear_cache()

        # ìƒíƒœ ë¦¬ì…‹
        self.state.status = PipelineStatus.UNINITIALIZED
        self.embeddings_loaded = False

        logger.info("âœ… Pipeline shutdown completed")


def main():
    """GraphRAG Pipeline í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Testing GraphRAG Pipeline...")

    try:
        # 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = GraphRAGPipeline(config_file="graphrag_config.yaml", auto_setup=True)

        # 2. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        status = pipeline.get_system_status()
        print(f"ğŸ“Š System Status:")
        print(f"   Pipeline: {status['pipeline_state']['status']}")
        print(f"   Components: {status['components']}")

        # 3. ì„ë² ë”© êµ¬ì¶• (í•„ìš”í•œ ê²½ìš°)
        if not status["embeddings_loaded"]:
            print(f"\nğŸ—ï¸ Building embeddings...")
            build_result = pipeline.build_embeddings()
            print(f"âœ… Built {build_result['total_embeddings']} embeddings")

        # 4. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        test_queries = [
            "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì€?",
            "AI ë° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì´ ì ìš©ëœ ì£¼ìš” taskëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            "ë°°í„°ë¦¬ ì „ê·¹ ê³µì •ì—ì„œ AIê°€ ì ìš©ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œìš”?",
        ]

        print(f"\nâ“ Testing queries...")
        for i, query in enumerate(test_queries[:1]):  # ì²« ë²ˆì§¸ë§Œ í…ŒìŠ¤íŠ¸
            print(f"\n{i+1}. {query}")

            result = pipeline.ask(query, return_context=True)

            print(f"âœ… Answer: {result.answer[:200]}...")
            print(
                f"ğŸ“Š Stats: {result.processing_time:.2f}s, {result.confidence_score:.3f} confidence"
            )
            print(f"ğŸ“„ Sources: {len(result.source_nodes)} nodes")

        # 5. ìµœì¢… ìƒíƒœ
        final_status = pipeline.get_system_status()
        print(f"\nğŸ“ˆ Final Stats:")
        print(
            f"   Queries processed: {final_status['pipeline_state']['total_queries_processed']}"
        )
        print(f"   Cache size: {final_status['cache_size']}")

        print(f"\nâœ… GraphRAG Pipeline test completed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
