"""
GraphRAG ë©”ì¸ íŒŒì´í”„ë¼ì¸
Main GraphRAG Pipeline Integration

ì „ì²´ GraphRAG ì‹œìŠ¤í…œì˜ í†µí•© ì¸í„°íŽ˜ì´ìŠ¤
- ì„¤ì • ê´€ë¦¬ ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
- ìž„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ìž¥ì†Œ êµ¬ì¶•
- ì¿¼ë¦¬ ë¶„ì„ ë° ì„œë¸Œê·¸ëž˜í”„ ì¶”ì¶œ
- ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™” ë° LLM ì—°ë™
- í†µí•© QA ì¸í„°íŽ˜ì´ìŠ¤ ì œê³µ
"""

import os
import json
import time
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np

# GraphRAG ì»´í¬ë„ŒíŠ¸ë“¤
try:
    from .config_manager import GraphRAGConfigManager
    from .query_analyzer import QueryAnalyzer, QueryAnalysisResult
    from .embeddings import (
        MultiNodeEmbedder,
        VectorStoreManager,
        create_embedding_model,
        is_ready as embeddings_ready,
    )
    from .embeddings.subgraph_extractor import SubgraphExtractor, SubgraphResult
    from .embeddings.context_serializer import ContextSerializer, SerializedContext
except ImportError as e:
    warnings.warn(f"Some GraphRAG components not available: {e}")

# LLM ê´€ë ¨
try:
    import torch
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        GenerationConfig,
        BitsAndBytesConfig,
    )

    _transformers_available = True
except ImportError:
    _transformers_available = False
    warnings.warn("Transformers not available. Local LLM will not work.")
# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# try:
#     from .langchain.qa_chain_builder import (
#         create_qa_chain_from_pipeline,
#         replace_pipeline_llm_with_qa_chain,
#         validate_qa_chain_integration,
#     )

#     _qa_chain_available = True
#     logger.info("âœ… QA Chain integration available")
# except ImportError as e:
#     _qa_chain_available = False
#     logger.warning(f"âš ï¸ QA Chain not available: {e}")

# âœ… ì „ì—­ ë³€ìˆ˜ëŠ” ìœ ì§€
try:
    # ì¼ë¶€ ì²´í¬ë§Œ ìˆ˜í–‰
    _qa_chain_available = True
    logger.info("âœ… QA Chain integration available")
except ImportError as e:
    _qa_chain_available = False
    logger.warning(f"âš ï¸ QA Chain not available: {e}")


class PipelineStatus(Enum):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""

    UNINITIALIZED = "uninitialized"
    INITIALIZING = "initializing"
    READY = "ready"
    PROCESSING = "processing"
    ERROR = "error"


@dataclass
class PipelineState:
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´"""

    status: PipelineStatus
    components_loaded: Dict[str, bool]
    last_error: Optional[str]
    initialization_time: Optional[float]
    total_queries_processed: int
    last_query_time: Optional[float]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {**asdict(self), "status": self.status.value}


@dataclass
class QAResult:
    """QA ê²°ê³¼ í´ëž˜ìŠ¤"""

    query: str
    answer: str
    subgraph_result: Optional[SubgraphResult]
    serialized_context: Optional[SerializedContext]
    query_analysis: Optional[QueryAnalysisResult]
    processing_time: float
    confidence_score: float
    source_nodes: List[str]

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "query": self.query,
            "answer": self.answer,
            "processing_time": self.processing_time,
            "confidence_score": self.confidence_score,
            "source_nodes": self.source_nodes,
            "subgraph_nodes": (
                self.subgraph_result.total_nodes if self.subgraph_result else 0
            ),
            "subgraph_edges": (
                self.subgraph_result.total_edges if self.subgraph_result else 0
            ),
            "query_complexity": (
                self.query_analysis.complexity.value
                if self.query_analysis
                else "unknown"
            ),
            "query_type": (
                self.query_analysis.query_type.value
                if self.query_analysis
                else "unknown"
            ),
        }


class LocalLLMManager:
    """ë¡œì»¬ LLM ê´€ë¦¬ í´ëž˜ìŠ¤"""

    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: LLM ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.model = None
        self.tokenizer = None
        self.generation_config = None
        self.is_loaded = False

    def load_model(self) -> None:
        """ëª¨ë¸ ë¡œë“œ - HuggingFace ID ì§€ì›"""
        if not _transformers_available:
            raise ImportError("Transformers not available for local LLM")

        model_path = self.config.get("model_path")

        logger.info(f"ðŸ¤– Loading model: {model_path}")

        # âœ… HuggingFace ID vs ë¡œì»¬ ê²½ë¡œ êµ¬ë¶„
        if "/" in model_path and not model_path.startswith("/"):
            # HuggingFace Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
            logger.info(f"ðŸ“¥ Downloading from HuggingFace Hub: {model_path}")
            use_auth_token = os.getenv("HUGGINGFACE_TOKEN")  # í•„ìš”ì‹œ í† í° ì‚¬ìš©
        else:
            # ë¡œì»¬ ê²½ë¡œ ê²€ì¦
            if not Path(model_path).exists():
                raise FileNotFoundError(f"Local model not found: {model_path}")
            logger.info(f"ðŸ“‚ Loading from local path: {model_path}")
            use_auth_token = None

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=self.config.get("trust_remote_code", True),
            use_auth_token=use_auth_token,
        )

        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # ì–‘ìží™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
        quantization_config = None
        if self.config.get("load_in_4bit", False):
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
        elif self.config.get("load_in_8bit", False):
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)

        # ëª¨ë¸ ë¡œë“œ
        model_kwargs = {
            "trust_remote_code": self.config.get("trust_remote_code", True),
            "device_map": self.config.get("device_map", "auto"),
            "torch_dtype": getattr(torch, self.config.get("torch_dtype", "bfloat16")),
        }

        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config

        self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)

        # ìƒì„± ì„¤ì •
        self.generation_config = GenerationConfig(
            temperature=self.config.get("temperature", 0.1),
            max_new_tokens=self.config.get("max_new_tokens", 2048),
            do_sample=self.config.get("do_sample", True),
            top_p=self.config.get("top_p", 0.9),
            top_k=self.config.get("top_k", 50),
            repetition_penalty=self.config.get("repetition_penalty", 1.1),
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )

        self.is_loaded = True
        logger.info("âœ… Local LLM loaded successfully")

    # except Exception as e:
    #     logger.error(f"âŒ Failed to load local LLM: {e}")
    #     raise

    def generate(self, prompt: str, max_length: Optional[int] = None) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.is_loaded:
            self.load_model()

        # try:
        # í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096,
            padding=True,
        )

        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # attention_mask í™•ì¸
        if "attention_mask" not in inputs:
            inputs["attention_mask"] = torch.ones_like(inputs["input_ids"])

        # YAML ì„¤ì • ê¸°ë°˜ ìƒì„± ì„¤ì • (ê¸°ë³¸ê°’ì€ ì•ˆì „í•˜ê²Œ)
        generation_config = GenerationConfig(
            temperature=max(
                0.01, min(2.0, self.config.get("temperature", 0.1))
            ),  # ì•ˆì „ ë²”ìœ„
            max_new_tokens=min(
                max_length or self.config.get("max_new_tokens", 512),
                self.config.get("max_new_tokens", 512),
            ),
            do_sample=self.config.get("do_sample", True),  # YAML ì„¤ì • ìš°ì„ 
            top_p=max(0.1, min(1.0, self.config.get("top_p", 0.9))),  # ì•ˆì „ ë²”ìœ„
            top_k=max(1, min(100, self.config.get("top_k", 50))),  # ì•ˆì „ ë²”ìœ„
            repetition_penalty=max(
                1.0, min(2.0, self.config.get("repetition_penalty", 1.1))
            ),  # ì•ˆì „ ë²”ìœ„
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            use_cache=True,
        )

        logger.info(
            f"ðŸ” Generation config: temp={generation_config.temperature}, "
            f"do_sample={generation_config.do_sample}, "
            f"max_tokens={generation_config.max_new_tokens}"
        )

        # ì²« ë²ˆì§¸ ì‹œë„: YAML ì„¤ì •ëŒ€ë¡œ
        with torch.no_grad():
            # try:
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                generation_config=generation_config,
                use_cache=True,
            )

            # except RuntimeError as cuda_error:
            #     if "CUDA" in str(cuda_error) or "device-side assert" in str(cuda_error):
            #         logger.warning(f"âš ï¸ CUDA error with YAML settings: {cuda_error}")
            #         logger.info("ðŸ”„ Retrying with safer settings...")

            #         # ë‘ ë²ˆì§¸ ì‹œë„: ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ìž¬ì‹œë„
            #         return self._retry_with_safe_settings(inputs, max_length)
            #     else:
            #         raise

        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_tokens = outputs[0][inputs["input_ids"].shape[1] :]
        generated_text = self.tokenizer.decode(
            generated_tokens,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        )

        result = generated_text.strip()
        if not result:
            result = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

        logger.info(f"âœ… Generated {len(result)} characters with YAML settings")
        return result

    # except Exception as e:
    #     logger.error(f"âŒ Text generation failed: {e}")
    #     return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def unload_model(self) -> None:
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.model:
            del self.model
            self.model = None
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.is_loaded = False
        logger.info("ðŸ—‘ï¸ Local LLM unloaded")


class GraphRAGPipeline:
    """GraphRAG ë©”ì¸ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        config_file: str = "graphrag_config.yaml",
        env_file: Optional[str] = None,
        auto_setup: bool = False,
    ):
        """
        Args:
            config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            env_file: í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ
            auto_setup: ìžë™ ì´ˆê¸°í™” ì—¬ë¶€
        """
        self.config_file = Path(config_file)
        self.env_file = Path(env_file) if env_file else None

        # ìƒíƒœ ê´€ë¦¬
        self.state = PipelineState(
            status=PipelineStatus.UNINITIALIZED,
            components_loaded={},
            last_error=None,
            initialization_time=None,
            total_queries_processed=0,
            last_query_time=None,
        )

        # ì»´í¬ë„ŒíŠ¸ë“¤
        self.config_manager = None
        self.query_analyzer = None
        self.embedder = None
        self.vector_store = None
        self.subgraph_extractor = None
        self.context_serializer = None
        self.llm_manager = None

        # ìºì‹œ
        self.query_cache = {}
        self.embeddings_loaded = False

        logger.info("ðŸš€ GraphRAG Pipeline initialized")

        if auto_setup:
            self.setup()

    def enable_qa_chain_optimization(self) -> None:
        """QA Chain ìµœì í™” í™œì„±í™”"""
        try:
            # âœ… ì§€ì—° importë§Œ ì‚¬ìš©
            from .langchain.qa_chain_builder import (
                create_qa_chain_from_pipeline,
                replace_pipeline_llm_with_qa_chain,
            )

            # ê²€ì¦ë„ ì§€ì—° importë¡œ
            from .langchain.qa_chain_builder import validate_qa_chain_integration

            validation = validate_qa_chain_integration(self.config_manager)

            if validation.get("status") not in ["ready", "partial"]:
                logger.warning("âš ï¸ QA Chain not ready for activation")
                logger.warning(f"   Status: {validation.get('status')}")
                if validation.get("recommendations"):
                    logger.warning("   Recommendations:")
                    for rec in validation["recommendations"][:3]:
                        logger.warning(f"      â€¢ {rec}")
                return

            if not hasattr(self, "_original_ask"):
                # ì›ë³¸ ë©”ì„œë“œ ë°±ì—…
                self._original_ask = self.ask

                # QA Chainìœ¼ë¡œ êµì²´
                optimized_pipeline = replace_pipeline_llm_with_qa_chain(self)
                self.ask = optimized_pipeline.ask
                self._qa_chain = getattr(optimized_pipeline, "_qa_chain", None)

                logger.info("âœ… QA Chain optimization enabled successfully")
                logger.info(
                    "ðŸ’¡ Use pipeline.ask() as usual - now with LangChain optimization!"
                )
            else:
                logger.info("â„¹ï¸ QA Chain optimization already enabled")

        except ImportError as e:
            logger.warning(f"âš ï¸ QA Chain not available: {e}")
            logger.info("ðŸ”„ Keeping original ask method")
            return
        except Exception as e:
            logger.error(f"âŒ Failed to enable QA Chain optimization: {e}")
            logger.info("ðŸ”„ Keeping original ask method")
            return

    def disable_qa_chain_optimization(self) -> None:
        """QA Chain ìµœì í™” ë¹„í™œì„±í™” (ì›ë³¸ ask ë©”ì„œë“œ ë³µì›)"""

        if hasattr(self, "_original_ask"):
            logger.info("ðŸ”„ Disabling QA Chain optimization...")
            self.ask = self._original_ask
            delattr(self, "_original_ask")
            if hasattr(self, "_qa_chain"):
                delattr(self, "_qa_chain")
            logger.info("âœ… Original ask method restored")
        else:
            logger.info("â„¹ï¸ QA Chain optimization not currently enabled")

    def get_qa_chain_stats(self) -> Optional[Dict[str, Any]]:
        """QA Chain ì‚¬ìš© í†µê³„ ì¡°íšŒ"""

        if hasattr(self, "_qa_chain") and self._qa_chain:
            try:
                # âœ… _llm ëŒ€ì‹  llm ì‚¬ìš©
                if hasattr(self._qa_chain, "llm") and hasattr(
                    self._qa_chain.llm, "get_usage_stats"
                ):
                    return self._qa_chain.llm.get_usage_stats()
                else:
                    return {"message": "LLM stats not available"}
            except Exception as e:
                logger.warning(f"âš ï¸ Could not get QA Chain stats: {e}")
        return None

    def validate_qa_chain_integration(self) -> Dict[str, Any]:
        """QA Chain í†µí•© ê°€ëŠ¥ì„± ê²€ì¦"""

        if not _qa_chain_available:
            return {
                "status": "not_available",
                "reason": "QA Chain components not imported",
            }

        return validate_qa_chain_integration(self.config_manager)

    def setup(self) -> None:
        """ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”"""
        logger.info("ðŸ”§ Setting up GraphRAG Pipeline...")
        self.state.status = PipelineStatus.INITIALIZING

        start_time = time.time()

        try:
            # 1. ì„¤ì • ê´€ë¦¬ìž ì´ˆê¸°í™”
            self._setup_config_manager()

            # 2. ì¿¼ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
            self._setup_query_analyzer()

            # 3. ìž„ë² ë”© ì‹œìŠ¤í…œ í™•ì¸
            self._check_embeddings_system()

            # 4. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”ê¸° ì´ˆê¸°í™”
            self._setup_context_serializer()

            # 5. LLM ê´€ë¦¬ìž ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
            self._setup_llm_manager()

            # 6. QA Chain ìµœì í™” í™•ì¸ (ìƒˆë¡œ ì¶”ê°€)
            self._check_qa_chain_availability()

            # 7. ì‹œìŠ¤í…œ ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ #6ì„ #7ë¡œ ë³€ê²½)
            self.state.initialization_time = time.time() - start_time
            self.state.status = PipelineStatus.READY
            self.state.last_error = None

            logger.info(
                f"âœ… GraphRAG Pipeline setup completed ({self.state.initialization_time:.2f}s)"
            )

        except Exception as e:
            self.state.status = PipelineStatus.ERROR
            self.state.last_error = str(e)
            logger.error(f"âŒ Pipeline setup failed: {e}")
            raise

    def _check_qa_chain_availability(self) -> None:
        """QA Chain ìµœì í™” ê°€ìš©ì„± í™•ì¸"""
        logger.info("ðŸ” Checking QA Chain optimization availability...")

        try:
            # âœ… ì§€ì—° importë¡œ ìˆœí™˜ import ë°©ì§€
            from .langchain.qa_chain_builder import validate_qa_chain_integration

            validation = validate_qa_chain_integration(self.config_manager)
            status = validation.get("status", "unknown")

            if status == "ready":
                logger.info("ðŸŽ¯ QA Chain optimization ready for activation")
                logger.info(
                    "ðŸ’¡ Call pipeline.enable_qa_chain_optimization() to activate"
                )
                self.state.components_loaded["qa_chain_ready"] = True
            elif status == "partial":
                logger.info("âš ï¸ QA Chain partially available - some components missing")
                self.state.components_loaded["qa_chain_ready"] = False

                # ê¶Œìž¥ì‚¬í•­ ì¶œë ¥
                recommendations = validation.get("recommendations", [])
                if recommendations:
                    logger.info("ðŸ“‹ Recommendations:")
                    for rec in recommendations[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                        logger.info(f"   â€¢ {rec}")
            else:
                logger.info(f"â„¹ï¸ QA Chain integration status: {status}")
                self.state.components_loaded["qa_chain_ready"] = False

        except ImportError as e:
            logger.warning(f"âš ï¸ QA Chain not available: {e}")
            self.state.components_loaded["qa_chain_ready"] = False
        except Exception as e:
            logger.debug(f"QA Chain validation failed: {e}")
            self.state.components_loaded["qa_chain_ready"] = False

    def _setup_config_manager(self) -> None:
        """ì„¤ì • ê´€ë¦¬ìž ì´ˆê¸°í™”"""
        logger.info("ðŸ“‹ Setting up config manager...")

        self.config_manager = GraphRAGConfigManager(
            config_file=str(self.config_file),
            env_file=str(self.env_file) if self.env_file else None,
        )

        self.state.components_loaded["config_manager"] = True
        logger.info("âœ… Config manager ready")

    def _setup_query_analyzer(self) -> None:
        """ì¿¼ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        logger.info("ðŸ” Setting up query analyzer...")

        self.query_analyzer = QueryAnalyzer()
        self.state.components_loaded["query_analyzer"] = True
        logger.info("âœ… Query analyzer ready")

    def _check_embeddings_system(self) -> None:
        """ìž„ë² ë”© ì‹œìŠ¤í…œ í™•ì¸"""
        logger.info("ðŸ” Checking embeddings system...")

        if not embeddings_ready():
            logger.warning("âš ï¸ Embeddings system not fully ready")
            self.state.components_loaded["embeddings"] = False
        else:
            self.state.components_loaded["embeddings"] = True
            logger.info("âœ… Embeddings system ready")

    def _setup_context_serializer(self) -> None:
        """ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”ê¸° ì´ˆê¸°í™”"""
        logger.info("ðŸ“ Setting up context serializer...")

        self.context_serializer = ContextSerializer()
        self.state.components_loaded["context_serializer"] = True
        logger.info("âœ… Context serializer ready")

    def _setup_llm_manager(self) -> None:
        """LLM ê´€ë¦¬ìž ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        logger.info("ðŸ¤– Setting up LLM manager...")

        llm_config = self.config_manager.get_llm_config()

        if llm_config.get("model_path"):  # ë¡œì»¬ ëª¨ë¸
            self.llm_manager = LocalLLMManager(llm_config)
            self.state.components_loaded["llm_manager"] = True
            logger.info("âœ… Local LLM manager ready (model will be loaded on demand)")
        else:
            logger.warning("âš ï¸ No local LLM configuration found")
            self.state.components_loaded["llm_manager"] = False

    def _ensure_embeddings_loaded(self) -> None:
        """ìž„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ í™•ì¸ - ëª¨ë“  model_type ì§€ì›"""
        if self.embeddings_loaded:
            return

        logger.info("ðŸ“¥ Loading embeddings system...")

        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        config = self.config_manager.config

        # í†µí•© ê·¸ëž˜í”„ íŒŒì¼ í™•ì¸
        unified_graph_path = config.graph.unified_graph_path
        if not Path(unified_graph_path).exists():
            raise FileNotFoundError(f"Unified graph not found: {unified_graph_path}")

        # ë²¡í„° ì €ìž¥ì†Œ ê²½ë¡œ í™•ì¸
        vector_store_root = config.paths.vector_store_root
        if not Path(vector_store_root).exists():
            Path(vector_store_root).mkdir(parents=True, exist_ok=True)
            logger.info(f"ðŸ“ Created vector store directory: {vector_store_root}")

        # ë²¡í„° ì €ìž¥ì†Œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        vector_config = self.config_manager.get_vector_store_config()

        # VectorStoreManager ì´ˆê¸°í™”
        from .embeddings.vector_store_manager import VectorStoreManager

        self.vector_store = VectorStoreManager(
            store_type=vector_config["store_type"],
            persist_directory=vector_config["persist_directory"],
            **{
                k: v
                for k, v in vector_config.items()
                if k not in ["store_type", "persist_directory"]
            },
        )

        # ìž„ë² ë”© ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  íƒ€ìž… ì§€ì›)
        embedding_config = self.config_manager.get_embeddings_config()  # âœ… ìœ ì—°í•¨

        from .embeddings import create_embedding_model

        embedder_model = create_embedding_model(
            model_name=embedding_config["model_name"],  # âœ… íƒ€ìž… ë¬´ê´€
            device=embedding_config["device"],  # âœ… íƒ€ìž… ë¬´ê´€
            cache_dir=embedding_config["cache_dir"],  # âœ… íƒ€ìž… ë¬´ê´€
        )

        # MultiNodeEmbedder ì´ˆê¸°í™”
        from .embeddings.multi_node_embedder import MultiNodeEmbedder

        self.embedder = MultiNodeEmbedder(
            unified_graph_path=config.graph.unified_graph_path,
            embedding_model=embedder_model,
            vector_store=self.vector_store,
            batch_size=embedding_config["batch_size"],  # âœ… íƒ€ìž… ë¬´ê´€
        )

        # SubgraphExtractor ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì„¤ì • êµ¬ì¡°)
        embedding_config = self.config_manager.get_embeddings_config()
        from .embeddings.subgraph_extractor import SubgraphExtractor

        self.subgraph_extractor = SubgraphExtractor(
            unified_graph_path=config.graph.unified_graph_path,
            vector_store_path=config.paths.vector_store_root,  # âœ… ê²½ë¡œ ì „ë‹¬
            embedding_model=embedding_config["model_name"],
            device=embedding_config["device"],
        )

        self.embeddings_loaded = True
        logger.info("âœ… Embeddings system loaded with flexible config structure")

    def build_embeddings(self, force_rebuild: bool = False) -> Dict[str, Any]:
        """ìž„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ìž¥ì†Œ êµ¬ì¶• - ìƒˆë¡œìš´ ê²½ë¡œ êµ¬ì¡° ì‚¬ìš©"""
        logger.info("ðŸ—ï¸ Building embeddings and vector store...")

        if self.state.status != PipelineStatus.READY:
            raise RuntimeError("Pipeline not ready. Call setup() first.")

        config = self.config_manager.config

        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.config_manager._create_directories()

        # ìž„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™” (ì„¤ì • ê´€ë¦¬ìž ì‚¬ìš©)
        from .embeddings.multi_node_embedder import create_embedder_with_config

        embedding_config = self.config_manager.get_embeddings_config()
        self.embedder = create_embedder_with_config(
            unified_graph_path=config.graph.unified_graph_path,
            config_manager=self.config_manager,
            device=embedding_config["device"],
        )

        # ë²¡í„° ì €ìž¥ì†Œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        vector_store_config = self.config_manager.get_vector_store_config()

        # ìž„ë² ë”© ìƒì„± (ìƒˆë¡œìš´ ê²½ë¡œ êµ¬ì¡°ë¡œ)
        embedding_results, saved_files = self.embedder.run_full_pipeline(
            output_dir=config.graph.vector_store_path,  # ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            use_cache=not force_rebuild,
            show_progress=True,
            vector_store_config=vector_store_config,
        )

        # ë²¡í„° ì €ìž¥ì†Œ êµ¬ì¶• (ì„¤ì • ê´€ë¦¬ìž ì‚¬ìš©)
        from .embeddings.vector_store_manager import create_vector_store_from_config

        self.vector_store = create_vector_store_from_config(
            config_manager=self.config_manager,
            store_type=vector_store_config["store_type"],
        )

        # ìž„ë² ë”© ê²°ê³¼ë¡œë¶€í„° ë²¡í„° ì €ìž¥ì†Œ ë¡œë“œ
        self.vector_store.load_from_embeddings(
            embedding_results,
            embeddings_dir=config.paths.vector_store.embeddings,
        )

        # í†µê³„ ë°˜í™˜
        total_nodes = sum(len(results) for results in embedding_results.values())

        result = {
            "total_embeddings": total_nodes,
            "embeddings_by_type": {k: len(v) for k, v in embedding_results.items()},
            "saved_files": {k: str(v) for k, v in saved_files.items()},
            "vector_store_info": self.vector_store.get_store_info(),
            "path_structure": {
                "vector_store_root": config.graph.vector_store_path,
                "embeddings_dir": config.paths.vector_store.embeddings,
                "store_specific_dir": vector_store_config["persist_directory"],
            },
        }

        logger.info(f"âœ… Embeddings built: {total_nodes:,} nodes")
        logger.info(f"ðŸ“‚ Structure created:")
        logger.info(f"   Root: {config.graph.vector_store_path}")
        logger.info(f"   Embeddings: {config.paths.vector_store.embeddings}")
        logger.info(f"   Vector Store: {vector_store_config['persist_directory']}")

        return result

    def ask(self, query: str, return_context: bool = False) -> Union[str, QAResult]:
        """ë©”ì¸ QA ì¸í„°íŽ˜ì´ìŠ¤

        Args:
            query: ì‚¬ìš©ìž ì§ˆë¬¸
            return_context: ìƒì„¸ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ ì—¬ë¶€

        Returns:
            ë‹µë³€ ë¬¸ìžì—´ ë˜ëŠ” QAResult ê°ì²´
        """
        if self.state.status != PipelineStatus.READY:
            raise RuntimeError("Pipeline not ready. Call setup() first.")

        start_time = time.time()
        self.state.status = PipelineStatus.PROCESSING

        # try:
        logger.info(f"â“ Processing query: '{query[:50]}...'")

        # 1. ìºì‹œ í™•ì¸
        if query in self.query_cache:
            logger.info("âœ… Cache hit")
            cached_result = self.query_cache[query]
            if return_context:
                return cached_result
            else:
                return cached_result.answer

        # 2. ìž„ë² ë”© ì‹œìŠ¤í…œ ë¡œë“œ
        self._ensure_embeddings_loaded()

        # 3. ì¿¼ë¦¬ ë¶„ì„
        query_analysis = self.query_analyzer.analyze(query)

        # 4. ì„œë¸Œê·¸ëž˜í”„ ì¶”ì¶œ
        subgraph_result = self.subgraph_extractor.extract_subgraph(
            query=query, query_analysis=query_analysis
        )

        # 5. ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™”
        serialized_context = self.context_serializer.serialize(
            subgraph_result=subgraph_result, query_analysis=query_analysis
        )

        # 6. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        context_text = getattr(serialized_context, "main_text", "") or ""
        if not isinstance(context_text, str):
            context_text = str(context_text) if context_text else "No context available"

        answer = self._generate_answer(query, context_text)
        # answer = self._generate_answer(query, serialized_context.main_text)

        # 7. ê²°ê³¼ êµ¬ì„±
        processing_time = time.time() - start_time

        qa_result = QAResult(
            query=query,
            answer=answer,
            subgraph_result=subgraph_result,
            serialized_context=serialized_context,
            query_analysis=query_analysis,
            processing_time=processing_time,
            confidence_score=subgraph_result.confidence_score,
            source_nodes=list(subgraph_result.nodes.keys()),
        )

        # 8. ìºì‹œ ì €ìž¥
        self.query_cache[query] = qa_result

        # 9. ìƒíƒœ ì—…ë°ì´íŠ¸
        self.state.total_queries_processed += 1
        self.state.last_query_time = processing_time
        self.state.status = PipelineStatus.READY

        logger.info(f"âœ… Query processed ({processing_time:.2f}s)")

        if return_context:
            return qa_result
        else:
            return answer

    # except Exception as e:
    #     self.state.status = PipelineStatus.ERROR
    #     self.state.last_error = str(e)
    #     logger.error(f"âŒ Query processing failed: {e}")

    #     # ê°„ë‹¨í•œ ë‹µë³€ ë°˜í™˜
    #     fallback_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    #     if return_context:
    #         return QAResult(
    #             query=query,
    #             answer=fallback_answer,
    #             subgraph_result=None,
    #             serialized_context=None,
    #             query_analysis=None,
    #             processing_time=time.time() - start_time,
    #             confidence_score=0.0,
    #             source_nodes=[],
    #         )
    #     else:
    #         return fallback_answer

    def _generate_answer(self, query: str, context: str) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì•ˆì „í•œ ë²„ì „)"""

        # ìž…ë ¥ ê²€ì¦
        if not isinstance(query, str):
            logger.error(f"âŒ Query is not a string: {type(query)} - {query}")
            query = str(query) if query is not None else "Unknown query"

        if not isinstance(context, str):
            logger.error(f"âŒ Context is not a string: {type(context)} - {context}")
            context = str(context) if context is not None else "No context available"

        # ë¹ˆ ë¬¸ìžì—´ ì²˜ë¦¬
        if not query.strip():
            query = "Unknown query"

        if not context.strip():
            context = "No context available"

        # try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_qa_prompt(query, context)

        # í”„ë¡¬í”„íŠ¸ ê²€ì¦
        if not isinstance(prompt, str):
            logger.error(f"âŒ Prompt is not a string: {type(prompt)}")
            prompt = f"ì§ˆë¬¸: {query}\në‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."

        logger.debug(f"ðŸ” Generated prompt length: {len(prompt)}")

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        if self.llm_manager and self.llm_manager.config.get("model_path"):
            # ë¡œì»¬ LLM ì‚¬ìš©
            # try:
            logger.info("ðŸ¤– Generating answer with local LLM...")
            answer = self.llm_manager.generate(prompt, max_length=1000)

            # ë‹µë³€ ê²€ì¦
            if not isinstance(answer, str):
                logger.error(f"âŒ LLM returned non-string: {type(answer)}")
                return f"LLM ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            if not answer.strip():
                logger.warning("âš ï¸ LLM returned empty response")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

            return answer.strip()

        # except Exception as e:
        #     logger.error(f"âŒ Local LLM generation failed: {e}")
        #     logger.debug(f"âŒ Prompt that caused error: {prompt[:200]}...")
        #     return f"ë¡œì»¬ LLM ì˜¤ë¥˜: {str(e)}"
        else:
            # API LLM í´ë°± ë˜ëŠ” ê¸°ë³¸ ë‹µë³€
            return (
                "ì£„ì†¡í•©ë‹ˆë‹¤. LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

    # except Exception as e:
    #     logger.error(f"âŒ Answer generation failed: {e}")
    #     return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _build_qa_prompt(self, query: str, context: str) -> str:
        """QA í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì•ˆì „í•œ ë²„ì „)"""

        # ìž…ë ¥ ê²€ì¦ ë° ì •ë¦¬
        query = str(query).strip() if query else "Unknown query"
        context = str(context).strip() if context else "No context available"

        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í¬ë‚˜ì´ì € í•œê³„ ê³ ë ¤)
        max_context_length = 3000  # ì•ˆì „í•œ ê¸¸ì´
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
            logger.warning(f"âš ï¸ Context truncated to {max_context_length} chars")

        prompt_template = """ë‹¤ìŒì€ í•™ìˆ  ì—°êµ¬ ë¬¸í—Œ ë¶„ì„ ì‹œìŠ¤í…œìž…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

    **ì»¨í…ìŠ¤íŠ¸:**
    {context}

    **ì§ˆë¬¸:** {query}

    **ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
    1. ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
    2. í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ì ì ˆížˆ í˜¼ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
    3. êµ¬ì²´ì ì¸ ë…¼ë¬¸, ì €ìž, ì—°êµ¬ ê²°ê³¼ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”
    4. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œí•˜ì„¸ìš”
    5. ë‹µë³€ì€ 500ìž ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš”

    **ë‹µë³€:**"""

        try:
            formatted_prompt = prompt_template.format(context=context, query=query)

            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¸
            if len(formatted_prompt) > 8000:  # í† í¬ë‚˜ì´ì € í•œê³„ ê³ ë ¤
                logger.warning(
                    f"âš ï¸ Prompt too long: {len(formatted_prompt)} chars, truncating..."
                )
                # ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ì¤„ìž„
                shorter_context = context[:1500] + "..."
                formatted_prompt = prompt_template.format(
                    context=shorter_context, query=query
                )

            return formatted_prompt

        except Exception as e:
            logger.error(f"âŒ Prompt formatting failed: {e}")
            # ìµœì†Œí•œì˜ ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸
            return f"ì§ˆë¬¸: {query}\n\nìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”."

    def get_subgraph(self, query: str) -> Optional[SubgraphResult]:
        """ì„œë¸Œê·¸ëž˜í”„ë§Œ ì¶”ì¶œ (LLM ì—†ì´)"""
        try:
            self._ensure_embeddings_loaded()

            query_analysis = self.query_analyzer.analyze(query)
            subgraph_result = self.subgraph_extractor.extract_subgraph(
                query=query, query_analysis=query_analysis
            )

            return subgraph_result

        except Exception as e:
            logger.error(f"âŒ Subgraph extraction failed: {e}")
            return None

    def batch_process(self, queries: List[str], max_workers: int = 2) -> List[QAResult]:
        """ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬"""
        logger.info(f"ðŸ“‹ Processing {len(queries)} queries in batch...")

        results = []
        for i, query in enumerate(queries):
            logger.info(f"Processing query {i+1}/{len(queries)}")
            result = self.ask(query, return_context=True)
            results.append(result)

        logger.info(f"âœ… Batch processing completed: {len(results)} results")
        return results

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ - ê²½ë¡œ ì •ë³´ í¬í•¨"""
        status = {
            "pipeline_state": self.state.to_dict(),
            "components": self.state.components_loaded,
            "embeddings_loaded": self.embeddings_loaded,
            "cache_size": len(self.query_cache),
            "memory_usage": self._get_memory_usage(),
        }

        # ì„¤ì • ê´€ë¦¬ìž ì •ë³´
        if self.config_manager:
            config = self.config_manager.config
            embedding_config = self.config_manager.get_embeddings_config()
            status["configuration"] = {
                "llm_provider": config.llm.provider,
                "embedding_model": embedding_config["model_name"],
                "vector_store_type": config.vector_store.store_type,
                "paths": {
                    "unified_graph": config.graph.unified_graph_path,
                    "vector_store_root": config.paths.vector_store_root,
                    "embeddings_dir": config.paths.vector_store.embeddings,
                    "store_directory": self.config_manager.get_vector_store_config()[
                        "persist_directory"
                    ],
                },
            }

        # ë²¡í„° ì €ìž¥ì†Œ ì •ë³´
        if self.vector_store:
            status["vector_store"] = self.vector_store.get_store_info()

        # LLM ìƒíƒœ
        if self.llm_manager:
            status["llm_loaded"] = self.llm_manager.is_loaded
            status["llm_model_path"] = self.llm_manager.config.get("model_path")

        # QA Chain ìƒíƒœ ì¶”ê°€
        status["qa_chain"] = {
            "available": _qa_chain_available,
            "enabled": hasattr(self, "_qa_chain"),
            "ready": self.state.components_loaded.get("qa_chain_ready", False),
            "stats": self.get_qa_chain_stats(),
        }
        return status

    def setup_from_config(
        self,
        config_file: str = "graphrag_config.yaml",
        auto_build_embeddings: bool = False,
        enable_qa_chain: bool = True,  # ìƒˆ íŒŒë¼ë¯¸í„° ì¶”ê°€
    ) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ë¡œë¶€í„° ì™„ì „ ìžë™ ì„¤ì •"""

        logger.info(f"ðŸš€ Setting up GraphRAG from config: {config_file}")

        # ì„¤ì • íŒŒì¼ë¡œ ì´ˆê¸°í™”
        self.config_file = Path(config_file)

        # ì„¤ì • ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.setup()

        setup_result = {
            "setup_completed": True,
            "config_loaded": True,
            "directories_created": True,
        }

        # ìžë™ ìž„ë² ë”© êµ¬ì¶•
        if auto_build_embeddings:
            try:
                build_result = self.build_embeddings()
                setup_result.update(
                    {
                        "embeddings_built": True,
                        "embedding_stats": build_result,
                    }
                )
            except Exception as e:
                logger.warning(f"âš ï¸ Auto embedding build failed: {e}")
                setup_result["embeddings_built"] = False
                setup_result["embedding_error"] = str(e)

        # QA Chain ìžë™ í™œì„±í™” (ìƒˆë¡œ ì¶”ê°€)
        if enable_qa_chain:
            try:
                logger.info("ðŸ”— Attempting to enable QA Chain optimization...")
                self.enable_qa_chain_optimization()
                setup_result["qa_chain_enabled"] = True
                setup_result["qa_chain_stats"] = self.get_qa_chain_stats()
                logger.info("âœ… QA Chain optimization enabled successfully")
            except Exception as e:
                logger.warning(f"âš ï¸ QA Chain auto-enable failed: {e}")
                setup_result["qa_chain_enabled"] = False
                setup_result["qa_chain_error"] = str(e)

                # QA Chainì´ ì‹¤íŒ¨í•´ë„ ì‹œìŠ¤í…œì€ ì •ìƒ ë™ìž‘
                logger.info("â„¹ï¸ Pipeline will continue with standard LLM method")
        else:
            setup_result["qa_chain_enabled"] = False
            logger.info("â„¹ï¸ QA Chain optimization skipped (enable_qa_chain=False)")

        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        status = self.get_system_status()
        setup_result["system_status"] = status

        logger.info("âœ… GraphRAG setup completed!")

        # ìµœì¢… ìƒíƒœ ìš”ì•½ ì¶œë ¥
        if setup_result.get("qa_chain_enabled"):
            logger.info("ðŸš€ Pipeline ready with QA Chain optimization")
        else:
            logger.info("ðŸ“ Pipeline ready with standard LLM method")

        return setup_result

    def rebuild_vector_store(
        self,
        new_store_type: Optional[str] = None,
        force_rebuild_embeddings: bool = False,
    ) -> Dict[str, Any]:
        """ë²¡í„° ì €ìž¥ì†Œ ìž¬êµ¬ì¶•"""

        logger.info("ðŸ”„ Rebuilding vector store...")

        config = self.config_manager.config
        current_store_type = config.vector_store.store_type
        target_store_type = new_store_type or current_store_type

        result = {
            "previous_store_type": current_store_type,
            "new_store_type": target_store_type,
            "embeddings_rebuilt": False,
            "store_migrated": False,
        }

        # ìž„ë² ë”© ìž¬êµ¬ì¶• (í•„ìš”ì‹œ)
        if force_rebuild_embeddings:
            logger.info("ðŸ—ï¸ Rebuilding embeddings...")
            build_result = self.build_embeddings(force_rebuild=True)
            result["embeddings_rebuilt"] = True
            result["embedding_stats"] = build_result

        # ë²¡í„° ì €ìž¥ì†Œ íƒ€ìž… ë³€ê²½ (í•„ìš”ì‹œ)
        if new_store_type and new_store_type != current_store_type:
            logger.info(f"ðŸ”„ Migrating from {current_store_type} to {new_store_type}")

            # ê¸°ì¡´ ë²¡í„° ì €ìž¥ì†Œ ë¡œë“œ
            self._ensure_embeddings_loaded()

            if self.vector_store:
                # ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰
                new_vector_store = self.vector_store.migrate_store_type(new_store_type)
                self.vector_store = new_vector_store

                # ì„¤ì • ì—…ë°ì´íŠ¸
                self.config_manager.update_config(
                    **{"vector_store.store_type": new_store_type}
                )

                result["store_migrated"] = True
                result["new_store_info"] = self.vector_store.get_store_info()

        logger.info("âœ… Vector store rebuild completed!")
        return result

    @classmethod
    def from_config_file(
        cls,
        config_file: str = "graphrag_config.yaml",
        auto_setup: bool = True,
        auto_build_embeddings: bool = False,
    ) -> "GraphRAGPipeline":
        """ì„¤ì • íŒŒì¼ë¡œë¶€í„° íŒŒì´í”„ë¼ì¸ ìƒì„± (í´ëž˜ìŠ¤ ë©”ì„œë“œ)"""

        pipeline = cls(config_file=config_file, auto_setup=False)

        if auto_setup:
            pipeline.setup_from_config(
                config_file=config_file,
                auto_build_embeddings=auto_build_embeddings,
            )

        return pipeline

    def _get_memory_usage(self) -> Dict[str, str]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            import psutil

            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
            memory = psutil.virtual_memory()

            usage = {
                "system_total": f"{memory.total / 1024**3:.1f}GB",
                "system_available": f"{memory.available / 1024**3:.1f}GB",
                "system_percent": f"{memory.percent:.1f}%",
            }

            # GPU ë©”ëª¨ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                gpu_allocated = torch.cuda.memory_allocated(0)
                gpu_cached = torch.cuda.memory_reserved(0)

                usage.update(
                    {
                        "gpu_total": f"{gpu_memory / 1024**3:.1f}GB",
                        "gpu_allocated": f"{gpu_allocated / 1024**3:.1f}GB",
                        "gpu_cached": f"{gpu_cached / 1024**3:.1f}GB",
                        "gpu_percent": f"{(gpu_allocated / gpu_memory) * 100:.1f}%",
                    }
                )

            return usage

        except ImportError:
            return {"status": "psutil not available"}
        except Exception as e:
            return {"error": str(e)}

    def clear_cache(self) -> None:
        """ìºì‹œ ì •ë¦¬"""
        self.query_cache.clear()
        logger.info("ðŸ—‘ï¸ Query cache cleared")

    def shutdown(self) -> None:
        """ì‹œìŠ¤í…œ ì¢…ë£Œ ë° ì •ë¦¬"""
        logger.info("ðŸ”Œ Shutting down GraphRAG Pipeline...")

        # LLM ì–¸ë¡œë“œ
        if self.llm_manager:
            self.llm_manager.unload_model()

        # ìºì‹œ ì •ë¦¬
        self.clear_cache()

        # ìƒíƒœ ë¦¬ì…‹
        self.state.status = PipelineStatus.UNINITIALIZED
        self.embeddings_loaded = False

        logger.info("âœ… Pipeline shutdown completed")


def create_graphrag_pipeline(
    config_file: str = "graphrag_config.yaml",
    auto_setup: bool = True,
    auto_build_embeddings: bool = False,
) -> GraphRAGPipeline:
    """GraphRAG íŒŒì´í”„ë¼ì¸ ìƒì„± íŽ¸ì˜ í•¨ìˆ˜"""

    return GraphRAGPipeline.from_config_file(
        config_file=config_file,
        auto_setup=auto_setup,
        auto_build_embeddings=auto_build_embeddings,
    )


def main():
    """GraphRAG Pipeline í…ŒìŠ¤íŠ¸ - QA Chain ìµœì í™” ì „ìš©"""
    print("ðŸ§ª Testing GraphRAG Pipeline with QA Chain optimization...")

    try:
        # 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = GraphRAGPipeline(config_file="graphrag_config.yaml", auto_setup=True)

        # 2. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        status = pipeline.get_system_status()
        print(f"ðŸ“Š System Status:")
        print(f"   Pipeline: {status['pipeline_state']['status']}")

        # QA Chain ìƒíƒœ í™•ì¸
        if "qa_chain" in status:
            qa_status = status["qa_chain"]
            print(f"   QA Chain Available: {qa_status['available']}")
            print(f"   QA Chain Ready: {qa_status['ready']}")
            print(f"   QA Chain Enabled: {qa_status['enabled']}")

        # 3. ìž„ë² ë”© êµ¬ì¶• (í•„ìš”í•œ ê²½ìš°)
        if not status["embeddings_loaded"]:
            print(f"\nðŸ—ï¸ Building embeddings...")
            build_result = pipeline.build_embeddings()
            print(f"âœ… Built {build_result['total_embeddings']} embeddings")

        # 4. QA Chain ì¤€ë¹„ ìƒíƒœ ê²€ì¦
        print(f"\nðŸ” Validating QA Chain integration...")
        validation = pipeline.validate_qa_chain_integration()
        print(f"   Status: {validation.get('status', 'unknown')}")

        if validation.get("recommendations"):
            print(f"   Recommendations:")
            for rec in validation["recommendations"][:3]:
                print(f"      â€¢ {rec}")

        # 5. QA Chain í™œì„±í™” (ë°”ë¡œ ì‹œìž‘)
        if validation.get("status") == "ready":
            print(f"\nðŸš€ Activating QA Chain optimization...")

            try:
                pipeline.enable_qa_chain_optimization()
                print(f"âœ… QA Chain optimization activated!")

                # 6. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
                test_queries = [
                    "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì€?",
                    "AI ë° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì´ ì ìš©ëœ ì£¼ìš” taskëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                    "ë°°í„°ë¦¬ ì „ê·¹ ê³µì •ì—ì„œ AIê°€ ì ìš©ë  ìˆ˜ ìžˆëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì´ ìžˆì„ê¹Œìš”?",
                ]

                print(f"\nâ“ Testing with QA CHAIN optimization...")

                for i, query in enumerate(test_queries[:2]):  # 2ê°œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
                    print(f"\n{i+1}. {query}")

                    start_time = time.time()
                    result = pipeline.ask(query, return_context=True)
                    response_time = time.time() - start_time

                    print(f"âœ… Answer: {result.answer[:300]}...")
                    print(f"ðŸ“Š Response time: {response_time:.2f}s")
                    print(f"ðŸ“Š Confidence: {result.confidence_score:.3f}")
                    print(f"ðŸ“„ Sources: {len(result.source_nodes)} nodes")

                    # ì²« ë²ˆì§¸ ì§ˆë¬¸ í›„ ìºì‹œ íš¨ê³¼ í™•ì¸
                    if i == 0:
                        print(f"\nðŸ”„ Testing cache effect - same question again...")
                        cache_start = time.time()
                        cache_result = pipeline.ask(query, return_context=True)
                        cache_time = time.time() - cache_start

                        speedup = (
                            response_time / cache_time
                            if cache_time > 0
                            else float("inf")
                        )
                        print(f"ðŸ“Š Cache response time: {cache_time:.2f}s")
                        print(f"ðŸš€ Speedup: {speedup:.1f}x faster")

                # 7. QA Chain í†µê³„
                qa_stats = pipeline.get_qa_chain_stats()
                if qa_stats:
                    print(f"\nðŸ“Š QA Chain Statistics:")
                    print(f"   Total calls: {qa_stats.get('total_calls', 0)}")
                    print(f"   Cache hits: {qa_stats.get('cache_hits', 0)}")
                    print(
                        f"   Cache hit ratio: {qa_stats.get('cache_hit_ratio', 0):.2%}"
                    )
                    print(
                        f"   Average response time: {qa_stats.get('average_time', 0):.2f}s"
                    )
                    print(f"   Success rate: {qa_stats.get('success_rate', 0):.2%}")
                    print(f"   Failed calls: {qa_stats.get('failed_calls', 0)}")

                # 8. LLM ì–´ëŒ‘í„° ìƒíƒœ í™•ì¸
                if hasattr(pipeline, "_qa_chain") and pipeline._qa_chain:
                    try:
                        llm_info = pipeline._qa_chain._llm.get_model_info()
                        print(f"\nðŸ¤– LLM Adapter Info:")
                        print(f"   Model path: {llm_info.get('model_path', 'unknown')}")
                        print(f"   Adapter mode: {llm_info.get('mode', 'unknown')}")
                        print(
                            f"   Temperature: {llm_info.get('temperature', 'unknown')}"
                        )
                        print(f"   Max tokens: {llm_info.get('max_tokens', 'unknown')}")
                        print(
                            f"   Caching enabled: {llm_info.get('caching_enabled', 'unknown')}"
                        )
                    except Exception as e:
                        print(f"âš ï¸ Could not get LLM adapter info: {e}")

                print(f"\nâœ… QA Chain optimization test completed successfully!")

            except Exception as e:
                print(f"âŒ QA Chain optimization failed: {e}")
                print(f"ðŸ”„ Error details:")
                import traceback

                traceback.print_exc()

        else:
            print(f"\nâŒ QA Chain not ready for testing")
            print(f"   Status: {validation.get('status')}")
            print(f"   Reason: {validation.get('reason', 'Unknown')}")

            if validation.get("recommendations"):
                print(f"   Please address these issues:")
                for rec in validation["recommendations"]:
                    print(f"      â€¢ {rec}")

            return

        # 9. ìµœì¢… ìƒíƒœ
        final_status = pipeline.get_system_status()
        print(f"\nðŸ“ˆ Final System State:")
        print(
            f"   Total queries processed: {final_status['pipeline_state']['total_queries_processed']}"
        )

        # QA Chain ìµœì¢… ìƒíƒœ
        if "qa_chain" in final_status:
            qa_final = final_status["qa_chain"]
            print(f"   QA Chain enabled: {qa_final['enabled']}")
            if qa_final["enabled"] and qa_final["stats"]:
                print(
                    f"   QA Chain total calls: {qa_final['stats'].get('total_calls', 0)}"
                )

        # ì‚¬ìš© ê°€ì´ë“œ
        print(f"\nðŸ’¡ QA Chain is now active! Usage:")
        print(f"   â€¢ Continue using: pipeline.ask('your question')")
        print(f"   â€¢ Check stats: pipeline.get_qa_chain_stats()")
        print(f"   â€¢ Disable if needed: pipeline.disable_qa_chain_optimization()")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback

        traceback.print_exc()


# def main():
#     """GraphRAG Pipeline í…ŒìŠ¤íŠ¸"""
#     print("ðŸ§ª Testing GraphRAG Pipeline...")

#     try:
#         # 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
#         pipeline = GraphRAGPipeline(config_file="graphrag_config.yaml", auto_setup=True)

#         # 2. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
#         status = pipeline.get_system_status()
#         print(f"ðŸ“Š System Status:")
#         print(f"   Pipeline: {status['pipeline_state']['status']}")
#         print(f"   Components: {status['components']}")

#         # 3. ìž„ë² ë”© êµ¬ì¶• (í•„ìš”í•œ ê²½ìš°)
#         if not status["embeddings_loaded"]:
#             print(f"\nðŸ—ï¸ Building embeddings...")
#             build_result = pipeline.build_embeddings()
#             print(f"âœ… Built {build_result['total_embeddings']} embeddings")

#         # 4. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
#         test_queries = [
#             "ë°°í„°ë¦¬ SoC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì€?",
#             "AI ë° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì´ ì ìš©ëœ ì£¼ìš” taskëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
#             "ë°°í„°ë¦¬ ì „ê·¹ ê³µì •ì—ì„œ AIê°€ ì ìš©ë  ìˆ˜ ìžˆëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì´ ìžˆì„ê¹Œìš”?",
#         ]

#         print(f"\nâ“ Testing queries...")
#         for i, query in enumerate(test_queries[:1]):  # ì²« ë²ˆì§¸ë§Œ í…ŒìŠ¤íŠ¸
#             print(f"\n{i+1}. {query}")

#             result = pipeline.ask(query, return_context=True)

#             print(f"âœ… Answer: {result.answer[:200]}...")
#             print(
#                 f"ðŸ“Š Stats: {result.processing_time:.2f}s, {result.confidence_score:.3f} confidence"
#             )
#             print(f"ðŸ“„ Sources: {len(result.source_nodes)} nodes")

#         # 5. ìµœì¢… ìƒíƒœ
#         final_status = pipeline.get_system_status()
#         print(f"\nðŸ“ˆ Final Stats:")
#         print(
#             f"   Queries processed: {final_status['pipeline_state']['total_queries_processed']}"
#         )
#         print(f"   Cache size: {final_status['cache_size']}")

#         print(f"\nâœ… GraphRAG Pipeline test completed!")

#     except Exception as e:
#         print(f"âŒ Test failed: {e}")
#         import traceback

#         traceback.print_exc()


if __name__ == "__main__":
    main()
