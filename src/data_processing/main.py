"""
í†µí•© ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
Integrated Data Processing Pipeline
"""

import json
import numpy as np
from pathlib import Path
from collections import Counter
from . import BibtexParser, PDFKeywordExtractor


def merge_keywords(bibtex_keywords, pdf_keywords):
    """Bibtexì™€ PDFì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œë¥¼ í†µí•©"""
    # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ì •ê·œí™”
    all_keywords = []

    # Bibtex í‚¤ì›Œë“œ ì¶”ê°€
    for kw in bibtex_keywords:
        if kw and isinstance(kw, str):
            all_keywords.append(kw.lower().strip())

    # PDF í‚¤ì›Œë“œ ì¶”ê°€
    for kw in pdf_keywords:
        if kw and isinstance(kw, str):
            all_keywords.append(kw.lower().strip())

    # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ë¬¸ìì—´ ì œê±°
    unique_keywords = list(set([kw for kw in all_keywords if kw and len(kw) > 2]))

    return sorted(unique_keywords)


def create_integrated_metadata(papers_bibtex, pdf_keywords_data, pdf_abstracts_data):
    """Bibtex, PDF í‚¤ì›Œë“œ, PDF Abstractë¥¼ í†µí•©í•œ ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ìƒì„±"""
    integrated_papers = []

    print("ğŸ”„ Integrating bibtex, PDF keywords, and PDF abstracts...")

    for paper in papers_bibtex:
        integrated_paper = paper.copy()

        # ì›ë³¸ í‚¤ì›Œë“œ ë³´ì¡´
        bibtex_keywords = paper.get("keywords", [])
        integrated_paper["bibtex_keywords"] = bibtex_keywords

        # PDF í‚¤ì›Œë“œ ì°¾ê¸° (ì œëª© ê¸°ë°˜ ë§¤ì¹­)
        pdf_keywords = []
        pdf_abstract = ""

        for pdf_data in pdf_keywords_data:
            if (
                pdf_data.get("title", "").lower().strip()
                == paper.get("title", "").lower().strip()
            ):
                pdf_keywords = pdf_data.get("pdf_keywords", [])
                break

        # PDF Abstract ì°¾ê¸° (ì œëª© ê¸°ë°˜ ë§¤ì¹­)
        for abstract_data in pdf_abstracts_data:
            if (
                abstract_data.get("title", "").lower().strip()
                == paper.get("title", "").lower().strip()
            ):
                pdf_abstract = abstract_data.get("abstract", "")
                break

        integrated_paper["pdf_keywords"] = pdf_keywords
        integrated_paper["abstract"] = pdf_abstract  # â­ Abstract ì¶”ê°€

        # í†µí•© í‚¤ì›Œë“œ ìƒì„±
        merged_keywords = merge_keywords(bibtex_keywords, pdf_keywords)
        integrated_paper["keywords"] = merged_keywords

        # í‚¤ì›Œë“œ ì†ŒìŠ¤ ì •ë³´
        keyword_sources = []
        if bibtex_keywords:
            keyword_sources.append("bibtex")
        if pdf_keywords:
            keyword_sources.append("pdf")
        if not keyword_sources:
            keyword_sources.append("none")

        integrated_paper["keyword_sources"] = keyword_sources
        integrated_paper["total_keyword_count"] = len(merged_keywords)
        integrated_paper["bibtex_keyword_count"] = len(bibtex_keywords)
        integrated_paper["pdf_keyword_count"] = len(pdf_keywords)
        integrated_paper["has_abstract"] = bool(pdf_abstract)  # Abstract ìœ ë¬´

        integrated_papers.append(integrated_paper)

    return integrated_papers


def run_complete_data_processing():
    """ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("ğŸš€ Starting integrated data processing pipeline...")

    # í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
    from src import BIBS_DIR, PDFS_DIR, PROCESSED_DIR

    # Step 1: Bibtex íŒŒì‹±
    print("\n" + "=" * 60)
    print("STEP 1: Parsing Bibtex Files")
    print("=" * 60)

    parser = BibtexParser(BIBS_DIR, PDFS_DIR)
    papers_bibtex = parser.parse_all_bibtex_files()

    # Bibtex ê¸°ë°˜ ì¤‘ê°„ ê²°ê³¼ ì €ì¥
    parser.save_metadata(PROCESSED_DIR / "step1_bibtex_metadata.json")

    # Bibtex ê¸°ë°˜ í†µê³„
    stats_bibtex = parser.get_statistics()
    print("\nğŸ“Š Bibtex-only Statistics:")
    for key, value in stats_bibtex.items():
        print(f"   {key}: {value}")

    # Step 2: PDFì—ì„œ í‚¤ì›Œë“œ ë° Abstract ì¶”ì¶œ
    print("\n" + "=" * 60)
    print("STEP 2: Extracting Keywords and Abstracts from PDFs")
    print("=" * 60)

    extractor = PDFKeywordExtractor()

    # PDF í‚¤ì›Œë“œ ê°œë³„ ì¶”ì¶œ (bibtex ë°ì´í„°ì™€ ë³„ë„)
    pdf_papers_with_keywords = []
    pdf_papers_with_abstracts = []

    for paper in papers_bibtex:
        if paper["has_pdf"] and paper["pdf_file"]:
            pdf_path = Path(paper["pdf_file"])
            if pdf_path.exists():
                try:
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    pdf_keywords = extractor.extract_keywords_from_pdf(pdf_path)
                    pdf_papers_with_keywords.append(
                        {
                            "title": paper["title"],
                            "pdf_file": paper["pdf_file"],
                            "pdf_keywords": pdf_keywords,
                        }
                    )

                    # Abstract ì¶”ì¶œ â­
                    pdf_text = extractor.extract_text_from_pdf(pdf_path)
                    abstract = extractor.extract_abstract(pdf_text) if pdf_text else ""
                    pdf_papers_with_abstracts.append(
                        {
                            "title": paper["title"],
                            "pdf_file": paper["pdf_file"],
                            "abstract": abstract,
                        }
                    )

                except Exception as e:
                    # ì‹¤íŒ¨ì‹œ ë¹ˆ ë°ì´í„°
                    pdf_papers_with_keywords.append(
                        {
                            "title": paper["title"],
                            "pdf_file": paper["pdf_file"],
                            "pdf_keywords": [],
                        }
                    )
                    pdf_papers_with_abstracts.append(
                        {
                            "title": paper["title"],
                            "pdf_file": paper["pdf_file"],
                            "abstract": "",
                        }
                    )

    print(f"ğŸ“„ Extracted keywords from {len(pdf_papers_with_keywords)} PDF files")
    print(f"ğŸ“ Extracted abstracts from {len(pdf_papers_with_abstracts)} PDF files")

    # Abstract í’ˆì§ˆ í™•ì¸
    abstracts_with_content = sum(
        1 for item in pdf_papers_with_abstracts if len(item["abstract"]) > 50
    )
    print(f"ğŸ“Š Papers with meaningful abstracts: {abstracts_with_content}")

    # Step 3: ë°ì´í„° í†µí•©
    print("\n" + "=" * 60)
    print("STEP 3: Integrating Bibtex + PDF Keywords + PDF Abstracts")
    print("=" * 60)

    integrated_papers = create_integrated_metadata(
        papers_bibtex, pdf_papers_with_keywords, pdf_papers_with_abstracts
    )

    # Step 4: í†µí•© ê²°ê³¼ ì €ì¥
    print("\n" + "=" * 60)
    print("STEP 4: Saving Integrated Results (Keywords + Abstracts)")
    print("=" * 60)

    # ìµœì¢… í†µí•© ë©”íƒ€ë°ì´í„° ì €ì¥
    final_file = PROCESSED_DIR / "integrated_papers_metadata.json"
    with open(final_file, "w", encoding="utf-8") as f:
        json.dump(integrated_papers, f, ensure_ascii=False, indent=2)

    # CSV ë²„ì „ë„ ì €ì¥
    import pandas as pd

    # DataFrame ìƒì„±ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    df_data = []
    for paper in integrated_papers:
        row = paper.copy()
        row["keywords"] = "; ".join(paper.get("keywords", []))
        row["bibtex_keywords"] = "; ".join(paper.get("bibtex_keywords", []))
        row["pdf_keywords"] = "; ".join(paper.get("pdf_keywords", []))
        row["authors"] = "; ".join(paper.get("authors", []))
        row["institutions"] = "; ".join(paper.get("institutions", []))
        row["keyword_sources"] = "; ".join(paper.get("keyword_sources", []))
        # AbstractëŠ” ì´ë¯¸ ë¬¸ìì—´ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
        df_data.append(row)

    df = pd.DataFrame(df_data)
    csv_file = PROCESSED_DIR / "integrated_papers_metadata.csv"
    df.to_csv(csv_file, index=False, encoding="utf-8")

    print(f"ğŸ’¾ Integrated metadata saved to:")
    print(f"   ğŸ“„ JSON: {final_file}")
    print(f"   ğŸ“Š CSV: {csv_file}")

    # Step 5: ì¢…í•© í†µê³„ ë° í‚¤ì›Œë“œ ë¶„ì„
    print("\n" + "=" * 60)
    print("FINAL INTEGRATED STATISTICS")
    print("=" * 60)

    # í‚¤ì›Œë“œ ì†ŒìŠ¤ë³„ í†µê³„
    papers_with_bibtex_only = sum(
        1
        for p in integrated_papers
        if "bibtex" in p["keyword_sources"] and "pdf" not in p["keyword_sources"]
    )
    papers_with_pdf_only = sum(
        1
        for p in integrated_papers
        if "pdf" in p["keyword_sources"] and "bibtex" not in p["keyword_sources"]
    )
    papers_with_both = sum(
        1
        for p in integrated_papers
        if "bibtex" in p["keyword_sources"] and "pdf" in p["keyword_sources"]
    )
    papers_with_none = sum(
        1 for p in integrated_papers if "none" in p["keyword_sources"]
    )

    total_integrated_keywords = sum(
        len(p.get("keywords", [])) for p in integrated_papers
    )
    total_bibtex_keywords = sum(
        p.get("bibtex_keyword_count", 0) for p in integrated_papers
    )
    total_pdf_keywords = sum(p.get("pdf_keyword_count", 0) for p in integrated_papers)

    # ëª¨ë“  ìœ ë‹ˆí¬ í‚¤ì›Œë“œ ìˆ˜ì§‘
    all_integrated_keywords = set()
    for paper in integrated_papers:
        all_integrated_keywords.update(paper.get("keywords", []))

    # Abstract í†µê³„ ê³„ì‚°
    abstracts_with_content = [
        p for p in integrated_papers if len(p.get("abstract", "")) > 50
    ]
    abstract_lengths = [
        len(p.get("abstract", "")) for p in integrated_papers if p.get("abstract")
    ]
    avg_abstract_length = np.mean(abstract_lengths) if abstract_lengths else 0

    print(f"ğŸ“„ Total papers processed: {len(integrated_papers)}")
    print(
        f"ğŸ”— Papers with PDF files: {sum(1 for p in integrated_papers if p['has_pdf'])}"
    )
    print(
        f"ğŸ“ Papers with abstracts: {sum(1 for p in integrated_papers if p.get('has_abstract', False))}"
    )
    print("")
    print("ğŸ“Š Keyword Source Distribution:")
    print(f"   ğŸ“ Bibtex keywords only: {papers_with_bibtex_only}")
    print(f"   ğŸ“‹ PDF keywords only: {papers_with_pdf_only}")
    print(f"   ğŸ”„ Both sources: {papers_with_both}")
    print(f"   âŒ No keywords: {papers_with_none}")
    print("")
    print("ğŸ”¢ Content Counts:")
    print(f"   ğŸ“ Total bibtex keywords: {total_bibtex_keywords}")
    print(f"   ğŸ“‹ Total PDF keywords: {total_pdf_keywords}")
    print(f"   ğŸ”„ Total integrated keywords: {total_integrated_keywords}")
    print(f"   ğŸ” Unique integrated keywords: {len(all_integrated_keywords)}")
    print(
        f"   ğŸ“ˆ Average keywords per paper: {total_integrated_keywords/len(integrated_papers):.1f}"
    )

    # Abstract í†µê³„ ì¶”ê°€
    abstracts_with_content = [
        p for p in integrated_papers if len(p.get("abstract", "")) > 50
    ]
    avg_abstract_length = np.mean(
        [len(p.get("abstract", "")) for p in integrated_papers if p.get("abstract")]
    )
    print(f"   ğŸ“„ Average abstract length: {avg_abstract_length:.0f} characters")

    # í‚¤ì›Œë“œê°€ ê°€ì¥ ë§ì€ ë…¼ë¬¸ top 5
    papers_by_keyword_count = sorted(
        integrated_papers, key=lambda x: len(x.get("keywords", [])), reverse=True
    )

    print(f"\nğŸ† Top 5 papers by integrated keyword count:")
    for i, paper in enumerate(papers_by_keyword_count[:5]):
        keyword_count = len(paper.get("keywords", []))
        bibtex_count = paper.get("bibtex_keyword_count", 0)
        pdf_count = paper.get("pdf_keyword_count", 0)
        title = (
            paper["title"][:50] + "..." if len(paper["title"]) > 50 else paper["title"]
        )
        print(f"   {i+1}. {title}")
        print(
            f"      Total: {keyword_count} (Bibtex: {bibtex_count}, PDF: {pdf_count})"
        )

    # ê°€ì¥ ë¹ˆë²ˆí•œ í‚¤ì›Œë“œ top 15
    all_keywords_list = []
    for paper in integrated_papers:
        all_keywords_list.extend(paper.get("keywords", []))

    keyword_freq = Counter(all_keywords_list)
    print(f"\nğŸ”¥ Top 15 most frequent integrated keywords:")
    for i, (keyword, count) in enumerate(keyword_freq.most_common(15)):
        print(f"   {i+1:2d}. {keyword}: {count} papers")

    # í‚¤ì›Œë“œ í†µê³„ ì €ì¥
    keyword_stats = {
        "total_papers": len(integrated_papers),
        "keyword_source_distribution": {
            "bibtex_only": papers_with_bibtex_only,
            "pdf_only": papers_with_pdf_only,
            "both_sources": papers_with_both,
            "no_keywords": papers_with_none,
        },
        "keyword_counts": {
            "total_bibtex_keywords": total_bibtex_keywords,
            "total_pdf_keywords": total_pdf_keywords,
            "total_integrated_keywords": total_integrated_keywords,
            "unique_integrated_keywords": len(all_integrated_keywords),
        },
        "abstract_statistics": {
            "papers_with_abstracts": sum(
                1 for p in integrated_papers if p.get("has_abstract", False)
            ),
            "papers_with_meaningful_abstracts": len(abstracts_with_content),
            "average_abstract_length": avg_abstract_length,
        },
        "top_keywords": keyword_freq.most_common(50),
        "keyword_frequencies": dict(keyword_freq),
    }

    stats_file = PROCESSED_DIR / "integrated_keyword_statistics.json"
    with open(stats_file, "w", encoding="utf-8") as f:
        json.dump(keyword_stats, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“Š Keyword statistics saved to: {stats_file}")
    print(f"\nâœ… Integrated data processing (with abstracts) completed successfully!")
    print(f"ğŸ“ Main output: {final_file}")

    return integrated_papers, final_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        papers, output_file = run_complete_data_processing()
        print(
            f"\nğŸ‰ Success! Integrated {len(papers)} papers with keywords and abstracts."
        )
        print(f"ğŸ“‚ Output file: {output_file}")

    except Exception as e:
        print(f"âŒ Error during processing: {e}")
        raise


if __name__ == "__main__":
    main()
