"""
PDFì—ì„œ í‚¤ì›Œë“œ ë° Abstract ì¶”ì¶œ ëª¨ë“ˆ
PDF Keyword and Abstract Extraction Module
"""

import re
import json
import warnings
import logging
from pathlib import Path
import pdfplumber
import PyPDF2
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tqdm import tqdm

# PDF ê´€ë ¨ ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pypdf2")

# PyPDF2 ë¡œê·¸ ë ˆë²¨ ì¡°ì •
logging.getLogger("pypdf2").setLevel(logging.ERROR)

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰)
try:
    nltk.data.find("tokenizers/punkt")
    nltk.data.find("corpora/stopwords")
except LookupError:
    print("Downloading NLTK data...")
    nltk.download("punkt", quiet=True)
    nltk.download("stopwords", quiet=True)


class PDFKeywordExtractor:
    """PDF íŒŒì¼ì—ì„œ í‚¤ì›Œë“œì™€ Abstractë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        # ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ ì‚¬ì „ (ì „ê¸°ì°¨ ë°°í„°ë¦¬ AI ë„ë©”ì¸)
        self.tech_keywords = {
            # AI/ML ê´€ë ¨
            "machine learning",
            "deep learning",
            "artificial intelligence",
            "neural network",
            "reinforcement learning",
            "supervised learning",
            "unsupervised learning",
            "convolutional neural network",
            "cnn",
            "lstm",
            "rnn",
            "transformer",
            "federated learning",
            "transfer learning",
            "ensemble learning",
            # ë°°í„°ë¦¬ ê´€ë ¨
            "lithium-ion battery",
            "li-ion battery",
            "battery management system",
            "bms",
            "state of charge",
            "soc",
            "state of health",
            "soh",
            "battery capacity",
            "battery degradation",
            "battery aging",
            "battery thermal management",
            "electrode",
            "cathode",
            "anode",
            "electrolyte",
            "separator",
            "lithium iron phosphate",
            "lifepo4",
            "nickel manganese cobalt",
            "nmc",
            # ì „ê¸°ì°¨ ê´€ë ¨
            "electric vehicle",
            "ev",
            "electric car",
            "plug-in hybrid",
            "phev",
            "vehicle-to-grid",
            "v2g",
            "charging station",
            "fast charging",
            "wireless charging",
            "battery swapping",
            "energy management",
            # ì œì¡°/ìƒì‚° ê´€ë ¨
            "battery production",
            "battery manufacturing",
            "electrode manufacturing",
            "coating process",
            "calendering",
            "drying process",
            "cell assembly",
            "quality control",
            "process optimization",
            "production line",
            # ê¸°íƒ€ ê¸°ìˆ 
            "optimization",
            "internet of things",
            "iot",
            "cyber-physical system",
            "smart grid",
            "energy storage",
            "renewable energy",
            "sustainability",
        }

        # ë¶ˆìš©ì–´ ì„¤ì •
        self.stop_words = set(stopwords.words("english"))

        # ì¶”ê°€ ë¶ˆìš©ì–´ (ë…¼ë¬¸ì—ì„œ í”í•œ ë‹¨ì–´ë“¤)
        self.additional_stopwords = {
            "paper",
            "study",
            "research",
            "method",
            "approach",
            "analysis",
            "result",
            "conclusion",
            "abstract",
            "introduction",
            "literature",
            "article",
            "journal",
            "conference",
            "proceedings",
            "ieee",
            "acm",
            "university",
            "department",
            "technology",
            "science",
            "engineering",
            "international",
            "national",
            "system",
            "systems",
            "based",
            "using",
        }
        self.stop_words.update(self.additional_stopwords)

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
        if not text:
            return ""
        # ì¤‘ê´„í˜¸ ì œê±°
        text = re.sub(r"[{}]", "", str(text))
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def extract_text_from_pdf(self, pdf_path):
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê²½ê³  ë©”ì‹œì§€ ì–µì œ)"""
        text = ""

        try:
            # pdfplumber ì‚¬ìš© (ë” ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                with pdfplumber.open(pdf_path) as pdf:
                    # ì²˜ìŒ 3í˜ì´ì§€ë§Œ ì¶”ì¶œ (ì œëª©, ì´ˆë¡, í‚¤ì›Œë“œê°€ ë³´í†µ ì—¬ê¸°ì— ìˆìŒ)
                    for page_num in range(min(3, len(pdf.pages))):
                        try:
                            page = pdf.pages[page_num]
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                        except Exception as page_error:
                            # ê°œë³„ í˜ì´ì§€ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                            continue

        except Exception as e:
            # pdfplumber ì‹¤íŒ¨ì‹œ PyPDF2 ì‚¬ìš©
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    with open(pdf_path, "rb") as file:
                        pdf_reader = PyPDF2.PdfReader(file)
                        # ì²˜ìŒ 3í˜ì´ì§€ë§Œ ì¶”ì¶œ
                        for page_num in range(min(3, len(pdf_reader.pages))):
                            try:
                                page = pdf_reader.pages[page_num]
                                extracted_text = page.extract_text()
                                if extracted_text:
                                    text += extracted_text + "\n"
                            except Exception as page_error:
                                # ê°œë³„ í˜ì´ì§€ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                                continue
            except Exception as e2:
                # ì¡°ìš©íˆ ì‹¤íŒ¨ (tqdmì— ë°©í•´ë˜ì§€ ì•Šë„ë¡)
                return ""

        return text

    def find_keywords_section(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ Keywords ì„¹ì…˜ ì°¾ê¸°"""
        keywords = []

        # Keywords ì„¹ì…˜ íŒ¨í„´ë“¤
        keyword_patterns = [
            r"Keywords?[:\-\s]+(.*?)(?:\n\s*\n|\n\s*[A-Z]|\n\s*\d+)",
            r"Index Terms?[:\-\s]+(.*?)(?:\n\s*\n|\n\s*[A-Z]|\n\s*\d+)",
            r"Key words?[:\-\s]+(.*?)(?:\n\s*\n|\n\s*[A-Z]|\n\s*\d+)",
        ]

        for pattern in keyword_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ ì •ì œ
                keyword_text = re.sub(r"\s+", " ", match).strip()
                # ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬
                kw_list = re.split(r"[,;]", keyword_text)
                keywords.extend([kw.strip().lower() for kw in kw_list if kw.strip()])

        return list(set(keywords))

    def extract_abstract(self, text):
        """ì´ˆë¡ ì„¹ì…˜ ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
        if not text:
            return ""

        # ë” ë‹¤ì–‘í•œ Abstract íŒ¨í„´ë“¤
        abstract_patterns = [
            # ê¸°ë³¸ íŒ¨í„´ë“¤
            r"Abstract[:\-\s]*\n\s*(.*?)(?:\n\s*\n|\n\s*(?:Keywords?|Index\s+Terms?|Introduction|1\.?\s*Introduction))",
            r"ABSTRACT[:\-\s]*\n\s*(.*?)(?:\n\s*\n|\n\s*(?:KEYWORDS?|INDEX\s+TERMS?|INTRODUCTION|1\.?\s*INTRODUCTION))",
            # ì½œë¡  ë’¤ ë°”ë¡œ í…ìŠ¤íŠ¸
            r"Abstract[:\-\s]*[:\-]\s*(.*?)(?:\n\s*\n|\n\s*(?:Keywords?|Introduction|1\.?\s*Introduction))",
            r"ABSTRACT[:\-\s]*[:\-]\s*(.*?)(?:\n\s*\n|\n\s*(?:KEYWORDS?|INTRODUCTION|1\.?\s*INTRODUCTION))",
            # ë” ìœ ì—°í•œ ëì  íƒì§€
            r"Abstract[:\-\s]+(.{100,2000}?)(?:\n\s*(?:Keywords?|Index\s+Terms?|Introduction|1\.?\s*Introduction|\d+\.?\s*Introduction))",
            r"ABSTRACT[:\-\s]+(.{100,2000}?)(?:\n\s*(?:KEYWORDS?|INDEX\s+TERMS?|INTRODUCTION|1\.?\s*INTRODUCTION|\d+\.?\s*INTRODUCTION))",
            # ì„¹ì…˜ ë²ˆí˜¸ê°€ ìˆëŠ” ê²½ìš°
            r"Abstract[:\-\s]*\n\s*(.*?)(?:\n\s*(?:1\.?\s*Introduction|2\.?\s*|Keywords?))",
            # ë” ê´€ëŒ€í•œ íŒ¨í„´ (ê¸¸ì´ ì œí•œ)
            r"Abstract[:\-\s]+(.*?)(?:\n\s*[A-Z][a-z]+\s*:|\n\s*\d+\.|\n\s*\n\s*[A-Z])",
        ]

        for i, pattern in enumerate(abstract_patterns):
            try:
                match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
                if match:
                    abstract = match.group(1)

                    # í…ìŠ¤íŠ¸ ì •ì œ
                    abstract = re.sub(r"\s+", " ", abstract).strip()

                    # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²ƒ í•„í„°ë§
                    if 50 <= len(abstract) <= 3000:
                        # ì˜ë¯¸ì—†ëŠ” ì‹œì‘ ë¶€ë¶„ ì œê±°
                        abstract = re.sub(r"^[:\-\s]+", "", abstract)
                        return abstract

            except Exception as e:
                continue

        # íŒ¨í„´ ë§¤ì¹­ì´ ì‹¤íŒ¨í•œ ê²½ìš°, ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        return self.extract_abstract_fallback(text)

    def extract_abstract_fallback(self, text):
        """ëŒ€ì•ˆ Abstract ì¶”ì¶œ ë°©ë²•"""
        lines = text.split("\n")

        # "Abstract" í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¤„ ì°¾ê¸°
        abstract_start_idx = -1
        for i, line in enumerate(lines):
            if re.search(r"\babstract\b", line, re.IGNORECASE):
                abstract_start_idx = i
                break

        if abstract_start_idx == -1:
            return ""

        # Abstract ì‹œì‘ì ë¶€í„° í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        abstract_lines = []
        for i in range(abstract_start_idx, min(abstract_start_idx + 20, len(lines))):
            line = lines[i].strip()

            # ì¢…ë£Œ ì¡°ê±´ë“¤
            if re.search(
                r"\b(keywords?|introduction|1\.?\s*introduction)\b", line, re.IGNORECASE
            ):
                break

            # Abstract ë¼ì¸ ìì²´ëŠ” ìŠ¤í‚µ
            if re.search(r"^\s*abstract\s*[:\-]?\s*$", line, re.IGNORECASE):
                continue

            if line and len(line) > 10:
                abstract_lines.append(line)

        if abstract_lines:
            abstract = " ".join(abstract_lines)
            abstract = re.sub(r"\s+", " ", abstract).strip()

            if 50 <= len(abstract) <= 3000:
                return abstract

        return ""

    def extract_title_from_text(self, text):
        """PDF í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ"""
        lines = text.split("\n")

        # ì²« ë²ˆì§¸ í˜ì´ì§€ì˜ ì²˜ìŒ ëª‡ ì¤„ì—ì„œ ì œëª© ì°¾ê¸°
        for i, line in enumerate(lines[:10]):
            line = line.strip()
            # ì œëª©ì€ ë³´í†µ ê¸¸ê³ , ëŒ€ë¬¸ìê°€ ë§ìœ¼ë©°, íŠ¹ì • íŒ¨í„´ì„ í”¼í•¨
            if (
                len(line) > 20
                and not re.match(r"^\d+", line)  # ìˆ«ìë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ
                and not line.lower().startswith(
                    ("abstract", "introduction", "keywords")
                )
                and len(line.split()) > 3
            ):
                return line

        return ""

    def extract_technical_keywords(self, text):
        """ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ (ì‚¬ì „ ê¸°ë°˜)"""
        text_lower = text.lower()
        found_keywords = []

        for keyword in self.tech_keywords:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)

        return found_keywords

    def extract_frequent_terms(self, text, min_freq=2, max_keywords=20):
        """ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = re.sub(r"[^\w\s]", " ", text.lower())
        tokens = word_tokenize(text)

        # ë¶ˆìš©ì–´ ì œê±° ë° í•„í„°ë§
        filtered_tokens = [
            token
            for token in tokens
            if (
                token not in self.stop_words
                and len(token) > 2
                and token.isalpha()
                and not token.isdigit()
            )
        ]

        # n-gram ì¶”ì¶œ (1-gram, 2-gram)
        unigrams = filtered_tokens
        bigrams = [
            f"{filtered_tokens[i]} {filtered_tokens[i+1]}"
            for i in range(len(filtered_tokens) - 1)
        ]

        # ë¹ˆë„ ê³„ì‚°
        term_freq = Counter(unigrams + bigrams)

        # ìµœì†Œ ë¹ˆë„ ì´ìƒì˜ í‚¤ì›Œë“œë§Œ ì„ íƒ
        frequent_terms = [term for term, freq in term_freq.items() if freq >= min_freq]

        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        top_terms = term_freq.most_common(max_keywords)
        return [term for term, freq in top_terms if freq >= min_freq]

    def extract_keywords_from_pdf(self, pdf_path):
        """PDFì—ì„œ ì¢…í•©ì ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf(pdf_path)
        if not text:
            return []

        all_keywords = []

        # 1. Keywords ì„¹ì…˜ì—ì„œ ì¶”ì¶œ
        section_keywords = self.find_keywords_section(text)
        all_keywords.extend(section_keywords)

        # 2. ê¸°ìˆ  í‚¤ì›Œë“œ ì‚¬ì „ ê¸°ë°˜ ì¶”ì¶œ
        tech_keywords = self.extract_technical_keywords(text)
        all_keywords.extend(tech_keywords)

        # 3. ì´ˆë¡ì—ì„œ ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        abstract = self.extract_abstract(text)
        if abstract:
            abstract_keywords = self.extract_frequent_terms(
                abstract, min_freq=1, max_keywords=10
            )
            all_keywords.extend(abstract_keywords)

        # 4. ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        frequent_keywords = self.extract_frequent_terms(
            text, min_freq=2, max_keywords=15
        )
        all_keywords.extend(frequent_keywords)

        # ì¤‘ë³µ ì œê±° ë° ì •ì œ
        unique_keywords = []
        seen = set()

        for kw in all_keywords:
            kw_clean = kw.lower().strip()
            if kw_clean and len(kw_clean) > 2 and kw_clean not in seen:
                unique_keywords.append(kw_clean)
                seen.add(kw_clean)

        return unique_keywords

    def process_all_pdfs(self, pdf_dir, papers_metadata):
        """ëª¨ë“  PDF íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        print("ğŸ” Extracting keywords from PDF files...")

        updated_papers = []
        successful_extractions = 0
        failed_extractions = 0

        for paper in tqdm(papers_metadata, desc="Processing PDFs", ncols=80):
            paper_copy = paper.copy()

            if paper["has_pdf"] and paper["pdf_file"]:
                pdf_path = Path(paper["pdf_file"])

                if pdf_path.exists():
                    try:
                        # PDFì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                        pdf_keywords = self.extract_keywords_from_pdf(pdf_path)

                        if pdf_keywords:  # í‚¤ì›Œë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œëœ ê²½ìš°
                            successful_extractions += 1

                        # ê¸°ì¡´ í‚¤ì›Œë“œì™€ ë³‘í•©
                        existing_keywords = paper.get("keywords", [])
                        combined_keywords = list(set(existing_keywords + pdf_keywords))

                        paper_copy["keywords"] = combined_keywords
                        paper_copy["pdf_keywords"] = pdf_keywords
                        paper_copy["keyword_source"] = "pdf_extracted"

                    except Exception as e:
                        # PDF ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ì¡°ìš©íˆ ì²˜ë¦¬
                        failed_extractions += 1
                        paper_copy["pdf_keywords"] = []
                        paper_copy["keyword_source"] = "pdf_failed"
                else:
                    paper_copy["pdf_keywords"] = []
                    paper_copy["keyword_source"] = "pdf_not_found"
            else:
                paper_copy["pdf_keywords"] = []
                paper_copy["keyword_source"] = "no_pdf"

            updated_papers.append(paper_copy)

        print(f"âœ… PDF processing completed:")
        print(f"   ğŸ“„ Total papers: {len(updated_papers)}")
        print(f"   âœ… Successful extractions: {successful_extractions}")
        if failed_extractions > 0:
            print(f"   âš ï¸  Failed extractions: {failed_extractions}")

        return updated_papers

    def save_updated_metadata(self, papers_metadata, output_dir):
        """ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        output_dir = Path(output_dir)

        # JSON ì €ì¥
        json_file = output_dir / "papers_metadata_with_pdf_keywords.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(papers_metadata, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Updated metadata saved to {json_file}")

        # í‚¤ì›Œë“œ í†µê³„ ìƒì„±
        all_keywords = []
        for paper in papers_metadata:
            all_keywords.extend(paper.get("keywords", []))

        keyword_stats = Counter(all_keywords)

        # í‚¤ì›Œë“œ í†µê³„ ì €ì¥
        stats_file = output_dir / "keyword_statistics.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "total_unique_keywords": len(keyword_stats),
                    "top_50_keywords": keyword_stats.most_common(50),
                    "keyword_frequencies": dict(keyword_stats),
                },
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"ğŸ“Š Keyword statistics saved to {stats_file}")

        return json_file

    def debug_abstract_extraction(self, pdf_path, save_debug=False):
        """Abstract ì¶”ì¶œ ë””ë²„ê¹…ìš© í•¨ìˆ˜"""
        text = self.extract_text_from_pdf(pdf_path)
        if not text:
            return {"error": "No text extracted from PDF"}

        # ì²˜ìŒ 1500ì ì •ë„ë§Œ í™•ì¸ (Abstractê°€ ë³´í†µ ì²« í˜ì´ì§€ì— ìˆìŒ)
        first_part = text[:1500]

        # Abstract ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
        abstract_mentions = []
        for match in re.finditer(r"abstract", text, re.IGNORECASE):
            start = max(0, match.start() - 50)
            end = min(len(text), match.end() + 200)
            context = text[start:end].replace("\n", " ").strip()
            abstract_mentions.append({"position": match.start(), "context": context})

        # ì‹¤ì œ ì¶”ì¶œ ê²°ê³¼
        extracted_abstract = self.extract_abstract(text)

        debug_info = {
            "pdf_path": str(pdf_path),
            "text_length": len(text),
            "first_1500_chars": first_part,
            "abstract_mentions": abstract_mentions,
            "extracted_abstract": extracted_abstract,
            "extraction_success": bool(extracted_abstract),
        }

        if save_debug:
            debug_file = Path(pdf_path).parent / f"debug_{Path(pdf_path).stem}.json"
            with open(debug_file, "w", encoding="utf-8") as f:
                json.dump(debug_info, f, ensure_ascii=False, indent=2)
            print(f"Debug info saved to: {debug_file}")

        return debug_info


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import PROCESSED_DIR

    # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_file = PROCESSED_DIR / "papers_metadata.json"

    if not metadata_file.exists():
        print("âŒ Papers metadata not found. Run bibtex_parser.py first.")
        return

    with open(metadata_file, "r", encoding="utf-8") as f:
        papers_metadata = json.load(f)

    print(f"ğŸ“„ Loaded {len(papers_metadata)} papers metadata")

    # PDF í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = PDFKeywordExtractor()

    # ëª¨ë“  PDFì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    updated_papers = extractor.process_all_pdfs(None, papers_metadata)

    # ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° ì €ì¥
    extractor.save_updated_metadata(updated_papers, PROCESSED_DIR)

    # í†µê³„ ì¶œë ¥
    papers_with_keywords = sum(
        1 for p in updated_papers if len(p.get("keywords", [])) > 0
    )
    total_keywords = sum(len(p.get("keywords", [])) for p in updated_papers)

    print(f"\nğŸ“Š Updated Statistics:")
    print(f"   Papers with keywords: {papers_with_keywords}/{len(updated_papers)}")
    print(f"   Total keywords extracted: {total_keywords}")
    print(f"   Average keywords per paper: {total_keywords/len(updated_papers):.1f}")


if __name__ == "__main__":
    main()
