"""
ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Semantic Similarity Graph Construction Module using Longformer
"""

import json
import re
import numpy as np
import torch
from pathlib import Path
from tqdm import tqdm
from transformers import LongformerTokenizer, LongformerModel
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import pandas as pd


class SemanticSimilarityExtractor:
    """Longformerë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, model_name="allenai/longformer-base-4096", device=None):
        """
        Args:
            model_name (str): ì‚¬ìš©í•  Longformer ëª¨ë¸ëª…
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None for auto)
        """
        self.model_name = model_name
        self.device = (
            device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        )

        print(f"ğŸ¤– Loading Longformer model: {model_name}")
        print(f"ğŸ’» Using device: {self.device}")

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)
        self.model = LongformerModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()

        # ìµœëŒ€ í† í° ê¸¸ì´
        self.max_length = 4096

        print(f"âœ… Model loaded successfully")

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì •ë¦¬"""
        if not text:
            return ""

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì ë§Œ ìœ ì§€)
        text = re.sub(r"[^\w\s\.\,\;\:\!\?\-\(\)]", " ", text)

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+", " ", text)

        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def prepare_text_input(self, title, abstract):
        """ì œëª©ê³¼ ì´ˆë¡ì„ ê²°í•©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„±"""
        title_clean = self.clean_text(title) if title else ""
        abstract_clean = self.clean_text(abstract) if abstract else ""

        if abstract_clean:
            input_text = f"Title: {title_clean} Abstract: {abstract_clean}"
        else:
            input_text = f"Title: {title_clean}"

        return input_text

    def analyze_text_lengths(self, papers_metadata):
        """í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ ë¶„ì„"""
        print("ğŸ“Š Analyzing text length distribution...")

        lengths = []
        texts_data = []

        for paper in papers_metadata:
            title = paper.get("title", "")
            abstract = paper.get("abstract", "")  # ì´ˆë¡ì´ ë©”íƒ€ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸

            # ì´ˆë¡ì´ ë©”íƒ€ë°ì´í„°ì— ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
            if not abstract:
                abstract = ""

            input_text = self.prepare_text_input(title, abstract)

            # í† í° ìˆ˜ ê³„ì‚°
            tokens = self.tokenizer.encode(input_text, add_special_tokens=True)
            token_length = len(tokens)

            lengths.append(token_length)
            texts_data.append(
                {
                    "title": title,
                    "abstract": abstract,
                    "input_text": input_text,
                    "token_length": token_length,
                }
            )

        lengths = np.array(lengths)

        print(f"ğŸ“ Text length statistics:")
        print(f"   Mean: {lengths.mean():.1f} tokens")
        print(f"   Median: {np.median(lengths):.1f} tokens")
        print(f"   95th percentile: {np.percentile(lengths, 95):.1f} tokens")
        print(f"   Max: {lengths.max()} tokens")
        print(f"   Papers > 4096 tokens: {sum(lengths > 4096)}")

        # ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ì €ì¥
        plt.figure(figsize=(10, 6))
        plt.hist(lengths, bins=50, alpha=0.7, edgecolor="black")
        plt.axvline(x=4096, color="red", linestyle="--", label="Longformer Max (4096)")
        plt.axvline(
            x=np.percentile(lengths, 95),
            color="orange",
            linestyle="--",
            label="95th percentile",
        )
        plt.xlabel("Token Length")
        plt.ylabel("Number of Papers")
        plt.title("Distribution of Text Lengths (Tokens)")
        plt.legend()
        plt.grid(True, alpha=0.3)

        return texts_data, lengths

    def extract_embeddings(self, papers_metadata, batch_size=8):
        """ë…¼ë¬¸ë“¤ì˜ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ"""
        print("ğŸ” Extracting semantic embeddings...")

        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
        texts_data, lengths = self.analyze_text_lengths(papers_metadata)

        # íš¨ê³¼ì ì¸ ìµœëŒ€ ê¸¸ì´ ì„¤ì • (95th percentile ë˜ëŠ” 4096 ì¤‘ ì‘ì€ ê°’)
        effective_max_length = min(int(np.percentile(lengths, 95)), self.max_length)
        print(f"ğŸ“ Using effective max length: {effective_max_length} tokens")

        embeddings = []
        paper_ids = []

        # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if self.device == "cuda":
            # A100 80GBë©´ ë” í° ë°°ì¹˜ë„ ê°€ëŠ¥í•˜ì§€ë§Œ ì•ˆì „í•˜ê²Œ ì„¤ì •
            batch_size = min(batch_size, 16)

        print(f"ğŸš€ Processing with batch size: {batch_size}")

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in tqdm(
            range(0, len(texts_data), batch_size), desc="Extracting embeddings"
        ):
            batch_texts = []
            batch_ids = []

            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            for j in range(i, min(i + batch_size, len(texts_data))):
                text_data = texts_data[j]
                input_text = text_data["input_text"]

                batch_texts.append(input_text)
                batch_ids.append(f"paper_{j}")

            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=effective_max_length,
                return_tensors="pt",
            )

            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # ì„ë² ë”© ì¶”ì¶œ
            with torch.no_grad():
                outputs = self.model(**inputs)

                # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš© (ë¬¸ì„œ ì „ì²´ í‘œí˜„)
                cls_embeddings = outputs.last_hidden_state[
                    :, 0, :
                ]  # [batch_size, hidden_size]

                # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                batch_embeddings = cls_embeddings.cpu().numpy()
                embeddings.extend(batch_embeddings)
                paper_ids.extend(batch_ids)

        embeddings = np.array(embeddings)
        print(f"âœ… Extracted embeddings: {embeddings.shape}")

        return embeddings, paper_ids, texts_data

    def compute_similarity_matrix(self, embeddings):
        """ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°"""
        print("ğŸ“Š Computing cosine similarity matrix...")

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = cosine_similarity(embeddings)

        print(f"âœ… Similarity matrix computed: {similarity_matrix.shape}")
        print(
            f"ğŸ“ˆ Similarity range: [{similarity_matrix.min():.3f}, {similarity_matrix.max():.3f}]"
        )

        return similarity_matrix

    def build_similarity_graph(self, similarity_matrix, paper_ids, top_k_percent=20):
        """ìœ ì‚¬ë„ í–‰ë ¬ì„ ê¸°ë°˜ìœ¼ë¡œ ë°©í–¥ ê·¸ë˜í”„ êµ¬ì¶•"""
        print(
            f"ğŸ”— Building similarity graph (top {top_k_percent}% connections per paper)..."
        )

        n_papers = len(paper_ids)
        top_k = max(1, int(n_papers * top_k_percent / 100))  # ê° ë…¼ë¬¸ë³„ ì—°ê²°í•  ìƒìœ„ kê°œ

        similarity_graph = {}
        total_edges = 0

        for i, paper_id in enumerate(paper_ids):
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  ìœ ì‚¬ë„ ê³„ì‚°
            similarities = similarity_matrix[i].copy()
            similarities[i] = -1  # ìê¸° ìì‹ ì€ ì œì™¸

            # ìƒìœ„ kê°œ ì„ íƒ
            top_indices = np.argsort(similarities)[-top_k:][::-1]  # ë‚´ë¦¼ì°¨ìˆœ

            connections = []
            for j in top_indices:
                if similarities[j] > 0:  # ì–‘ìˆ˜ ìœ ì‚¬ë„ë§Œ
                    connections.append(
                        {
                            "target_paper": paper_ids[j],
                            "similarity": float(similarities[j]),
                        }
                    )
                    total_edges += 1

            similarity_graph[paper_id] = connections

        print(f"âœ… Similarity graph built:")
        print(f"   ğŸ“„ Papers: {n_papers}")
        print(f"   ğŸ”— Total edges: {total_edges}")
        print(f"   ğŸ“ˆ Average edges per paper: {total_edges/n_papers:.1f}")
        print(f"   ğŸ¯ Target edges per paper: {top_k}")

        return similarity_graph

    def analyze_similarity_distribution(self, similarity_matrix):
        """ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„"""
        print("ğŸ“Š Analyzing similarity distribution...")

        # ëŒ€ê°ì„  ì œì™¸ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸)
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        similarities = similarity_matrix[mask]

        print(f"ğŸ“ˆ Similarity statistics:")
        print(f"   Mean: {similarities.mean():.3f}")
        print(f"   Median: {np.median(similarities):.3f}")
        print(f"   Std: {similarities.std():.3f}")
        print(f"   Min: {similarities.min():.3f}")
        print(f"   Max: {similarities.max():.3f}")
        print(f"   80th percentile: {np.percentile(similarities, 80):.3f}")
        print(f"   90th percentile: {np.percentile(similarities, 90):.3f}")
        print(f"   95th percentile: {np.percentile(similarities, 95):.3f}")

        return similarities

    def save_similarity_graph(
        self, similarity_graph, similarity_matrix, paper_ids, texts_data, output_dir
    ):
        """ìœ ì‚¬ë„ ê·¸ë˜í”„ì™€ ê´€ë ¨ ë°ì´í„° ì €ì¥"""
        output_dir = Path(output_dir)

        # 1. ê·¸ë˜í”„ êµ¬ì¡° ì €ì¥ (ê°„ë‹¨í•œ í˜•íƒœ)
        graph_file = output_dir / "semantic_similarity_graph.json"
        with open(graph_file, "w", encoding="utf-8") as f:
            json.dump(similarity_graph, f, ensure_ascii=False, indent=2)

        # 2. ìœ ì‚¬ë„ í–‰ë ¬ ì €ì¥ (numpy format)
        matrix_file = output_dir / "similarity_matrix.npy"
        np.save(matrix_file, similarity_matrix)

        # 3. ë…¼ë¬¸ ID ë§¤í•‘ ì €ì¥
        mapping_file = output_dir / "paper_id_mapping.json"
        paper_mapping = {
            "paper_ids": paper_ids,
            "id_to_index": {paper_id: i for i, paper_id in enumerate(paper_ids)},
        }
        with open(mapping_file, "w", encoding="utf-8") as f:
            json.dump(paper_mapping, f, ensure_ascii=False, indent=2)

        # 4. í…ìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ (ê²€ì¦ìš©)
        texts_file = output_dir / "processed_texts.json"
        with open(texts_file, "w", encoding="utf-8") as f:
            json.dump(texts_data, f, ensure_ascii=False, indent=2)

        # 5. ê·¸ë˜í”„ í†µê³„ ì €ì¥
        stats = self.get_graph_statistics(similarity_graph, similarity_matrix)
        stats_file = output_dir / "similarity_graph_statistics.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Similarity graph saved:")
        print(f"   ğŸ”— Graph: {graph_file}")
        print(f"   ğŸ“Š Matrix: {matrix_file}")
        print(f"   ğŸ—‚ï¸  Mapping: {mapping_file}")
        print(f"   ğŸ“„ Texts: {texts_file}")
        print(f"   ğŸ“ˆ Statistics: {stats_file}")

        return graph_file

    def get_graph_statistics(self, similarity_graph, similarity_matrix):
        """ê·¸ë˜í”„ í†µê³„ ì •ë³´ ìƒì„±"""
        total_edges = sum(len(connections) for connections in similarity_graph.values())
        n_papers = len(similarity_graph)

        # ìœ ì‚¬ë„ ë¶„í¬
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        similarities = similarity_matrix[mask]

        # ì—°ê²° ìˆ˜ ë¶„í¬
        edge_counts = [len(connections) for connections in similarity_graph.values()]

        stats = {
            "graph_info": {
                "total_papers": n_papers,
                "total_edges": total_edges,
                "average_edges_per_paper": total_edges / n_papers,
                "graph_density": total_edges / (n_papers * (n_papers - 1)),
            },
            "similarity_distribution": {
                "mean": float(similarities.mean()),
                "median": float(np.median(similarities)),
                "std": float(similarities.std()),
                "min": float(similarities.min()),
                "max": float(similarities.max()),
                "percentiles": {
                    "80th": float(np.percentile(similarities, 80)),
                    "90th": float(np.percentile(similarities, 90)),
                    "95th": float(np.percentile(similarities, 95)),
                },
            },
            "edge_distribution": {
                "mean_edges": float(np.mean(edge_counts)),
                "median_edges": float(np.median(edge_counts)),
                "min_edges": int(np.min(edge_counts)),
                "max_edges": int(np.max(edge_counts)),
            },
        }

        return stats

    def process_papers(self, papers_metadata, top_k_percent=20, batch_size=8):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ Starting semantic similarity analysis...")

        # 1. ì„ë² ë”© ì¶”ì¶œ
        embeddings, paper_ids, texts_data = self.extract_embeddings(
            papers_metadata, batch_size=batch_size
        )

        # 2. ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        similarity_matrix = self.compute_similarity_matrix(embeddings)

        # 3. ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„
        self.analyze_similarity_distribution(similarity_matrix)

        # 4. ê·¸ë˜í”„ êµ¬ì¶•
        similarity_graph = self.build_similarity_graph(
            similarity_matrix, paper_ids, top_k_percent=top_k_percent
        )

        return similarity_graph, similarity_matrix, paper_ids, texts_data


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import PROCESSED_DIR

    # í†µí•© ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_file = PROCESSED_DIR / "integrated_papers_metadata.json"

    if not metadata_file.exists():
        print("âŒ Integrated papers metadata not found. Run main.py first.")
        return

    with open(metadata_file, "r", encoding="utf-8") as f:
        papers_metadata = json.load(f)

    print(f"ğŸ“„ Loaded {len(papers_metadata)} papers metadata")

    # Semantic Similarity ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = SemanticSimilarityExtractor()

    # ì „ì²´ ì²˜ë¦¬
    similarity_graph, similarity_matrix, paper_ids, texts_data = (
        extractor.process_papers(
            papers_metadata,
            top_k_percent=20,  # ê° ë…¼ë¬¸ë³„ ìƒìœ„ 20% ì—°ê²°
            batch_size=8,  # A100 80GBì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥
        )
    )

    # ê²°ê³¼ ì €ì¥
    output_file = extractor.save_similarity_graph(
        similarity_graph, similarity_matrix, paper_ids, texts_data, PROCESSED_DIR
    )

    print(f"âœ… Semantic similarity analysis completed!")
    print(f"ğŸ“ Main output: {output_file}")

    return similarity_graph, output_file


if __name__ == "__main__":
    main()
