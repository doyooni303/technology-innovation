"""
Bibtex íŒŒì¼ íŒŒì‹± ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ëª¨ë“ˆ
"""

import os
import pandas as pd
from pathlib import Path
from pybtex.database import parse_file
from pybtex.database.input import bibtex
from tqdm import tqdm
import json
import re


class BibtexParser:
    """Bibtex íŒŒì¼ë“¤ì„ íŒŒì‹±í•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, bibs_dir, pdfs_dir):
        """
        Args:
            bibs_dir (str/Path): bibtex íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            pdfs_dir (str/Path): PDF íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        """
        self.bibs_dir = Path(bibs_dir)
        self.pdfs_dir = Path(pdfs_dir)
        self.papers_metadata = []

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
        if not text:
            return ""
        # ì¤‘ê´„í˜¸ ì œê±°
        text = re.sub(r"[{}]", "", str(text))
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def extract_keywords(self, entry):
        """í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜"""
        keywords = []

        # keywords í•„ë“œì—ì„œ ì¶”ì¶œ
        if "keywords" in entry.fields:
            kw_text = self.clean_text(entry.fields["keywords"])
            # ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬
            keywords.extend(
                [kw.strip() for kw in re.split(r"[,;]", kw_text) if kw.strip()]
            )

        # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        title = self.clean_text(entry.fields.get("title", ""))
        title_keywords = self.extract_title_keywords(title)
        keywords.extend(title_keywords)

        # ì¤‘ë³µ ì œê±° ë° ì •ì œ
        keywords = list(set([kw.lower().strip() for kw in keywords if len(kw) > 2]))
        return keywords

    def extract_title_keywords(self, title):
        """ì œëª©ì—ì„œ ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tech_keywords = [
            "machine learning",
            "deep learning",
            "artificial intelligence",
            "AI",
            "reinforcement learning",
            "neural network",
            "battery",
            "lithium-ion",
            "electric vehicle",
            "EV",
            "charging",
            "energy management",
            "optimization",
            "IoT",
            "blockchain",
            "federated learning",
            "CNN",
            "LSTM",
            "RNN",
        ]

        found_keywords = []
        title_lower = title.lower()

        for keyword in tech_keywords:
            if keyword.lower() in title_lower:
                found_keywords.append(keyword)

        return found_keywords

    def extract_authors_and_institutions(self, entry):
        """ì €ìì™€ ì†Œì†ê¸°ê´€ ì •ë³´ ì¶”ì¶œ"""
        authors = []
        institutions = set()

        if "author" in entry.persons:
            for person in entry.persons["author"]:
                # ì €ìëª… ì¶”ì¶œ
                first_names = " ".join(person.first_names)
                last_names = " ".join(person.last_names)
                full_name = f"{first_names} {last_names}".strip()
                authors.append(full_name)

        # ì†Œì†ê¸°ê´€ì€ ë³´í†µ ë³„ë„ í•„ë“œì— ìˆì§€ë§Œ, ê°„ë‹¨íˆ ì²˜ë¦¬
        if "institution" in entry.fields:
            inst_text = self.clean_text(entry.fields["institution"])
            institutions.add(inst_text)

        # author í•„ë“œì—ì„œ ì†Œì†ê¸°ê´€ ì •ë³´ ì¶”ì¶œ ì‹œë„
        if "author" in entry.fields:
            author_text = self.clean_text(entry.fields["author"])
            # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ëŒ€í•™/ê¸°ê´€ëª… ì¶”ì¶œ
            inst_patterns = [
                r"University of ([^,\n]+)",
                r"([^,\n]*University[^,\n]*)",
                r"([^,\n]*Institute[^,\n]*)",
                r"([^,\n]*College[^,\n]*)",
            ]

            for pattern in inst_patterns:
                matches = re.findall(pattern, author_text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        institutions.update([m.strip() for m in match if m.strip()])
                    else:
                        institutions.add(match.strip())

        return authors, list(institutions)

    def parse_single_bibtex(self, bibtex_file):
        """ê°œë³„ bibtex íŒŒì¼ íŒŒì‹±"""
        try:
            # bibtex íŒŒì¼ ì½ê¸°
            with open(bibtex_file, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()

            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ íŒŒì‹± (pybtexê°€ íŒŒì¼ ê²½ë¡œë¥¼ ìš”êµ¬í•¨)
            temp_file = bibtex_file.with_suffix(".bib")
            with open(temp_file, "w", encoding="utf-8") as f:
                f.write(content)

            # pybtexë¡œ íŒŒì‹±
            bib_data = parse_file(str(temp_file))

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            temp_file.unlink()

            papers = []
            for key, entry in bib_data.entries.items():
                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                title = self.clean_text(entry.fields.get("title", ""))
                year = entry.fields.get("year", "")
                journal = self.clean_text(entry.fields.get("journal", ""))
                publisher = self.clean_text(entry.fields.get("publisher", ""))

                # ì €ì ë° ì†Œì†ê¸°ê´€ ì¶”ì¶œ
                authors, institutions = self.extract_authors_and_institutions(entry)

                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self.extract_keywords(entry)

                # PDF íŒŒì¼ ë§¤ì¹­ í™•ì¸
                pdf_file = self.find_matching_pdf(bibtex_file.stem)

                paper_data = {
                    "bibtex_key": key,
                    "title": title,
                    "authors": authors,
                    "institutions": institutions,
                    "year": year,
                    "journal": journal,
                    "publisher": publisher,
                    "keywords": keywords,
                    "bibtex_file": str(bibtex_file),
                    "pdf_file": str(pdf_file) if pdf_file else None,
                    "has_pdf": pdf_file is not None,
                }

                papers.append(paper_data)

            return papers

        except Exception as e:
            print(f"âŒ Error parsing {bibtex_file}: {e}")
            return []

    def find_matching_pdf(self, bibtex_stem):
        """bibtex íŒŒì¼ì— ëŒ€ì‘í•˜ëŠ” PDF íŒŒì¼ ì°¾ê¸°"""
        # ë™ì¼í•œ íŒŒì¼ëª…ìœ¼ë¡œ PDF ì°¾ê¸°
        pdf_file = self.pdfs_dir / f"{bibtex_stem}.pdf"

        if pdf_file.exists():
            return pdf_file

        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì°¾ê¸°
        for pdf_file in self.pdfs_dir.glob("*.pdf"):
            if pdf_file.stem.lower() == bibtex_stem.lower():
                return pdf_file

        return None

    def parse_all_bibtex_files(self):
        """ëª¨ë“  bibtex íŒŒì¼ë“¤ì„ íŒŒì‹±í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        print("ğŸ” Scanning bibtex files...")
        bibtex_files = list(self.bibs_dir.glob("*.txt"))

        print(f"ğŸ“„ Found {len(bibtex_files)} bibtex files")

        all_papers = []
        failed_files = []

        for bibtex_file in tqdm(bibtex_files, desc="Parsing bibtex files"):
            papers = self.parse_single_bibtex(bibtex_file)
            if papers:
                all_papers.extend(papers)
            else:
                failed_files.append(bibtex_file)

        self.papers_metadata = all_papers

        print(f"âœ… Successfully parsed {len(all_papers)} papers")
        if failed_files:
            print(f"âŒ Failed to parse {len(failed_files)} files:")
            for f in failed_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                print(f"   - {f.name}")

        return all_papers

    def save_metadata(self, output_file):
        """ë©”íƒ€ë°ì´í„°ë¥¼ JSON/CSV íŒŒì¼ë¡œ ì €ì¥"""
        output_path = Path(output_file)

        if output_path.suffix.lower() == ".json":
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(self.papers_metadata, f, ensure_ascii=False, indent=2)
        elif output_path.suffix.lower() == ".csv":
            df = pd.DataFrame(self.papers_metadata)
            df.to_csv(output_path, index=False, encoding="utf-8")

        print(f"ğŸ’¾ Metadata saved to {output_path}")

    def get_statistics(self):
        """íŒŒì‹±ëœ ë°ì´í„°ì˜ í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.papers_metadata:
            return {}

        df = pd.DataFrame(self.papers_metadata)

        stats = {
            "total_papers": len(self.papers_metadata),
            "papers_with_pdf": df["has_pdf"].sum(),
            "papers_without_pdf": (~df["has_pdf"]).sum(),
            "papers_with_keywords": (df["keywords"].apply(len) > 0).sum(),
            "avg_authors_per_paper": df["authors"].apply(len).mean(),
            "total_unique_authors": len(
                set([author for authors in df["authors"] for author in authors])
            ),
            "total_unique_institutions": len(
                set(
                    [
                        inst
                        for institutions in df["institutions"]
                        for inst in institutions
                    ]
                )
            ),
            "total_unique_keywords": len(
                set([kw for keywords in df["keywords"] for kw in keywords])
            ),
        }

        return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
    from src import BIBS_DIR, PDFS_DIR, PROCESSED_DIR

    # BibtexParser ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    parser = BibtexParser(BIBS_DIR, PDFS_DIR)

    # ëª¨ë“  bibtex íŒŒì¼ íŒŒì‹±
    papers = parser.parse_all_bibtex_files()

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    parser.save_metadata(PROCESSED_DIR / "papers_metadata.json")
    parser.save_metadata(PROCESSED_DIR / "papers_metadata.csv")

    # í†µê³„ ì •ë³´ ì¶œë ¥
    stats = parser.get_statistics()
    print("\nğŸ“Š Statistics:")
    for key, value in stats.items():
        print(f"   {key}: {value}")


if __name__ == "__main__":
    main()
