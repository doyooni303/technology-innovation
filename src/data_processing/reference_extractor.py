"""
PDFì—ì„œ References ì¶”ì¶œ ë° ì¸ìš© ê´€ê³„ ë¶„ì„ ëª¨ë“ˆ
Reference Extraction and Citation Analysis Module
"""

import re
import json
import warnings
from pathlib import Path
import pdfplumber
import PyPDF2
from collections import defaultdict
from tqdm import tqdm
from difflib import SequenceMatcher

# PDF ê´€ë ¨ ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")


class ReferenceExtractor:
    """PDFì—ì„œ Referencesë¥¼ ì¶”ì¶œí•˜ê³  ì¸ìš© ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        self.reference_patterns = [
            r"References?\s*\n",
            r"REFERENCES?\s*\n",
            r"Bibliography\s*\n",
            r"BIBLIOGRAPHY\s*\n",
            r"Literature Cited\s*\n",
            r"Works Cited\s*\n",
        ]

        # ë…„ë„ íŒ¨í„´ (2000-2025)
        self.year_pattern = r"\b(20[0-2][0-9])\b"

        # ì €ìëª… íŒ¨í„´ (ê°„ë‹¨í•œ í˜•íƒœ)
        self.author_pattern = r"^([A-Z][a-z]+(?:\s+[A-Z]\.?\s*)*[A-Z][a-z]+)"

    def extract_full_text_from_pdf(self, pdf_path):
        """PDF ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""

        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                with pdfplumber.open(pdf_path) as pdf:
                    for page in pdf.pages:
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                        except:
                            continue

        except Exception:
            # pdfplumber ì‹¤íŒ¨ì‹œ PyPDF2 ì‚¬ìš©
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    with open(pdf_path, "rb") as file:
                        pdf_reader = PyPDF2.PdfReader(file)
                        for page in pdf_reader.pages:
                            try:
                                extracted_text = page.extract_text()
                                if extracted_text:
                                    text += extracted_text + "\n"
                            except:
                                continue
            except Exception:
                return ""

        return text

    def find_references_section(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ References ì„¹ì…˜ ì°¾ê¸°"""
        references_start = -1

        # References ì‹œì‘ì  ì°¾ê¸°
        for pattern in self.reference_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                references_start = match.end()
                break

        if references_start == -1:
            return ""

        # References ëì  ì°¾ê¸° (Appendix, ë‹¤ìŒ ì„¹ì…˜ ë“±)
        end_patterns = [
            r"\n\s*Appendix",
            r"\n\s*APPENDIX",
            r"\n\s*Author.*Biography",
            r"\n\s*AUTHOR.*BIOGRAPHY",
            r"\n\s*Acknowledgments?",
            r"\n\s*ACKNOWLEDGMENTS?",
            r"\n\s*Supplementary",
            r"\n\s*Index",
            r"\n\s*\d+\.\s*Conclusion",  # ë‹¤ìŒ ì„¹ì…˜ ë²ˆí˜¸
        ]

        references_text = text[references_start:]
        references_end = len(references_text)

        for pattern in end_patterns:
            match = re.search(pattern, references_text, re.IGNORECASE)
            if match:
                references_end = min(references_end, match.start())

        return references_text[:references_end]

    def parse_references(self, references_text):
        """References í…ìŠ¤íŠ¸ë¥¼ ê°œë³„ ì°¸ê³ ë¬¸í—Œìœ¼ë¡œ íŒŒì‹±"""
        if not references_text.strip():
            return []

        references = []

        # ì°¸ê³ ë¬¸í—Œ í•­ëª© ë¶„ë¦¬ íŒ¨í„´ë“¤
        split_patterns = [
            r"\n\s*\[\d+\]",  # [1], [2] í˜•íƒœ
            r"\n\s*\d+\.\s",  # 1. 2. í˜•íƒœ
            r"\n\s*\(\d+\)",  # (1), (2) í˜•íƒœ
            r"\n(?=[A-Z][a-z]+.*?20[0-2][0-9])",  # ì €ìëª…ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
        ]

        # ê°€ì¥ ì í•©í•œ íŒ¨í„´ ì°¾ê¸°
        best_pattern = None
        max_splits = 0

        for pattern in split_patterns:
            splits = re.split(pattern, references_text)
            if len(splits) > max_splits:
                max_splits = len(splits)
                best_pattern = pattern

        if best_pattern and max_splits > 1:
            ref_items = re.split(best_pattern, references_text)
        else:
            # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ë¹ˆ ì¤„ë¡œ ë¶„ë¦¬
            ref_items = re.split(r"\n\s*\n", references_text)

        # ê° ì°¸ê³ ë¬¸í—Œ ì •ì œ
        for item in ref_items:
            item = item.strip()
            if len(item) > 20:  # ë„ˆë¬´ ì§§ì€ ê²ƒì€ ì œì™¸
                # ì—¬ëŸ¬ ì¤„ì„ í•œ ì¤„ë¡œ í•©ì¹˜ê¸°
                item = re.sub(r"\s+", " ", item)
                references.append(item)

        return references

    def extract_reference_info(self, reference_text):
        """ê°œë³„ ì°¸ê³ ë¬¸í—Œì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        info = {
            "raw_text": reference_text,
            "authors": [],
            "title": "",
            "year": "",
            "journal": "",
            "confidence": 0.0,
        }

        # ë…„ë„ ì¶”ì¶œ
        year_matches = re.findall(self.year_pattern, reference_text)
        if year_matches:
            info["year"] = year_matches[0]  # ì²« ë²ˆì§¸ ë…„ë„
            info["confidence"] += 0.3

        # ì œëª© ì¶”ì¶œ (í°ë”°ì˜´í‘œ ì•ˆì˜ í…ìŠ¤íŠ¸)
        title_matches = re.findall(r'"([^"]+)"', reference_text)
        if title_matches:
            info["title"] = title_matches[0].strip()
            info["confidence"] += 0.4
        else:
            # í°ë”°ì˜´í‘œê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„
            # ì €ìëª… ë‹¤ìŒë¶€í„° ë…„ë„/ì €ë„ëª… ì „ê¹Œì§€
            title_pattern = (
                r"[A-Z][a-z]+.*?\.\s*([^.]+?)\s*(?:"
                + self.year_pattern
                + r"|In\s|Proceedings|Journal)"
            )
            title_match = re.search(title_pattern, reference_text)
            if title_match:
                potential_title = title_match.group(1).strip()
                if len(potential_title) > 10:
                    info["title"] = potential_title
                    info["confidence"] += 0.2

        # ì €ì ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        # ì²« ë²ˆì§¸ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤
        author_pattern = r"^([A-Z][a-z]+(?:\s+[A-Z]\.?\s*)*[A-Z][a-z]+)"
        author_match = re.match(author_pattern, reference_text)
        if author_match:
            info["authors"] = [author_match.group(1).strip()]
            info["confidence"] += 0.3

        return info

    def match_reference_to_papers(self, reference_info, paper_titles):
        """ì°¸ê³ ë¬¸í—Œì„ ë…¼ë¬¸ ë°ì´í„°ì…‹ì˜ ë…¼ë¬¸ë“¤ê³¼ ë§¤ì¹­"""
        if not reference_info["title"]:
            return None, 0.0

        ref_title = reference_info["title"].lower().strip()
        if len(ref_title) < 10:  # ë„ˆë¬´ ì§§ì€ ì œëª©ì€ ì œì™¸
            return None, 0.0

        best_match = None
        best_similarity = 0.0

        for paper_id, paper_title in paper_titles.items():
            paper_title_clean = paper_title.lower().strip()

            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = SequenceMatcher(None, ref_title, paper_title_clean).ratio()

            # ì •í™•í•œ ë§¤ì¹˜ ìš°ì„ 
            if ref_title in paper_title_clean or paper_title_clean in ref_title:
                similarity += 0.3

            # ë…„ë„ê°€ ì¼ì¹˜í•˜ë©´ ë³´ë„ˆìŠ¤
            if reference_info["year"] and reference_info["year"] in paper_title:
                similarity += 0.1

            if similarity > best_similarity and similarity > 0.7:  # ì„ê³„ê°’
                best_similarity = similarity
                best_match = paper_id

        return best_match, best_similarity

    def extract_citations_for_paper(self, paper_id, pdf_path, all_paper_titles):
        """ê°œë³„ ë…¼ë¬¸ì˜ ì¸ìš© ê´€ê³„ ì¶”ì¶œ"""
        text = self.extract_full_text_from_pdf(pdf_path)
        if not text:
            return []

        # References ì„¹ì…˜ ì¶”ì¶œ
        references_text = self.find_references_section(text)
        if not references_text:
            return []

        # ì°¸ê³ ë¬¸í—Œ íŒŒì‹±
        references = self.parse_references(references_text)

        citations = []
        for ref_text in references:
            ref_info = self.extract_reference_info(ref_text)

            # ë…¼ë¬¸ ë°ì´í„°ì…‹ê³¼ ë§¤ì¹­
            matched_paper, similarity = self.match_reference_to_papers(
                ref_info, all_paper_titles
            )

            if matched_paper and similarity > 0.7:
                citations.append(
                    {
                        "cited_paper_id": matched_paper,
                        "similarity": similarity,
                        "reference_text": (
                            ref_text[:200] + "..." if len(ref_text) > 200 else ref_text
                        ),
                        "extracted_title": ref_info["title"],
                        "extracted_year": ref_info["year"],
                    }
                )

        return citations

    def build_citation_network(self, papers_metadata):
        """ì „ì²´ ë…¼ë¬¸ ë°ì´í„°ì…‹ì˜ ì¸ìš© ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•"""
        print("ğŸ”— Building citation network from PDF references...")

        # ë…¼ë¬¸ ì œëª© ë”•ì…”ë„ˆë¦¬ ìƒì„± (ë§¤ì¹­ìš©)
        paper_titles = {}
        pdf_papers = {}

        for i, paper in enumerate(papers_metadata):
            paper_id = f"paper_{i}"
            paper_titles[paper_id] = paper["title"]

            if paper["has_pdf"] and paper["pdf_file"]:
                pdf_papers[paper_id] = paper["pdf_file"]

        print(f"ğŸ“„ Processing {len(pdf_papers)} papers with PDFs...")

        citation_network = {}
        total_citations = 0

        # ê° ë…¼ë¬¸ì˜ ì¸ìš© ê´€ê³„ ì¶”ì¶œ
        for paper_id, pdf_path in tqdm(pdf_papers.items(), desc="Extracting citations"):
            try:
                citations = self.extract_citations_for_paper(
                    paper_id, pdf_path, paper_titles
                )
                citation_network[paper_id] = citations
                total_citations += len(citations)

            except Exception as e:
                citation_network[paper_id] = []
                continue

        # í†µê³„ ì •ë³´
        papers_with_citations = sum(1 for cites in citation_network.values() if cites)

        print(f"âœ… Citation network extraction completed:")
        print(f"   ğŸ“„ Papers processed: {len(citation_network)}")
        print(f"   ğŸ”— Papers with citations: {papers_with_citations}")
        print(f"   ğŸ“Š Total citations found: {total_citations}")
        print(
            f"   ğŸ“ˆ Average citations per paper: {total_citations/len(citation_network):.1f}"
        )

        return citation_network

    def save_citation_network(self, citation_network, output_dir):
        """ì¸ìš© ë„¤íŠ¸ì›Œí¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        output_dir = Path(output_dir)

        # ìƒì„¸ ì •ë³´ ì €ì¥
        detailed_file = output_dir / "citation_network_detailed.json"
        with open(detailed_file, "w", encoding="utf-8") as f:
            json.dump(citation_network, f, ensure_ascii=False, indent=2)

        # ê°„ë‹¨í•œ í˜•íƒœë¡œ ë³€í™˜ (ê·¸ë˜í”„ êµ¬ì¶•ìš©)
        simple_network = {}
        for citing_paper, citations in citation_network.items():
            simple_network[citing_paper] = [
                {
                    "cited_paper": cite["cited_paper_id"],
                    "similarity": cite["similarity"],
                }
                for cite in citations
            ]

        simple_file = output_dir / "citation_network_simple.json"
        with open(simple_file, "w", encoding="utf-8") as f:
            json.dump(simple_network, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Citation network saved:")
        print(f"   ğŸ“„ Detailed: {detailed_file}")
        print(f"   ğŸ“Š Simple: {simple_file}")

        return simple_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import PROCESSED_DIR

    # í†µí•© ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_file = PROCESSED_DIR / "integrated_papers_metadata.json"

    if not metadata_file.exists():
        print("âŒ Integrated papers metadata not found. Run main.py first.")
        return

    with open(metadata_file, "r", encoding="utf-8") as f:
        papers_metadata = json.load(f)

    print(f"ğŸ“„ Loaded {len(papers_metadata)} papers metadata")

    # Reference ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = ReferenceExtractor()

    # ì¸ìš© ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•
    citation_network = extractor.build_citation_network(papers_metadata)

    # ê²°ê³¼ ì €ì¥
    output_file = extractor.save_citation_network(citation_network, PROCESSED_DIR)

    return citation_network, output_file


if __name__ == "__main__":
    main()
