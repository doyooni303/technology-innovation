"""
í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Keyword Co-occurrence Graph Construction Module
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from itertools import combinations
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import normalize
import re


class KeywordCooccurrenceGraphBuilder:
    """í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(
        self, min_keyword_freq=2, min_cooccurrence=2, max_keywords_per_paper=50
    ):
        """
        Args:
            min_keyword_freq (int): í¬í•¨í•  í‚¤ì›Œë“œì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„
            min_cooccurrence (int): ê·¸ë˜í”„ì— í¬í•¨í•  ìµœì†Œ ë™ì‹œ ì¶œí˜„ íšŸìˆ˜
            max_keywords_per_paper (int): ë…¼ë¬¸ë‹¹ ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜ (ë…¸ì´ì¦ˆ ì œê±°)
        """
        self.min_keyword_freq = min_keyword_freq
        self.min_cooccurrence = min_cooccurrence
        self.max_keywords_per_paper = max_keywords_per_paper

        # ê²°ê³¼ ì €ì¥ìš©
        self.keyword_freq = Counter()
        self.cooccurrence_matrix = defaultdict(lambda: defaultdict(int))
        self.keyword_to_papers = defaultdict(set)
        self.paper_to_keywords = {}

    def clean_keyword(self, keyword):
        """í‚¤ì›Œë“œ ì •ì œ í•¨ìˆ˜"""
        if not keyword or not isinstance(keyword, str):
            return None

        # ì†Œë¬¸ì ë³€í™˜
        keyword = keyword.lower().strip()

        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í‚¤ì›Œë“œ ì œê±°
        if len(keyword) < 2 or len(keyword) > 50:
            return None

        # ìˆ«ìë§Œ ìˆëŠ” í‚¤ì›Œë“œ ì œê±°
        if keyword.isdigit():
            return None

        # íŠ¹ìˆ˜ ë¬¸ìë§Œ ìˆëŠ” í‚¤ì›Œë“œ ì œê±°
        if re.match(r"^[^a-zA-Z0-9]+$", keyword):
            return None

        # ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            "the",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
            "from",
            "up",
            "about",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "between",
            "among",
            "this",
            "that",
            "these",
            "those",
            "i",
            "me",
            "my",
            "myself",
            "we",
            "our",
            "ours",
            "ourselves",
            "you",
            "your",
            "yours",
            "yourself",
            "yourselves",
            "he",
            "him",
            "his",
            "himself",
            "she",
            "her",
            "hers",
            "herself",
            "it",
            "its",
            "itself",
            "they",
            "them",
            "their",
            "theirs",
            "themselves",
            "paper",
            "study",
            "research",
            "method",
            "approach",
            "analysis",
            "result",
            "conclusion",
        }

        if keyword in stopwords:
            return None

        return keyword

    def extract_keywords_from_papers(self, papers_metadata):
        """ë…¼ë¬¸ë“¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ” Extracting and cleaning keywords from papers...")

        valid_papers = 0
        total_raw_keywords = 0
        total_clean_keywords = 0

        for i, paper in enumerate(tqdm(papers_metadata, desc="Processing papers")):
            paper_id = f"paper_{i}"
            title = paper.get("title", "")
            keywords = paper.get("keywords", [])

            if not keywords:
                continue

            # í‚¤ì›Œë“œ ì •ì œ
            clean_keywords = []
            total_raw_keywords += len(keywords)

            for keyword in keywords:
                clean_kw = self.clean_keyword(keyword)
                if clean_kw:
                    clean_keywords.append(clean_kw)

            # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ (í’ˆì§ˆ í™•ë³´)
            if len(clean_keywords) > self.max_keywords_per_paper:
                # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ í‚¤ì›Œë“œë§Œ ì„ íƒ
                clean_keywords = clean_keywords[: self.max_keywords_per_paper]

            if clean_keywords:
                self.paper_to_keywords[paper_id] = {
                    "title": title,
                    "keywords": clean_keywords,
                    "original_keyword_count": len(keywords),
                    "clean_keyword_count": len(clean_keywords),
                }

                # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
                for keyword in clean_keywords:
                    self.keyword_freq[keyword] += 1
                    self.keyword_to_papers[keyword].add(paper_id)

                valid_papers += 1
                total_clean_keywords += len(clean_keywords)

        print(f"âœ… Keyword extraction completed:")
        print(f"   ğŸ“„ Papers with keywords: {valid_papers}/{len(papers_metadata)}")
        print(f"   ğŸ”¤ Raw keywords: {total_raw_keywords}")
        print(f"   âœ¨ Clean keywords: {total_clean_keywords}")
        print(f"   ğŸ“Š Unique keywords: {len(self.keyword_freq)}")
        print(f"   ğŸ“ˆ Avg keywords per paper: {total_clean_keywords/valid_papers:.1f}")

    def filter_keywords_by_frequency(self):
        """ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ í•„í„°ë§"""
        print(f"ğŸ” Filtering keywords (min frequency: {self.min_keyword_freq})...")

        # ìµœì†Œ ë¹ˆë„ ì´ìƒì˜ í‚¤ì›Œë“œë§Œ ì„ íƒ
        frequent_keywords = {
            kw
            for kw, freq in self.keyword_freq.items()
            if freq >= self.min_keyword_freq
        }

        print(f"   ğŸ“Š Keywords before filtering: {len(self.keyword_freq)}")
        print(f"   âœ… Keywords after filtering: {len(frequent_keywords)}")

        # ë…¼ë¬¸ ë°ì´í„°ì—ì„œë„ í•„í„°ë§ëœ í‚¤ì›Œë“œë§Œ ìœ ì§€
        filtered_papers = {}
        for paper_id, paper_data in self.paper_to_keywords.items():
            filtered_keywords = [
                kw for kw in paper_data["keywords"] if kw in frequent_keywords
            ]

            if filtered_keywords:  # í‚¤ì›Œë“œê°€ ë‚¨ì•„ìˆëŠ” ë…¼ë¬¸ë§Œ ìœ ì§€
                filtered_papers[paper_id] = paper_data.copy()
                filtered_papers[paper_id]["keywords"] = filtered_keywords
                filtered_papers[paper_id]["filtered_keyword_count"] = len(
                    filtered_keywords
                )

        self.paper_to_keywords = filtered_papers

        # í‚¤ì›Œë“œ ë¹ˆë„ë„ ì—…ë°ì´íŠ¸
        self.keyword_freq = {
            kw: freq
            for kw, freq in self.keyword_freq.items()
            if kw in frequent_keywords
        }

        print(f"   ğŸ“„ Papers remaining: {len(self.paper_to_keywords)}")

        return frequent_keywords

    def compute_cooccurrence_matrix(self):
        """í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ í–‰ë ¬ ê³„ì‚°"""
        print("ğŸ“Š Computing keyword co-occurrence matrix...")

        total_pairs = 0

        for paper_id, paper_data in tqdm(
            self.paper_to_keywords.items(), desc="Computing co-occurrences"
        ):
            keywords = paper_data["keywords"]

            # ë…¼ë¬¸ ë‚´ ëª¨ë“  í‚¤ì›Œë“œ ìŒì˜ ë™ì‹œ ì¶œí˜„ ê³„ì‚°
            for kw1, kw2 in combinations(keywords, 2):
                # ì‚¬ì „ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
                if kw1 > kw2:
                    kw1, kw2 = kw2, kw1

                self.cooccurrence_matrix[kw1][kw2] += 1
                total_pairs += 1

        print(f"âœ… Co-occurrence computation completed:")
        print(f"   ğŸ”— Total keyword pairs processed: {total_pairs}")
        print(
            f"   ğŸ“Š Unique co-occurring pairs: {sum(len(v) for v in self.cooccurrence_matrix.values())}"
        )

    def build_cooccurrence_graph(self):
        """ë™ì‹œ ì¶œí˜„ ê·¸ë˜í”„ êµ¬ì¶•"""
        print(
            f"ğŸ”— Building co-occurrence graph (min co-occurrence: {self.min_cooccurrence})..."
        )

        # NetworkX ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()

        # í‚¤ì›Œë“œ ë…¸ë“œ ì¶”ê°€ (ë¹ˆë„ ì •ë³´ í¬í•¨)
        for keyword, freq in self.keyword_freq.items():
            G.add_node(
                keyword,
                node_type="keyword",
                frequency=freq,
                papers=list(self.keyword_to_papers[keyword]),
            )

        # ë™ì‹œ ì¶œí˜„ ì—£ì§€ ì¶”ê°€
        edges_added = 0
        cooccurrence_weights = []

        for kw1, cooccurrences in self.cooccurrence_matrix.items():
            for kw2, count in cooccurrences.items():
                if count >= self.min_cooccurrence:
                    # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ ê³„ì‚° (Jaccard similarity)
                    kw1_freq = self.keyword_freq[kw1]
                    kw2_freq = self.keyword_freq[kw2]
                    jaccard_sim = count / (kw1_freq + kw2_freq - count)

                    G.add_edge(
                        kw1,
                        kw2,
                        weight=count,
                        jaccard_similarity=jaccard_sim,
                        normalized_weight=count / max(kw1_freq, kw2_freq),
                    )

                    edges_added += 1
                    cooccurrence_weights.append(count)

        print(f"âœ… Graph construction completed:")
        print(f"   ğŸ”— Nodes (keywords): {G.number_of_nodes()}")
        print(f"   ğŸ“Š Edges (co-occurrences): {G.number_of_edges()}")
        print(f"   ğŸ“ˆ Graph density: {nx.density(G):.4f}")

        if cooccurrence_weights:
            print(f"   ğŸ¯ Co-occurrence weight stats:")
            print(f"      Min: {min(cooccurrence_weights)}")
            print(f"      Max: {max(cooccurrence_weights)}")
            print(f"      Mean: {np.mean(cooccurrence_weights):.1f}")
            print(f"      Median: {np.median(cooccurrence_weights):.1f}")

        return G

    def analyze_graph_properties(self, G):
        """ê·¸ë˜í”„ ì†ì„± ë¶„ì„"""
        print("ğŸ“ˆ Analyzing graph properties...")

        # ê¸°ë³¸ í†µê³„
        stats = {
            "basic_stats": {
                "num_nodes": G.number_of_nodes(),
                "num_edges": G.number_of_edges(),
                "density": float(nx.density(G)),  # âœ… float ë³€í™˜
                "is_connected": nx.is_connected(G),
            }
        }

        # ì—°ê²° ì„±ë¶„ ë¶„ì„
        if not nx.is_connected(G):
            components = list(nx.connected_components(G))
            stats["connectivity"] = {
                "num_components": len(components),
                "largest_component_size": len(max(components, key=len)),
                "component_sizes": [len(comp) for comp in components],
            }

        # ì¤‘ì‹¬ì„± ë¶„ì„ (ìƒìœ„ í‚¤ì›Œë“œë“¤ë§Œ)
        if G.number_of_nodes() > 0:
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(
                G, k=min(1000, G.number_of_nodes())
            )
            eigenvector_centrality = nx.eigenvector_centrality(
                G, max_iter=1000, tol=1e-3
            )

            # âœ… numpy íƒ€ì…ì„ floatë¡œ ë³€í™˜
            stats["centrality"] = {
                "top_degree": [
                    (k, float(v))
                    for k, v in sorted(
                        degree_centrality.items(), key=lambda x: x[1], reverse=True
                    )[:10]
                ],
                "top_betweenness": [
                    (k, float(v))
                    for k, v in sorted(
                        betweenness_centrality.items(), key=lambda x: x[1], reverse=True
                    )[:10]
                ],
                "top_eigenvector": [
                    (k, float(v))
                    for k, v in sorted(
                        eigenvector_centrality.items(), key=lambda x: x[1], reverse=True
                    )[:10]
                ],
            }

        # í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜
        if G.number_of_nodes() > 0:
            clustering_coeffs = nx.clustering(G)
            stats["clustering"] = {
                "average_clustering": float(nx.average_clustering(G)),  # âœ… float ë³€í™˜
                "global_clustering": float(nx.transitivity(G)),  # âœ… float ë³€í™˜
                "top_clustered_keywords": [
                    (k, float(v))
                    for k, v in sorted(
                        clustering_coeffs.items(), key=lambda x: x[1], reverse=True
                    )[:10]
                ],  # âœ… float ë³€í™˜
            }

        return stats

    def save_graph_and_analysis(self, G, stats, output_dir):
        """ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(output_dir)

        # 1. NetworkX ê·¸ë˜í”„ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (GraphRAGìš©)
        graph_data = {"nodes": [], "edges": []}

        # ë…¸ë“œ ì •ë³´
        for node in G.nodes():
            node_data = G.nodes[node].copy()
            node_data["id"] = node
            # papers ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
            if "papers" in node_data:
                node_data["papers"] = list(node_data["papers"])
            graph_data["nodes"].append(node_data)

        # ì—£ì§€ ì •ë³´
        for edge in G.edges():
            edge_data = G.edges[edge].copy()
            edge_data["source"] = edge[0]
            edge_data["target"] = edge[1]
            graph_data["edges"].append(edge_data)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        graph_file = output_dir / "keyword_cooccurrence_graph.json"
        with open(graph_file, "w", encoding="utf-8") as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)

        # 2. GraphML íŒŒì¼ë¡œ ì €ì¥ (Gephi, Cytoscape ë“±ì—ì„œ ì‚¬ìš©)
        # GraphMLì€ ë³µì¡í•œ ë°ì´í„° íƒ€ì…ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
        try:
            # GraphML í˜¸í™˜ì„ ìœ„í•´ ê·¸ë˜í”„ ë³µì‚¬ ë° ì†ì„± ë³€í™˜
            G_graphml = G.copy()

            # ë…¸ë“œ ì†ì„±ì„ GraphML í˜¸í™˜ í˜•íƒœë¡œ ë³€í™˜
            for node in G_graphml.nodes():
                # papers ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
                if "papers" in G_graphml.nodes[node]:
                    papers_list = G_graphml.nodes[node]["papers"]
                    if isinstance(papers_list, (list, set)):
                        G_graphml.nodes[node]["papers"] = ";".join(
                            str(p) for p in papers_list
                        )

                # ê¸°íƒ€ ë³µì¡í•œ íƒ€ì…ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                for attr_name, attr_value in G_graphml.nodes[node].items():
                    if isinstance(attr_value, (list, set, dict)):
                        if isinstance(attr_value, dict):
                            G_graphml.nodes[node][attr_name] = json.dumps(attr_value)
                        else:
                            G_graphml.nodes[node][attr_name] = ";".join(
                                str(v) for v in attr_value
                            )
                    elif not isinstance(attr_value, (str, int, float, bool)):
                        G_graphml.nodes[node][attr_name] = str(attr_value)

            # ì—£ì§€ ì†ì„±ë„ í™•ì¸
            for edge in G_graphml.edges():
                for attr_name, attr_value in G_graphml.edges[edge].items():
                    if isinstance(attr_value, (list, set, dict)):
                        if isinstance(attr_value, dict):
                            G_graphml.edges[edge][attr_name] = json.dumps(attr_value)
                        else:
                            G_graphml.edges[edge][attr_name] = ";".join(
                                str(v) for v in attr_value
                            )
                    elif not isinstance(attr_value, (str, int, float, bool)):
                        G_graphml.edges[edge][attr_name] = str(attr_value)

            graphml_file = output_dir / "keyword_cooccurrence_graph.graphml"
            nx.write_graphml(G_graphml, graphml_file)

        except Exception as e:
            print(f"âš ï¸  GraphML ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“„ JSON íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}")
            graphml_file = None

        # 3. í†µê³„ ë¶„ì„ ê²°ê³¼ ì €ì¥
        stats_file = output_dir / "keyword_cooccurrence_analysis.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        # 4. í‚¤ì›Œë“œ ë¹ˆë„ ë°ì´í„° ì €ì¥
        keyword_data = {
            "keyword_frequencies": dict(self.keyword_freq),
            "keyword_to_papers": {
                kw: list(papers) for kw, papers in self.keyword_to_papers.items()
            },
            "total_keywords": len(self.keyword_freq),
            "min_frequency_threshold": self.min_keyword_freq,
        }

        keywords_file = output_dir / "keyword_frequencies.json"
        with open(keywords_file, "w", encoding="utf-8") as f:
            json.dump(keyword_data, f, ensure_ascii=False, indent=2)

        # 5. ë™ì‹œ ì¶œí˜„ í–‰ë ¬ ì €ì¥ (CSV í˜•íƒœ)
        cooccurrence_data = []
        for kw1, cooccurrences in self.cooccurrence_matrix.items():
            for kw2, count in cooccurrences.items():
                if count >= self.min_cooccurrence:
                    cooccurrence_data.append(
                        {
                            "keyword1": kw1,
                            "keyword2": kw2,
                            "cooccurrence_count": count,
                            "jaccard_similarity": count
                            / (self.keyword_freq[kw1] + self.keyword_freq[kw2] - count),
                        }
                    )

        cooccurrence_df = pd.DataFrame(cooccurrence_data)
        cooccurrence_file = output_dir / "keyword_cooccurrence_matrix.csv"
        cooccurrence_df.to_csv(cooccurrence_file, index=False, encoding="utf-8")

        print(f"ğŸ’¾ Results saved:")
        print(f"   ğŸ”— Graph (JSON): {graph_file}")
        if graphml_file:
            print(f"   ğŸ”— Graph (GraphML): {graphml_file}")
        print(f"   ğŸ“Š Analysis: {stats_file}")
        print(f"   ğŸ”¤ Keywords: {keywords_file}")
        print(f"   ğŸ“ˆ Co-occurrence Matrix: {cooccurrence_file}")

        return graph_file

    def create_visualization_data(self, G, stats):
        """ì‹œê°í™”ìš© ë°ì´í„° ìƒì„±"""

        # ìƒìœ„ í‚¤ì›Œë“œë“¤ì˜ ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ (ì‹œê°í™”ìš©)
        top_keywords = [kw for kw, freq in Counter(self.keyword_freq).most_common(50)]
        subgraph = G.subgraph(top_keywords).copy()

        # ìœ„ì¹˜ ê³„ì‚° (spring layout)
        pos = nx.spring_layout(subgraph, k=1, iterations=50)

        visualization_data = {
            "subgraph_nodes": [],
            "subgraph_edges": [],
            "layout_positions": {
                k: [float(v[0]), float(v[1])] for k, v in pos.items()
            },  # âœ… numpy float ë³€í™˜
        }

        # ë…¸ë“œ ë°ì´í„° (í¬ê¸°ëŠ” ë¹ˆë„, ìƒ‰ìƒì€ ì¤‘ì‹¬ì„±)
        degree_centrality = nx.degree_centrality(subgraph)

        for node in subgraph.nodes():
            node_info = {
                "id": node,
                "frequency": self.keyword_freq[node],
                "degree_centrality": float(
                    degree_centrality.get(node, 0)
                ),  # âœ… float ë³€í™˜
                "degree": subgraph.degree(node),
                "x": float(pos[node][0]),  # âœ… numpy floatë¥¼ Python floatë¡œ ë³€í™˜
                "y": float(pos[node][1]),  # âœ… numpy floatë¥¼ Python floatë¡œ ë³€í™˜
            }
            visualization_data["subgraph_nodes"].append(node_info)

        # ì—£ì§€ ë°ì´í„°
        for edge in subgraph.edges():
            edge_info = {
                "source": edge[0],
                "target": edge[1],
                "weight": subgraph.edges[edge]["weight"],
                "jaccard_similarity": float(
                    subgraph.edges[edge]["jaccard_similarity"]
                ),  # âœ… float ë³€í™˜
            }
            visualization_data["subgraph_edges"].append(edge_info)

        return visualization_data

    def process_papers(self, papers_metadata):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ Starting keyword co-occurrence graph construction...")

        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        self.extract_keywords_from_papers(papers_metadata)

        # 2. ë¹ˆë„ ê¸°ì¤€ í•„í„°ë§
        frequent_keywords = self.filter_keywords_by_frequency()

        # 3. ë™ì‹œ ì¶œí˜„ í–‰ë ¬ ê³„ì‚°
        self.compute_cooccurrence_matrix()

        # 4. ê·¸ë˜í”„ êµ¬ì¶•
        G = self.build_cooccurrence_graph()

        # 5. ê·¸ë˜í”„ ë¶„ì„
        stats = self.analyze_graph_properties(G)

        # 6. ì‹œê°í™” ë°ì´í„° ìƒì„±
        viz_data = self.create_visualization_data(G, stats)
        stats["visualization_data"] = viz_data

        return G, stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import RAW_EXTRACTIONS_DIR, GRAPHS_DIR

    # í†µí•© ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_file = RAW_EXTRACTIONS_DIR / "integrated_papers_metadata.json"

    if not metadata_file.exists():
        print("âŒ Integrated papers metadata not found. Run main.py first.")
        return

    with open(metadata_file, "r", encoding="utf-8") as f:
        papers_metadata = json.load(f)

    print(f"ğŸ“„ Loaded {len(papers_metadata)} papers metadata")

    # Keyword Co-occurrence Graph Builder ì´ˆê¸°í™”
    builder = KeywordCooccurrenceGraphBuilder(
        min_keyword_freq=2,  # ìµœì†Œ 2ë²ˆ ì´ìƒ ì¶œí˜„í•œ í‚¤ì›Œë“œë§Œ í¬í•¨
        min_cooccurrence=2,  # ìµœì†Œ 2ë²ˆ ì´ìƒ ë™ì‹œ ì¶œí˜„í•œ ìŒë§Œ ì—£ì§€ë¡œ ì—°ê²°
        max_keywords_per_paper=30,  # ë…¼ë¬¸ë‹¹ ìµœëŒ€ 30ê°œ í‚¤ì›Œë“œ (í’ˆì§ˆ í™•ë³´)
    )

    # ì „ì²´ ì²˜ë¦¬
    G, stats = builder.process_papers(papers_metadata)

    # ê²°ê³¼ ì €ì¥
    output_file = builder.save_graph_and_analysis(G, stats, GRAPHS_DIR)

    print(f"\nâœ… Keyword co-occurrence graph construction completed!")
    print(f"ğŸ“ Main output: {output_file}")

    # ìš”ì•½ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š Final Summary:")
    print(f"   ğŸ”¤ Total unique keywords: {len(builder.keyword_freq)}")
    print(f"   ğŸ”— Graph nodes: {G.number_of_nodes()}")
    print(f"   ğŸ“Š Graph edges: {G.number_of_edges()}")
    print(f"   ğŸ¯ Graph density: {nx.density(G):.4f}")

    if stats.get("centrality"):
        top_keywords = stats["centrality"]["top_degree"][:5]
        print(f"   ğŸ† Top 5 central keywords:")
        for i, (kw, centrality) in enumerate(top_keywords):
            freq = builder.keyword_freq[kw]
            print(f"      {i+1}. {kw} (freq: {freq}, centrality: {centrality:.3f})")

    return G, output_file


if __name__ == "__main__":
    main()
