"""
ì €ì í˜‘ì—… ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Author Collaboration Network Graph Construction Module
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from itertools import combinations
from tqdm import tqdm
import re


class AuthorCollaborationGraphBuilder:
    """ì €ì í˜‘ì—… ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, min_collaborations=1, min_papers=1):
        """
        Args:
            min_collaborations (int): ê·¸ë˜í”„ì— í¬í•¨í•  ìµœì†Œ í˜‘ì—… íšŸìˆ˜
            min_papers (int): ê·¸ë˜í”„ì— í¬í•¨í•  ì €ìì˜ ìµœì†Œ ë…¼ë¬¸ ìˆ˜
        """
        self.min_collaborations = min_collaborations
        self.min_papers = min_papers

        # ê²°ê³¼ ì €ì¥ìš©
        self.author_papers = defaultdict(list)  # ì €ìë³„ ë…¼ë¬¸ ëª©ë¡
        self.paper_authors = {}  # ë…¼ë¬¸ë³„ ì €ì ëª©ë¡
        self.collaboration_matrix = defaultdict(lambda: defaultdict(int))
        self.author_stats = {}
        self.papers_metadata = None

    def clean_author_name(self, author_name):
        """ì €ìëª… ì •ì œ í•¨ìˆ˜"""
        if not author_name or not isinstance(author_name, str):
            return None

        # ê¸°ë³¸ ì •ì œ
        author_name = author_name.strip()

        # ë„ˆë¬´ ì§§ì€ ì´ë¦„ ì œì™¸
        if len(author_name) < 2:
            return None

        # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if author_name.isdigit():
            return None

        # ê¸°ë³¸ì ì¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬)
        author_name = re.sub(r"[^\w\s\.\-]", " ", author_name)
        author_name = re.sub(r"\s+", " ", author_name).strip()

        # ì¼ë°˜ì ì¸ í˜•íƒœë¡œ ì •ê·œí™”
        # "Smith, John" â†’ "John Smith" í˜•íƒœë¡œ ë³€í™˜
        if "," in author_name:
            parts = author_name.split(",", 1)
            if len(parts) == 2:
                last_name = parts[0].strip()
                first_name = parts[1].strip()
                author_name = f"{first_name} {last_name}".strip()

        # ì—°ì†ëœ ê³µë°± ì œê±°
        author_name = re.sub(r"\s+", " ", author_name)

        return author_name

    def normalize_author_name(self, author_name):
        """ì €ìëª… ì •ê·œí™” (ë™ì¼ ì¸ë¬¼ ì¸ì‹ì„ ìœ„í•œ ê³ ê¸‰ ì •ì œ)"""
        if not author_name:
            return None

        # ê¸°ë³¸ ì •ì œ
        clean_name = self.clean_author_name(author_name)
        if not clean_name:
            return None

        # ì†Œë¬¸ì ë³€í™˜
        normalized = clean_name.lower()

        # ì ê³¼ ê³µë°± ì •ë¦¬
        normalized = re.sub(r"\.", "", normalized)
        normalized = re.sub(r"\s+", " ", normalized).strip()

        return normalized

    def extract_author_collaborations(self, papers_metadata):
        """ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì €ì í˜‘ì—… ê´€ê³„ ì¶”ì¶œ"""
        print("ğŸ” Extracting author collaborations from papers...")

        valid_papers = 0
        total_authors = 0
        collaboration_pairs = 0

        for i, paper in enumerate(tqdm(papers_metadata, desc="Processing papers")):
            paper_id = f"paper_{i}"
            title = paper.get("title", "")
            authors = paper.get("authors", [])
            year = paper.get("year", "")
            journal = paper.get("journal", "")

            if not authors or len(authors) < 1:
                continue

            # ì €ìëª… ì •ì œ
            clean_authors = []
            for author in authors:
                clean_author = self.clean_author_name(author)
                if clean_author:
                    clean_authors.append(clean_author)

            if len(clean_authors) < 1:
                continue

            # ë…¼ë¬¸-ì €ì ë§¤í•‘ ì €ì¥
            self.paper_authors[paper_id] = {
                "title": title,
                "authors": clean_authors,
                "year": year,
                "journal": journal,
                "author_count": len(clean_authors),
            }

            # ì €ì-ë…¼ë¬¸ ë§¤í•‘ ì €ì¥
            for author in clean_authors:
                self.author_papers[author].append(
                    {
                        "paper_id": paper_id,
                        "title": title,
                        "year": year,
                        "journal": journal,
                        "co_authors": [a for a in clean_authors if a != author],
                    }
                )

            # í˜‘ì—… ê´€ê³„ ê³„ì‚° (ë…¼ë¬¸ ë‚´ ëª¨ë“  ì €ì ìŒ)
            if len(clean_authors) > 1:
                for author1, author2 in combinations(clean_authors, 2):
                    # ì‚¬ì „ìˆœ ì •ë ¬ë¡œ ì¤‘ë³µ ë°©ì§€
                    if author1 > author2:
                        author1, author2 = author2, author1

                    self.collaboration_matrix[author1][author2] += 1
                    collaboration_pairs += 1

            valid_papers += 1
            total_authors += len(clean_authors)

        print(f"âœ… Author collaboration extraction completed:")
        print(f"   ğŸ“„ Papers with authors: {valid_papers}")
        print(f"   ğŸ‘¥ Total author instances: {total_authors}")
        print(f"   ğŸ¤ Unique authors: {len(self.author_papers)}")
        print(f"   ğŸ”— Collaboration pairs: {collaboration_pairs}")
        print(f"   ğŸ“ˆ Avg authors per paper: {total_authors/valid_papers:.1f}")

    def calculate_author_statistics(self):
        """ì €ìë³„ í†µê³„ ê³„ì‚°"""
        print("ğŸ“Š Calculating author statistics...")

        for author, papers in self.author_papers.items():
            # ê¸°ë³¸ í†µê³„
            paper_count = len(papers)
            years = [p["year"] for p in papers if p["year"]]
            journals = [p["journal"] for p in papers if p["journal"]]

            # í™œë™ ê¸°ê°„
            if years:
                try:
                    year_ints = [int(y) for y in years if str(y).isdigit()]
                    if year_ints:
                        first_year = min(year_ints)
                        last_year = max(year_ints)
                        active_years = last_year - first_year + 1
                    else:
                        first_year = last_year = active_years = 0
                except:
                    first_year = last_year = active_years = 0
            else:
                first_year = last_year = active_years = 0

            # í˜‘ì—…ì ìˆ˜ ê³„ì‚°
            collaborators = set()
            for paper in papers:
                collaborators.update(paper["co_authors"])

            # ì €ë„ ë‹¤ì–‘ì„±
            unique_journals = len(set(journals)) if journals else 0

            self.author_stats[author] = {
                "paper_count": paper_count,
                "collaborator_count": len(collaborators),
                "first_year": first_year,
                "last_year": last_year,
                "active_years": active_years,
                "unique_journals": unique_journals,
                "most_frequent_journal": (
                    Counter(journals).most_common(1)[0][0] if journals else ""
                ),
                "collaborators": list(collaborators),
                "papers": papers,
            }

        print(f"âœ… Author statistics calculated for {len(self.author_stats)} authors")

    def filter_authors_by_activity(self):
        """í™œë™ ìˆ˜ì¤€ ê¸°ì¤€ìœ¼ë¡œ ì €ì í•„í„°ë§"""
        print(f"ğŸ” Filtering authors (min papers: {self.min_papers})...")

        # ìµœì†Œ ë…¼ë¬¸ ìˆ˜ ì´ìƒì˜ ì €ìë§Œ ì„ íƒ
        active_authors = {
            author
            for author, stats in self.author_stats.items()
            if stats["paper_count"] >= self.min_papers
        }

        print(f"   ğŸ“Š Authors before filtering: {len(self.author_stats)}")
        print(f"   âœ… Authors after filtering: {len(active_authors)}")

        # í˜‘ì—… ê´€ê³„ë„ í•„í„°ë§
        filtered_collaborations = defaultdict(lambda: defaultdict(int))
        for author1, collaborations in self.collaboration_matrix.items():
            if author1 in active_authors:
                for author2, count in collaborations.items():
                    if author2 in active_authors and count >= self.min_collaborations:
                        filtered_collaborations[author1][author2] = count

        self.collaboration_matrix = filtered_collaborations

        # ì €ì í†µê³„ë„ í•„í„°ë§
        self.author_stats = {
            author: stats
            for author, stats in self.author_stats.items()
            if author in active_authors
        }

        self.author_papers = {
            author: papers
            for author, papers in self.author_papers.items()
            if author in active_authors
        }

        print(f"   ğŸ“„ Active authors remaining: {len(self.author_stats)}")
        return active_authors

    def build_collaboration_graph(self):
        """í˜‘ì—… ê´€ê³„ ê·¸ë˜í”„ êµ¬ì¶•"""
        print(f"ğŸ”— Building author collaboration graph...")

        # ë¬´ë°©í–¥ ê·¸ë˜í”„ ìƒì„± (í˜‘ì—…ì€ ìƒí˜¸ì )
        G = nx.Graph()

        # ì €ì ë…¸ë“œ ì¶”ê°€
        for author, stats in self.author_stats.items():
            G.add_node(
                author,
                node_type="author",
                name=author,
                paper_count=stats["paper_count"],
                collaborator_count=stats["collaborator_count"],
                first_year=stats["first_year"],
                last_year=stats["last_year"],
                active_years=stats["active_years"],
                unique_journals=stats["unique_journals"],
                most_frequent_journal=stats["most_frequent_journal"],
            )

        # í˜‘ì—… ì—£ì§€ ì¶”ê°€
        edges_added = 0
        collaboration_weights = []

        for author1, collaborations in self.collaboration_matrix.items():
            for author2, count in collaborations.items():
                if count >= self.min_collaborations:
                    if G.has_node(author1) and G.has_node(author2):
                        # ê³µë™ ë…¼ë¬¸ ì œëª©ë“¤ ìˆ˜ì§‘
                        common_papers = []
                        for paper_info in self.author_papers[author1]:
                            if author2 in paper_info["co_authors"]:
                                common_papers.append(
                                    {
                                        "paper_id": paper_info["paper_id"],
                                        "title": paper_info["title"],
                                        "year": paper_info["year"],
                                    }
                                )

                        G.add_edge(
                            author1,
                            author2,
                            edge_type="collaboration",
                            collaboration_count=count,
                            weight=count,  # ì‹œê°í™”ìš©
                            common_papers=common_papers[:5],
                        )  # ì²˜ìŒ 5ê°œë§Œ ì €ì¥

                        edges_added += 1
                        collaboration_weights.append(count)

        print(f"âœ… Author collaboration graph constructed:")
        print(f"   ğŸ‘¥ Nodes (authors): {G.number_of_nodes()}")
        print(f"   ğŸ¤ Edges (collaborations): {G.number_of_edges()}")
        print(f"   ğŸ“ˆ Graph density: {nx.density(G):.4f}")

        if collaboration_weights:
            print(f"   ğŸ¯ Collaboration count stats:")
            print(f"      Min: {min(collaboration_weights)}")
            print(f"      Max: {max(collaboration_weights)}")
            print(f"      Mean: {np.mean(collaboration_weights):.1f}")
            print(f"      Median: {np.median(collaboration_weights):.1f}")

        return G

    def analyze_graph_properties(self, G):
        """ê·¸ë˜í”„ ì†ì„± ë¶„ì„"""
        print("ğŸ“ˆ Analyzing author collaboration graph properties...")

        # ê¸°ë³¸ í†µê³„
        stats = {
            "basic_stats": {
                "num_nodes": G.number_of_nodes(),
                "num_edges": G.number_of_edges(),
                "density": float(nx.density(G)),
                "is_connected": nx.is_connected(G),
            }
        }

        # ì—°ê²° ì„±ë¶„ ë¶„ì„
        if not nx.is_connected(G):
            components = list(nx.connected_components(G))
            stats["connectivity"] = {
                "num_components": len(components),
                "largest_component_size": len(max(components, key=len)),
                "component_sizes": [
                    len(comp) for comp in components[:10]
                ],  # ìƒìœ„ 10ê°œë§Œ
            }

        # ì¤‘ì‹¬ì„± ë¶„ì„
        if G.number_of_nodes() > 0:
            degree_centrality = nx.degree_centrality(G)

            # ë² íŠ¸ìœ„ë„ˆìŠ¤ ì¤‘ì‹¬ì„± (í° ê·¸ë˜í”„ì˜ ê²½ìš° ìƒ˜í”Œë§)
            if G.number_of_nodes() < 1000:
                betweenness_centrality = nx.betweenness_centrality(G)
            else:
                betweenness_centrality = nx.betweenness_centrality(
                    G, k=min(500, G.number_of_nodes())
                )

            stats["centrality"] = {
                "top_degree": [
                    (k, float(v))
                    for k, v in sorted(
                        degree_centrality.items(), key=lambda x: x[1], reverse=True
                    )[:10]
                ],
                "top_betweenness": [
                    (k, float(v))
                    for k, v in sorted(
                        betweenness_centrality.items(), key=lambda x: x[1], reverse=True
                    )[:10]
                ],
            }

            # ì‹¤ì œ í˜‘ì—… ìˆ˜ë¡œë„ ì •ë ¬
            degrees = dict(G.degree())

            stats["degree_stats"] = {
                "top_collaborators_by_count": sorted(
                    degrees.items(), key=lambda x: x[1], reverse=True
                )[:10],
                "avg_degree": float(np.mean(list(degrees.values()))),
                "max_degree": max(degrees.values()),
                "min_degree": min(degrees.values()),
            }

        # í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        if G.number_of_nodes() > 0:
            clustering_coeffs = nx.clustering(G)
            stats["clustering"] = {
                "average_clustering": float(nx.average_clustering(G)),
                "global_clustering": float(nx.transitivity(G)),
                "top_clustered_authors": [
                    (k, float(v))
                    for k, v in sorted(
                        clustering_coeffs.items(), key=lambda x: x[1], reverse=True
                    )[:10]
                ],
            }

        # í˜‘ì—… íŒ¨í„´ ë¶„ì„
        collaboration_analysis = self.analyze_collaboration_patterns(G)
        stats["collaboration_patterns"] = collaboration_analysis

        return stats

    def analyze_collaboration_patterns(self, G):
        """í˜‘ì—… íŒ¨í„´ ë¶„ì„"""
        print("ğŸ” Analyzing collaboration patterns...")

        # ë…¼ë¬¸ ìˆ˜ë³„ ì €ì ë¶„í¬
        paper_counts = [G.nodes[author]["paper_count"] for author in G.nodes()]

        # í˜‘ì—… ê°•ë„ ë¶„í¬
        collaboration_strengths = []
        for edge in G.edges():
            collaboration_strengths.append(G.edges[edge]["collaboration_count"])

        # í™œë™ ê¸°ê°„ ë¶„ì„
        active_years = [
            G.nodes[author]["active_years"]
            for author in G.nodes()
            if G.nodes[author]["active_years"] > 0
        ]

        # ì €ë„ ë‹¤ì–‘ì„± ë¶„ì„
        journal_diversity = [G.nodes[author]["unique_journals"] for author in G.nodes()]

        patterns = {
            "paper_count_distribution": {
                "mean": float(np.mean(paper_counts)),
                "median": float(np.median(paper_counts)),
                "max": int(max(paper_counts)),
                "min": int(min(paper_counts)),
            },
            "collaboration_strength_distribution": {
                "mean": (
                    float(np.mean(collaboration_strengths))
                    if collaboration_strengths
                    else 0
                ),
                "median": (
                    float(np.median(collaboration_strengths))
                    if collaboration_strengths
                    else 0
                ),
                "max": (
                    int(max(collaboration_strengths)) if collaboration_strengths else 0
                ),
                "min": (
                    int(min(collaboration_strengths)) if collaboration_strengths else 0
                ),
            },
            "activity_period_distribution": {
                "mean": float(np.mean(active_years)) if active_years else 0,
                "median": float(np.median(active_years)) if active_years else 0,
                "max": int(max(active_years)) if active_years else 0,
            },
            "journal_diversity_distribution": {
                "mean": float(np.mean(journal_diversity)),
                "median": float(np.median(journal_diversity)),
                "max": int(max(journal_diversity)),
            },
        }

        return patterns

    def create_author_info_map(self, G):
        """ì €ì ì •ë³´ë¥¼ ì‰½ê²Œ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ë§µ ìƒì„±"""
        author_info = {}

        for author in G.nodes():
            node_data = G.nodes[author]
            author_info[author] = {
                "name": author,
                "paper_count": node_data.get("paper_count", 0),
                "collaborator_count": node_data.get("collaborator_count", 0),
                "degree": G.degree(author),  # ì‹¤ì œ ê·¸ë˜í”„ì—ì„œì˜ ì—°ê²° ìˆ˜
                "first_year": node_data.get("first_year", ""),
                "last_year": node_data.get("last_year", ""),
                "active_years": node_data.get("active_years", 0),
                "unique_journals": node_data.get("unique_journals", 0),
                "most_frequent_journal": node_data.get("most_frequent_journal", ""),
            }

        return author_info

    def save_collaboration_graph_and_analysis(self, G, stats, output_dir):
        """Author collaboration ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(output_dir)

        # 1. NetworkX ê·¸ë˜í”„ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (GraphRAGìš©)
        graph_data = {"nodes": [], "edges": []}

        # ë…¸ë“œ ì •ë³´
        for node in G.nodes():
            node_data = G.nodes[node].copy()
            node_data["id"] = node
            graph_data["nodes"].append(node_data)

        # ì—£ì§€ ì •ë³´
        for edge in G.edges():
            edge_data = G.edges[edge].copy()
            edge_data["source"] = edge[0]
            edge_data["target"] = edge[1]
            # common_papers ë¦¬ìŠ¤íŠ¸ ìœ ì§€ (JSON ì§ë ¬í™” ê°€ëŠ¥)
            graph_data["edges"].append(edge_data)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        graph_file = output_dir / "author_collaboration_graph.json"
        with open(graph_file, "w", encoding="utf-8") as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)

        # 2. GraphML íŒŒì¼ë¡œ ì €ì¥
        try:
            # GraphML í˜¸í™˜ì„ ìœ„í•´ ê·¸ë˜í”„ ë³µì‚¬ ë° ì†ì„± ë³€í™˜
            G_graphml = G.copy()

            # ì—£ì§€ì˜ common_papers ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            for edge in G_graphml.edges():
                if "common_papers" in G_graphml.edges[edge]:
                    common_papers = G_graphml.edges[edge]["common_papers"]
                    if isinstance(common_papers, list):
                        # ë…¼ë¬¸ ì •ë³´ë¥¼ ê°„ë‹¨í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
                        paper_titles = [p.get("title", "")[:50] for p in common_papers]
                        G_graphml.edges[edge]["common_papers"] = ";".join(paper_titles)

                # ê¸°íƒ€ ë³µì¡í•œ íƒ€ì…ë“¤ ë³€í™˜
                for attr_name, attr_value in G_graphml.edges[edge].items():
                    if (
                        isinstance(attr_value, (list, dict))
                        and attr_name != "common_papers"
                    ):
                        if isinstance(attr_value, dict):
                            G_graphml.edges[edge][attr_name] = json.dumps(attr_value)
                        else:
                            G_graphml.edges[edge][attr_name] = ";".join(
                                str(v) for v in attr_value
                            )
                    elif not isinstance(attr_value, (str, int, float, bool)):
                        G_graphml.edges[edge][attr_name] = str(attr_value)

            graphml_file = output_dir / "author_collaboration_graph.graphml"
            nx.write_graphml(G_graphml, graphml_file)

        except Exception as e:
            print(f"âš ï¸  GraphML ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“„ JSON íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}")
            graphml_file = None

        # 3. ë¶„ì„ ê²°ê³¼ ì €ì¥
        stats_file = output_dir / "author_collaboration_analysis.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        # 4. ì €ì ì •ë³´ í…Œì´ë¸” ì €ì¥
        author_info = self.create_author_info_map(G)
        author_info_file = output_dir / "author_collaboration_info.json"
        with open(author_info_file, "w", encoding="utf-8") as f:
            json.dump(author_info, f, ensure_ascii=False, indent=2)

        # 5. CSV í˜•íƒœì˜ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        edge_list = []
        for edge in G.edges():
            edge_info = G.edges[edge].copy()
            edge_info.update(
                {
                    "author1": edge[0],
                    "author2": edge[1],
                    "author1_papers": G.nodes[edge[0]].get("paper_count", 0),
                    "author2_papers": G.nodes[edge[1]].get("paper_count", 0),
                    "author1_first_year": G.nodes[edge[0]].get("first_year", ""),
                    "author2_first_year": G.nodes[edge[1]].get("first_year", ""),
                }
            )
            # common_papers í•„ë“œ ê°„ì†Œí™”
            if "common_papers" in edge_info:
                edge_info["common_papers_count"] = len(edge_info["common_papers"])
                edge_info.pop("common_papers")  # CSVì—ì„œëŠ” ì œì™¸
            edge_list.append(edge_info)

        edge_df = pd.DataFrame(edge_list)
        edge_file = output_dir / "author_collaboration_edges.csv"
        edge_df.to_csv(edge_file, index=False, encoding="utf-8")

        print(f"ğŸ’¾ Author collaboration graph results saved:")
        print(f"   ğŸ”— Graph (JSON): {graph_file}")
        if graphml_file:
            print(f"   ğŸ”— Graph (GraphML): {graphml_file}")
        print(f"   ğŸ“Š Analysis: {stats_file}")
        print(f"   ğŸ‘¥ Author Info: {author_info_file}")
        print(f"   ğŸ“ˆ Edge List: {edge_file}")

        return graph_file

    def process_papers(self, papers_metadata):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ Starting author collaboration graph construction...")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.papers_metadata = papers_metadata

        # 1. ì €ì í˜‘ì—… ê´€ê³„ ì¶”ì¶œ
        self.extract_author_collaborations(papers_metadata)

        # 2. ì €ì í†µê³„ ê³„ì‚°
        self.calculate_author_statistics()

        # 3. í™œë™ ìˆ˜ì¤€ ê¸°ì¤€ í•„í„°ë§
        active_authors = self.filter_authors_by_activity()

        # 4. í˜‘ì—… ê·¸ë˜í”„ êµ¬ì¶•
        G = self.build_collaboration_graph()

        # 5. ê·¸ë˜í”„ ë¶„ì„
        stats = self.analyze_graph_properties(G)

        return G, stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import RAW_EXTRACTIONS_DIR, GRAPHS_DIR

    # í†µí•© ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_file = RAW_EXTRACTIONS_DIR / "integrated_papers_metadata.json"

    if not metadata_file.exists():
        print(f"âŒ Metadata not found: {metadata_file}")
        print("Please run data processing pipeline first.")
        return

    with open(metadata_file, "r", encoding="utf-8") as f:
        papers_metadata = json.load(f)

    print(f"ğŸ“„ Loaded {len(papers_metadata)} papers metadata")

    # Author Collaboration Graph Builder ì´ˆê¸°í™”
    builder = AuthorCollaborationGraphBuilder(
        min_collaborations=1,  # ìµœì†Œ 1ë²ˆ ì´ìƒ í˜‘ì—…
        min_papers=1,  # ìµœì†Œ 1í¸ ì´ìƒ ë…¼ë¬¸ (ëª¨ë“  ì €ì í¬í•¨)
    )

    # ì „ì²´ ì²˜ë¦¬
    G, stats = builder.process_papers(papers_metadata)

    # ê²°ê³¼ ì €ì¥
    output_file = builder.save_collaboration_graph_and_analysis(G, stats, GRAPHS_DIR)

    print(f"\nâœ… Author collaboration graph construction completed!")
    print(f"ğŸ“ Main output: {output_file}")

    # ìš”ì•½ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š Final Summary:")
    print(f"   ğŸ‘¥ Total authors: {G.number_of_nodes()}")
    print(f"   ğŸ¤ Collaboration relationships: {G.number_of_edges()}")
    print(f"   ğŸ“ˆ Graph density: {nx.density(G):.4f}")

    if stats.get("degree_stats"):
        top_collaborators = stats["degree_stats"]["top_collaborators_by_count"][:3]
        print(f"   ğŸ† Top 3 most collaborative authors:")
        for i, (author, collab_count) in enumerate(top_collaborators):
            paper_count = G.nodes[author].get("paper_count", 0)
            print(
                f"      {i+1}. {author} ({collab_count} collaborations, {paper_count} papers)"
            )

    return G, output_file


if __name__ == "__main__":
    main()
