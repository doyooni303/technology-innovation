"""
ì˜ë¯¸ì  ìœ ì‚¬ë„ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Semantic Similarity Graph Construction Module

ê¸°ì¡´ semantic_similarity_extractor.pyì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼
NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from tqdm import tqdm


class SemanticSimilarityGraphBuilder:
    """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, min_similarity=0.7):
        """
        Args:
            min_similarity (float): ê·¸ë˜í”„ì— í¬í•¨í•  ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        self.min_similarity = min_similarity
        self.similarity_data = None
        self.papers_metadata = None

    # âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
    def load_similarity_data(self, similarity_file, metadata_file):
        """ê¸°ì¡´ì— ì¶”ì¶œëœ similarity ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ Loading semantic similarity data and metadata...")

        # âœ… Semantic similarity JSON ë°ì´í„° ë¡œë“œ
        with open(similarity_file, "r", encoding="utf-8") as f:
            self.similarity_data = json.load(f)

        # Papers ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_file, "r", encoding="utf-8") as f:
            self.papers_metadata = json.load(f)

        print(f"   ğŸ“„ Similarity data loaded: {len(self.similarity_data)} papers")
        print(f"   ğŸ“š Metadata loaded: {len(self.papers_metadata)} papers")

        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        if self.similarity_data:
            sample_paper = next(iter(self.similarity_data.values()))
            if sample_paper:
                sample_connection = sample_paper[0]
                required_fields = ["target_paper", "similarity"]
                missing_fields = [
                    field for field in required_fields if field not in sample_connection
                ]
                if missing_fields:
                    print(
                        f"âš ï¸  Warning: Missing fields in similarity data: {missing_fields}"
                    )
                else:
                    print("âœ… Similarity data structure validated")

    def analyze_similarity_data(self):
        """Similarity ë°ì´í„° ë¶„ì„"""
        print("ğŸ“Š Analyzing semantic similarity data...")

        total_connections = sum(
            len(connections) for connections in self.similarity_data.values()
        )
        papers_with_connections = sum(
            1 for connections in self.similarity_data.values() if connections
        )

        all_similarities = []
        for connections in self.similarity_data.values():
            for conn in connections:
                all_similarities.append(conn["similarity"])

        print(f"   ğŸ”— Total similarity connections: {total_connections}")
        print(f"   ğŸ“„ Papers with outgoing connections: {papers_with_connections}")
        print(
            f"   ğŸ“ˆ Avg connections per paper: {total_connections/len(self.similarity_data):.1f}"
        )

        if all_similarities:
            print(f"   ğŸ“Š Similarity distribution:")
            print(f"      Min: {min(all_similarities):.4f}")
            print(f"      Max: {max(all_similarities):.4f}")
            print(f"      Mean: {np.mean(all_similarities):.4f}")
            print(f"      Median: {np.median(all_similarities):.4f}")

    def build_similarity_graph(self):
        """Similarity ë°ì´í„°ë¡œë¶€í„° NetworkX ë°©í–¥ ê·¸ë˜í”„ êµ¬ì¶•"""
        print(f"ğŸ”— Building directed semantic similarity graph...")

        # âœ… ë°©í–¥ ê·¸ë˜í”„ ìƒì„± (ê° ë…¼ë¬¸ì˜ "ì„ í˜¸" ê´€ê³„ë¥¼ í‘œí˜„)
        G = nx.DiGraph()  # â† DiGraph ì‚¬ìš©

        # ë…¼ë¬¸ ID to ë©”íƒ€ë°ì´í„° ë§¤í•‘ ìƒì„±
        paper_metadata_map = {}
        for i, paper in enumerate(self.papers_metadata):
            paper_id = f"paper_{i}"
            paper_metadata_map[paper_id] = paper

        # ëª¨ë“  ë…¼ë¬¸ì„ ë…¸ë“œë¡œ ì¶”ê°€
        for paper_id, metadata in paper_metadata_map.items():

            G.add_node(
                paper_id,
                node_type="paper",
                title=metadata.get("title", ""),
                abstract=metadata.get("abstract", ""),  # âœ… Abstract ì¶”ì¶œ
                authors=metadata.get("authors", []),
                year=metadata.get("year", ""),
                journal=metadata.get("journal", ""),
                keywords=metadata.get("keywords", []),
                has_pdf=metadata.get("has_pdf", False),
                has_abstract=metadata.get("has_abstract", False),
                abstract_length=len(metadata.get("abstract", "")),  # âœ… Abstract ê¸¸ì´
            )

        # âœ… ë°©í–¥ì„± ìˆëŠ” ìœ ì‚¬ë„ ì—£ì§€ ì¶”ê°€
        edges_added = 0
        similarity_weights = []

        for source_paper, connections in self.similarity_data.items():
            if not connections:
                continue

            for connection in connections:
                target_paper = connection["target_paper"]
                similarity = connection["similarity"]

                # ìê¸° ìì‹ ê³¼ì˜ ì—°ê²° ì œì™¸
                if source_paper != target_paper:
                    # ì–‘ìª½ ë…¼ë¬¸ì´ ëª¨ë‘ ê·¸ë˜í”„ì— ìˆëŠ” ê²½ìš°ë§Œ ì—£ì§€ ì¶”ê°€
                    if G.has_node(source_paper) and G.has_node(target_paper):
                        # âœ… ë°©í–¥ì„± ìˆëŠ” ì—£ì§€ (source â†’ target)
                        G.add_edge(
                            source_paper,
                            target_paper,
                            edge_type="semantic_similarity",
                            similarity=float(similarity),
                            weight=float(similarity),
                            rank=len(similarity_weights) + 1,
                        )  # ìˆœìœ„ ì •ë³´ ì¶”ê°€

                        edges_added += 1
                        similarity_weights.append(similarity)

        print(f"âœ… Directed semantic similarity graph constructed:")
        print(f"   ğŸ“„ Nodes (papers): {G.number_of_nodes()}")
        print(f"   ğŸ”— Directed edges (similarities): {G.number_of_edges()}")
        print(f"   ğŸ“ˆ Graph density: {nx.density(G):.6f}")
        print(
            f"   ğŸ¯ Average out-degree: {G.number_of_edges()/G.number_of_nodes():.1f}"
        )

        return G

    def analyze_graph_properties(self, G):
        """ë°©í–¥ ê·¸ë˜í”„ ì†ì„± ë¶„ì„"""
        print("ğŸ“ˆ Analyzing directed semantic similarity graph properties...")

        # ê¸°ë³¸ í†µê³„
        stats = {
            "basic_stats": {
                "num_nodes": G.number_of_nodes(),
                "num_edges": G.number_of_edges(),
                "density": float(nx.density(G)),
                "is_weakly_connected": nx.is_weakly_connected(G),
                "is_strongly_connected": nx.is_strongly_connected(G),
            }
        }

        # ë°©í–¥ ê·¸ë˜í”„ ì—°ê²°ì„± ë¶„ì„
        if not nx.is_strongly_connected(G):
            weak_components = list(nx.weakly_connected_components(G))
            strong_components = list(nx.strongly_connected_components(G))

            stats["connectivity"] = {
                "num_weak_components": len(weak_components),
                "num_strong_components": len(strong_components),
                "largest_weak_component": len(max(weak_components, key=len)),
                "largest_strong_component": len(max(strong_components, key=len)),
            }

        # âœ… ë°©í–¥ ê·¸ë˜í”„ ì¤‘ì‹¬ì„± ë¶„ì„ (ìˆ˜ì •ëœ ìš©ì–´)
        if G.number_of_nodes() > 0:
            in_degree_centrality = nx.in_degree_centrality(G)  # ë§ì´ ì„ íƒë°›ëŠ” ë…¼ë¬¸
            out_degree_centrality = nx.out_degree_centrality(G)  # ë§ì´ ì„ íƒí•˜ëŠ” ë…¼ë¬¸

            # ê°€ì¥ "ì¸ê¸° ìˆëŠ”" ë…¼ë¬¸ë“¤ (ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì´ ìœ ì‚¬í•˜ë‹¤ê³  ì„ íƒ)
            most_similar_to = sorted(
                in_degree_centrality.items(), key=lambda x: x[1], reverse=True
            )[:10]

            # ê°€ì¥ "í¬ê´„ì ì¸" ë…¼ë¬¸ë“¤ (ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ê³¼ ìœ ì‚¬ì ì„ ë§ì´ ì°¾ìŒ)
            most_similar = sorted(
                out_degree_centrality.items(), key=lambda x: x[1], reverse=True
            )[:10]

            stats["centrality"] = {
                "most_similar_to_papers": [
                    (k, float(v)) for k, v in most_similar_to
                ],  # âœ… ìˆ˜ì •
                "most_similar_papers": [
                    (k, float(v)) for k, v in most_similar
                ],  # âœ… ìˆ˜ì •
            }

            # ì‹¤ì œ ì—°ê²° ìˆ˜ë¡œë„ ì •ë ¬
            in_degrees = dict(G.in_degree())
            out_degrees = dict(G.out_degree())

            stats["degree_stats"] = {
                "top_similar_to_by_count": sorted(
                    in_degrees.items(), key=lambda x: x[1], reverse=True  # âœ… ìˆ˜ì •
                )[:10],
                "top_similar_by_count": sorted(
                    out_degrees.items(), key=lambda x: x[1], reverse=True  # âœ… ìˆ˜ì •
                )[:10],
                "avg_in_degree": float(np.mean(list(in_degrees.values()))),
                "avg_out_degree": float(np.mean(list(out_degrees.values()))),
                "max_in_degree": max(in_degrees.values()),
                "max_out_degree": max(out_degrees.values()),
            }

        return stats

    def analyze_similarity_distribution(self, G):
        """ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„"""
        similarities = []
        for edge in G.edges():
            similarity = G.edges[edge]["similarity"]
            similarities.append(similarity)

        if not similarities:
            return {}

        distribution_stats = {
            "mean": float(np.mean(similarities)),
            "median": float(np.median(similarities)),
            "std": float(np.std(similarities)),
            "min": float(min(similarities)),
            "max": float(max(similarities)),
            "percentiles": {
                "25th": float(np.percentile(similarities, 25)),
                "75th": float(np.percentile(similarities, 75)),
                "90th": float(np.percentile(similarities, 90)),
                "95th": float(np.percentile(similarities, 95)),
            },
        }

        return distribution_stats

    def create_paper_info_map(self, G):
        """ë…¼ë¬¸ ì •ë³´ë¥¼ ì‰½ê²Œ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ë§µ ìƒì„±"""
        paper_info = {}

        for node in G.nodes():
            node_data = G.nodes[node]
            paper_info[node] = {
                "title": node_data.get("title", ""),
                "abstract": node_data.get("abstract", ""),
                "authors": ", ".join(node_data.get("authors", [])),
                "year": node_data.get("year", ""),
                "journal": node_data.get("journal", ""),
                "keywords": ", ".join(node_data.get("keywords", [])),
                "degree": G.degree(node),  # ì—°ê²°ëœ ë…¼ë¬¸ ìˆ˜
                "has_pdf": node_data.get("has_pdf", False),
                "has_abstract": node_data.get("has_abstract", False),
                "abstract_length": node_data.get(
                    "abstract_length", 0
                ),  # âœ… Abstract ê¸¸ì´
            }

        return paper_info

    def save_similarity_graph_and_analysis(self, G, stats, output_dir):
        """Similarity ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ ì €ì¥ (XML í˜¸í™˜ì„± ê°œì„ )"""
        output_dir = Path(output_dir)

        # 1. NetworkX ê·¸ë˜í”„ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (GraphRAGìš©)
        graph_data = {"nodes": [], "edges": []}

        # ë…¸ë“œ ì •ë³´
        for node in G.nodes():
            node_data = G.nodes[node].copy()
            node_data["id"] = node
            # ë¦¬ìŠ¤íŠ¸ íƒ€ì… í•„ë“œë“¤ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
            if "authors" in node_data and isinstance(node_data["authors"], list):
                node_data["authors"] = list(node_data["authors"])
            if "keywords" in node_data and isinstance(node_data["keywords"], list):
                node_data["keywords"] = list(node_data["keywords"])
            graph_data["nodes"].append(node_data)

        # ì—£ì§€ ì •ë³´
        for edge in G.edges():
            edge_data = G.edges[edge].copy()
            edge_data["source"] = edge[0]
            edge_data["target"] = edge[1]
            # numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            for key, value in edge_data.items():
                if isinstance(value, np.floating):
                    edge_data[key] = float(value)
            graph_data["edges"].append(edge_data)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        graph_file = output_dir / "semantic_similarity_network_graph.json"
        with open(graph_file, "w", encoding="utf-8") as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)

        # 2. âœ… GraphML íŒŒì¼ë¡œ ì €ì¥ (XML í˜¸í™˜ì„± ê°œì„ )
        try:
            G_graphml = G.copy()

            # âœ… XML í˜¸í™˜ ë¬¸ìì—´ ì •ì œ í•¨ìˆ˜
            def clean_xml_string(text):
                if not isinstance(text, str):
                    text = str(text)

                # NULL ë°”ì´íŠ¸ ë° ì œì–´ ë¬¸ì ì œê±°
                import re

                text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", "", text)

                # XML íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
                text = text.replace("&", "&amp;")
                text = text.replace("<", "&lt;")
                text = text.replace(">", "&gt;")
                text = text.replace('"', "&quot;")
                text = text.replace("'", "&apos;")

                return text

            # ë…¸ë“œ ì†ì„±ì„ GraphML í˜¸í™˜ í˜•íƒœë¡œ ë³€í™˜
            for node in G_graphml.nodes():
                # ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì†ì„±ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                if "authors" in G_graphml.nodes[node]:
                    authors_list = G_graphml.nodes[node]["authors"]
                    if isinstance(authors_list, (list, set)):
                        cleaned_authors = [
                            clean_xml_string(str(a)) for a in authors_list
                        ]
                        G_graphml.nodes[node]["authors"] = ";".join(cleaned_authors)

                if "keywords" in G_graphml.nodes[node]:
                    keywords_list = G_graphml.nodes[node]["keywords"]
                    if isinstance(keywords_list, (list, set)):
                        cleaned_keywords = [
                            clean_xml_string(str(k)) for k in keywords_list
                        ]
                        G_graphml.nodes[node]["keywords"] = ";".join(cleaned_keywords)

                # âœ… Abstractì™€ ê¸°íƒ€ ë¬¸ìì—´ í•„ë“œë“¤ ì •ì œ
                for attr_name, attr_value in G_graphml.nodes[node].items():
                    if isinstance(attr_value, str):
                        # Abstract, title ë“± ë¬¸ìì—´ í•„ë“œë“¤ ì •ì œ
                        G_graphml.nodes[node][attr_name] = clean_xml_string(attr_value)
                    elif isinstance(attr_value, (list, set, dict)):
                        if isinstance(attr_value, dict):
                            G_graphml.nodes[node][attr_name] = clean_xml_string(
                                json.dumps(attr_value)
                            )
                        else:
                            cleaned_list = [
                                clean_xml_string(str(v)) for v in attr_value
                            ]
                            G_graphml.nodes[node][attr_name] = ";".join(cleaned_list)
                    elif not isinstance(attr_value, (int, float, bool)):
                        G_graphml.nodes[node][attr_name] = clean_xml_string(
                            str(attr_value)
                        )

            # ì—£ì§€ ì†ì„±ë„ ì •ì œ
            for edge in G_graphml.edges():
                for attr_name, attr_value in G_graphml.edges[edge].items():
                    if isinstance(attr_value, str):
                        G_graphml.edges[edge][attr_name] = clean_xml_string(attr_value)
                    elif isinstance(attr_value, (list, set, dict)):
                        if isinstance(attr_value, dict):
                            G_graphml.edges[edge][attr_name] = clean_xml_string(
                                json.dumps(attr_value)
                            )
                        else:
                            cleaned_list = [
                                clean_xml_string(str(v)) for v in attr_value
                            ]
                            G_graphml.edges[edge][attr_name] = ";".join(cleaned_list)
                    elif not isinstance(attr_value, (int, float, bool)):
                        G_graphml.edges[edge][attr_name] = clean_xml_string(
                            str(attr_value)
                        )

            graphml_file = output_dir / "semantic_similarity_network_graph.graphml"
            nx.write_graphml(
                G_graphml, graphml_file, encoding="utf-8", prettyprint=True
            )
            print(f"   ğŸ”— Graph (GraphML): {graphml_file}")

        except Exception as e:
            print(f"âš ï¸  GraphML ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“„ JSON íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}")
            graphml_file = None

        # 3. ë¶„ì„ ê²°ê³¼ ì €ì¥
        stats_file = output_dir / "semantic_similarity_network_analysis.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        # 4. ë…¼ë¬¸ ì •ë³´ í…Œì´ë¸” ì €ì¥
        paper_info = self.create_paper_info_map(G)
        paper_info_file = output_dir / "semantic_similarity_papers_info.json"
        with open(paper_info_file, "w", encoding="utf-8") as f:
            json.dump(paper_info, f, ensure_ascii=False, indent=2)

        # 5. CSV í˜•íƒœì˜ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        edge_list = []
        for edge in G.edges():
            edge_info = G.edges[edge].copy()
            edge_info.update(
                {
                    "source_paper": edge[0],
                    "target_paper": edge[1],
                    "source_title": G.nodes[edge[0]].get("title", ""),
                    "target_title": G.nodes[edge[1]].get("title", ""),
                    "source_year": G.nodes[edge[0]].get("year", ""),
                    "target_year": G.nodes[edge[1]].get("year", ""),
                    # âœ… Abstract ì •ë³´ ì¶”ê°€
                    "source_has_abstract": G.nodes[edge[0]].get("has_abstract", False),
                    "target_has_abstract": G.nodes[edge[1]].get("has_abstract", False),
                    "source_abstract_length": G.nodes[edge[0]].get(
                        "abstract_length", 0
                    ),
                    "target_abstract_length": G.nodes[edge[1]].get(
                        "abstract_length", 0
                    ),
                }
            )
            # numpy íƒ€ì… ë³€í™˜
            for key, value in edge_info.items():
                if isinstance(value, np.floating):
                    edge_info[key] = float(value)
            edge_list.append(edge_info)

        edge_df = pd.DataFrame(edge_list)
        edge_file = output_dir / "semantic_similarity_network_edges.csv"
        edge_df.to_csv(edge_file, index=False, encoding="utf-8")

        print(f"ğŸ’¾ Semantic similarity graph results saved:")
        print(f"   ğŸ”— Graph (JSON): {graph_file}")
        print(f"   ğŸ“Š Analysis: {stats_file}")
        print(f"   ğŸ“„ Paper Info: {paper_info_file}")
        print(f"   ğŸ“ˆ Edge List: {edge_file}")

        return graph_file

    def process_similarity_data(self, similarity_file, metadata_file):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ Starting semantic similarity graph construction...")

        # 1. ë°ì´í„° ë¡œë“œ
        self.load_similarity_data(similarity_file, metadata_file)

        # 2. ë°ì´í„° ë¶„ì„
        self.analyze_similarity_data()

        # 3. ê·¸ë˜í”„ êµ¬ì¶•
        G = self.build_similarity_graph()

        # 4. ê·¸ë˜í”„ ë¶„ì„
        stats = self.analyze_graph_properties(G)

        return G, stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import RAW_EXTRACTIONS_DIR, GRAPHS_DIR  # âœ… ë³€ê²½

    # âœ… ì…ë ¥ íŒŒì¼ë“¤ (raw_extractionsì—ì„œ)
    similarity_file = RAW_EXTRACTIONS_DIR / "semantic_similarity_graph.json"
    metadata_file = RAW_EXTRACTIONS_DIR / "integrated_papers_metadata.json"

    if not similarity_file.exists():
        print(f"âŒ Semantic similarity data not found: {similarity_file}")
        print("Please run semantic_similarity_extractor.py first.")
        return

    if not metadata_file.exists():
        print(f"âŒ Metadata not found: {metadata_file}")
        print("Please run data processing pipeline first.")
        return

    print(f"ğŸ“‚ Similarity file: {similarity_file}")
    print(f"ğŸ“‚ Metadata file: {metadata_file}")

    # Semantic Similarity Graph Builder ì´ˆê¸°í™”
    builder = SemanticSimilarityGraphBuilder(min_similarity=0.7)

    # ì „ì²´ ì²˜ë¦¬
    G, stats = builder.process_similarity_data(similarity_file, metadata_file)

    # âœ… ê²°ê³¼ ì €ì¥ (graphs í´ë”ì—)
    output_file = builder.save_similarity_graph_and_analysis(G, stats, GRAPHS_DIR)

    print(f"\nâœ… Semantic similarity graph construction completed!")
    print(f"ğŸ“ Main output: {output_file}")

    return G, output_file


if __name__ == "__main__":
    main()
