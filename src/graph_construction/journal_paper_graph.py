"""
ì €ë„-ë…¼ë¬¸ ì´ë¶„ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Journal-Paper Bipartite Graph Construction Module
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from tqdm import tqdm
import re


class JournalPaperGraphBuilder:
    """ì €ë„-ë…¼ë¬¸ ì´ë¶„ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, min_papers_per_journal=1):
        """
        Args:
            min_papers_per_journal (int): ê·¸ë˜í”„ì— í¬í•¨í•  ì €ë„ì˜ ìµœì†Œ ë…¼ë¬¸ ìˆ˜
        """
        self.min_papers_per_journal = min_papers_per_journal

        # ê²°ê³¼ ì €ì¥ìš©
        self.journal_papers = defaultdict(list)  # ì €ë„ë³„ ë…¼ë¬¸ ëª©ë¡
        self.paper_journals = {}  # ë…¼ë¬¸ë³„ ì €ë„ ì •ë³´
        self.journal_stats = {}
        self.papers_metadata = None

        # ì €ë„ëª… ì•½ì–´ -> Full name ë§¤í•‘
        self.journal_abbreviations = {
            # IEEE ì €ë„ë“¤
            r"IEEE Trans\.?\s*Power\s*Syst\.?": "IEEE Transactions on Power Systems",
            r"IEEE Trans\.?\s*Ind\.?\s*Electron\.?": "IEEE Transactions on Industrial Electronics",
            r"IEEE Trans\.?\s*Energy\s*Convers\.?": "IEEE Transactions on Energy Conversion",
            r"IEEE Trans\.?\s*Smart\s*Grid": "IEEE Transactions on Smart Grid",
            r"IEEE Trans\.?\s*Veh\.?\s*Technol\.?": "IEEE Transactions on Vehicular Technology",
            r"IEEE Trans\.?\s*Neural\s*Netw\.?": "IEEE Transactions on Neural Networks",
            r"IEEE Trans\.?\s*Pattern\s*Anal\.?": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            r"IEEE Trans\.?\s*Autom\.?\s*Sci\.?\s*Eng\.?": "IEEE Transactions on Automation Science and Engineering",
            # ê¸°íƒ€ ì£¼ìš” ì €ë„ë“¤
            r"J\.?\s*Power\s*Sources": "Journal of Power Sources",
            r"Appl\.?\s*Energy": "Applied Energy",
            r"Renew\.?\s*Sustain\.?\s*Energy\s*Rev\.?": "Renewable and Sustainable Energy Reviews",
            r"Energy\s*Build\.?": "Energy and Buildings",
            r"Int\.?\s*J\.?\s*Electr\.?\s*Power": "International Journal of Electrical Power & Energy Systems",
            # Nature, Science ê³„ì—´
            r"Nat\.?\s*Energy": "Nature Energy",
            r"Nat\.?\s*Commun\.?": "Nature Communications",
            r"Sci\.?\s*Rep\.?": "Scientific Reports",
            # Elsevier ì €ë„ë“¤
            r"Expert\s*Syst\.?\s*Appl\.?": "Expert Systems with Applications",
            r"Neurocomputing": "Neurocomputing",
            r"Comput\.?\s*Chem\.?\s*Eng\.?": "Computers & Chemical Engineering",
        }

    def normalize_journal_name(self, journal_name):
        """ì €ë„ëª… ì •ê·œí™” (ì•½ì–´ â†’ Full name)"""
        if not journal_name or not isinstance(journal_name, str):
            return None

        # ê¸°ë³¸ ì •ì œ
        journal_name = journal_name.strip()

        # ë„ˆë¬´ ì§§ì€ ì´ë¦„ ì œì™¸
        if len(journal_name) < 3:
            return None

        # ì•½ì–´ë¥¼ Full nameìœ¼ë¡œ ë³€í™˜
        normalized = journal_name
        for abbrev_pattern, full_name in self.journal_abbreviations.items():
            if re.search(abbrev_pattern, journal_name, re.IGNORECASE):
                normalized = full_name
                break

        # ê¸°ë³¸ì ì¸ ì •ì œ
        normalized = re.sub(r"\s+", " ", normalized).strip()

        # ì²« ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ
        if normalized:
            normalized = normalized[0].upper() + normalized[1:]

        return normalized

    def extract_journal_paper_relationships(self, papers_metadata):
        """ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì €ë„-ë…¼ë¬¸ ê´€ê³„ ì¶”ì¶œ"""
        print("ğŸ” Extracting journal-paper relationships...")

        valid_papers = 0
        papers_with_journals = 0
        unknown_journals = 0

        for i, paper in enumerate(tqdm(papers_metadata, desc="Processing papers")):
            paper_id = f"paper_{i}"
            title = paper.get("title", "")
            abstract = paper.get("abstract", "")
            journal = paper.get("journal", "")
            year = paper.get("year", "")
            authors = paper.get("authors", [])
            keywords = paper.get("keywords", [])

            # ì €ë„ ì •ë³´ ì •ì œ
            if journal:
                normalized_journal = self.normalize_journal_name(journal)
                if normalized_journal:
                    papers_with_journals += 1
                else:
                    unknown_journals += 1
                    normalized_journal = journal  # ì •ì œ ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
            else:
                unknown_journals += 1
                normalized_journal = "Unknown Journal"

            # ë…¼ë¬¸-ì €ë„ ë§¤í•‘ ì €ì¥
            self.paper_journals[paper_id] = {
                "title": title,
                "abstract": abstract,
                "journal": normalized_journal,
                "original_journal": journal,
                "year": year,
                "authors": authors,
                "keywords": keywords,
                "author_count": len(authors) if authors else 0,
                "keyword_count": len(keywords) if keywords else 0,
                "has_abstract": bool(abstract.strip()),
                "abstract_length": len(abstract),
            }

            # ì €ë„-ë…¼ë¬¸ ë§¤í•‘ ì €ì¥
            self.journal_papers[normalized_journal].append(
                {
                    "paper_id": paper_id,
                    "title": title,
                    "abstract": abstract,
                    "year": year,
                    "authors": authors,
                    "keywords": keywords,
                }
            )

            valid_papers += 1

        print(f"âœ… Journal-paper relationship extraction completed:")
        print(f"   ğŸ“„ Total papers processed: {valid_papers}")
        print(f"   ğŸ“° Papers with journals: {papers_with_journals}")
        print(f"   â“ Papers with unknown/missing journals: {unknown_journals}")
        print(f"   ğŸ“š Unique journals: {len(self.journal_papers)}")
        print(
            f"   ğŸ“ˆ Avg papers per journal: {valid_papers/len(self.journal_papers):.1f}"
        )

    def calculate_journal_statistics(self):
        """ì €ë„ë³„ í†µê³„ ê³„ì‚°"""
        print("ğŸ“Š Calculating journal statistics...")

        for journal, papers in self.journal_papers.items():
            # ê¸°ë³¸ í†µê³„
            paper_count = len(papers)
            years = [p["year"] for p in papers if p["year"]]
            authors_all = [author for p in papers for author in p.get("authors", [])]
            keywords_all = [kw for p in papers for kw in p.get("keywords", [])]

            # í™œë™ ê¸°ê°„
            if years:
                try:
                    year_ints = [int(y) for y in years if str(y).isdigit()]
                    if year_ints:
                        first_year = min(year_ints)
                        last_year = max(year_ints)
                        active_years = last_year - first_year + 1
                        # ë…„ë„ë³„ ë…¼ë¬¸ ìˆ˜
                        papers_by_year = Counter(year_ints)
                    else:
                        first_year = last_year = active_years = 0
                        papers_by_year = {}
                except:
                    first_year = last_year = active_years = 0
                    papers_by_year = {}
            else:
                first_year = last_year = active_years = 0
                papers_by_year = {}

            # ì €ì í†µê³„
            unique_authors = len(set(authors_all)) if authors_all else 0
            avg_authors_per_paper = (
                len(authors_all) / paper_count if paper_count > 0 else 0
            )

            # í‚¤ì›Œë“œ í†µê³„
            unique_keywords = len(set(keywords_all)) if keywords_all else 0
            keyword_frequency = Counter(keywords_all)
            top_keywords = keyword_frequency.most_common(10)

            # ì €ë„ íƒ€ì… ë¶„ë¥˜
            journal_type = self.classify_journal_type(journal)

            self.journal_stats[journal] = {
                "name": journal,
                "paper_count": paper_count,
                "unique_authors": unique_authors,
                "unique_keywords": unique_keywords,
                "first_year": first_year,
                "last_year": last_year,
                "active_years": active_years,
                "avg_authors_per_paper": avg_authors_per_paper,
                "journal_type": journal_type,
                "papers_by_year": dict(papers_by_year),
                "top_keywords": top_keywords,
                "papers": papers,
            }

        print(
            f"âœ… Journal statistics calculated for {len(self.journal_stats)} journals"
        )

    def classify_journal_type(self, journal_name):
        """ì €ë„ íƒ€ì… ë¶„ë¥˜"""
        name_lower = journal_name.lower()

        if "ieee" in name_lower:
            return "IEEE"
        elif any(word in name_lower for word in ["nature", "science"]):
            return "High-Impact"
        elif any(word in name_lower for word in ["energy", "power", "electric"]):
            return "Energy & Power"
        elif any(
            word in name_lower
            for word in ["neural", "machine", "artificial", "intelligence"]
        ):
            return "AI & ML"
        elif any(word in name_lower for word in ["computer", "computing", "software"]):
            return "Computer Science"
        elif any(
            word in name_lower for word in ["engineering", "industrial", "automation"]
        ):
            return "Engineering"
        elif any(
            word in name_lower for word in ["applied", "international", "journal"]
        ):
            return "Applied Science"
        else:
            return "Other"

    def filter_journals_by_activity(self):
        """í™œë™ ìˆ˜ì¤€ ê¸°ì¤€ìœ¼ë¡œ ì €ë„ í•„í„°ë§"""
        print(f"ğŸ” Filtering journals (min papers: {self.min_papers_per_journal})...")

        # ìµœì†Œ ë…¼ë¬¸ ìˆ˜ ì´ìƒì˜ ì €ë„ë§Œ ì„ íƒ
        active_journals = {
            journal
            for journal, stats in self.journal_stats.items()
            if stats["paper_count"] >= self.min_papers_per_journal
        }

        print(f"   ğŸ“Š Journals before filtering: {len(self.journal_stats)}")
        print(f"   âœ… Journals after filtering: {len(active_journals)}")

        # ì €ë„ í†µê³„ë„ í•„í„°ë§
        self.journal_stats = {
            journal: stats
            for journal, stats in self.journal_stats.items()
            if journal in active_journals
        }

        self.journal_papers = {
            journal: papers
            for journal, papers in self.journal_papers.items()
            if journal in active_journals
        }

        # ë…¼ë¬¸-ì €ë„ ë§¤í•‘ë„ í•„í„°ë§
        filtered_paper_journals = {}
        for paper_id, paper_info in self.paper_journals.items():
            if paper_info["journal"] in active_journals:
                filtered_paper_journals[paper_id] = paper_info

        self.paper_journals = filtered_paper_journals

        print(f"   ğŸ“„ Active journals remaining: {len(self.journal_stats)}")
        print(f"   ğŸ“° Papers with active journals: {len(self.paper_journals)}")

        return active_journals

    def build_journal_paper_graph(self):
        """ì €ë„-ë…¼ë¬¸ ì´ë¶„ ê·¸ë˜í”„ êµ¬ì¶•"""
        print(f"ğŸ”— Building journal-paper bipartite graph...")

        # ë°©í–¥ ê·¸ë˜í”„ ìƒì„± (Paper â†’ Journal)
        G = nx.DiGraph()

        # ì €ë„ ë…¸ë“œ ì¶”ê°€
        for journal, stats in self.journal_stats.items():
            G.add_node(
                journal,
                node_type="journal",
                name=journal,
                paper_count=stats["paper_count"],
                unique_authors=stats["unique_authors"],
                unique_keywords=stats["unique_keywords"],
                first_year=stats["first_year"],
                last_year=stats["last_year"],
                active_years=stats["active_years"],
                avg_authors_per_paper=stats["avg_authors_per_paper"],
                journal_type=stats["journal_type"],
                top_keywords=[kw for kw, count in stats["top_keywords"]],
            )

        # ë…¼ë¬¸ ë…¸ë“œ ì¶”ê°€
        for paper_id, paper_info in self.paper_journals.items():
            G.add_node(
                paper_id,
                node_type="paper",
                title=paper_info["title"],
                abstract=paper_info.get("abstract", ""),
                year=paper_info["year"],
                author_count=paper_info["author_count"],
                keyword_count=paper_info["keyword_count"],
                authors=paper_info["authors"][:5],  # ì²˜ìŒ 5ëª…ë§Œ ì €ì¥
                keywords=paper_info["keywords"][:10],
                has_abstract=paper_info.get("has_abstract", False),
                abstract_length=paper_info.get("abstract_length", 0),
            )  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥

        # Published_in ì—£ì§€ ì¶”ê°€ (Paper â†’ Journal)
        edges_added = 0

        for paper_id, paper_info in self.paper_journals.items():
            journal = paper_info["journal"]
            year = paper_info["year"]

            if G.has_node(paper_id) and G.has_node(journal):
                G.add_edge(
                    paper_id, journal, edge_type="published_in", year=year, weight=1.0
                )  # ë‹¨ìˆœ ê´€ê³„ì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ëŠ” 1

                edges_added += 1

        print(f"âœ… Journal-paper bipartite graph constructed:")
        print(
            f"   ğŸ“° Journal nodes: {sum(1 for n in G.nodes() if G.nodes[n]['node_type'] == 'journal')}"
        )
        print(
            f"   ğŸ“„ Paper nodes: {sum(1 for n in G.nodes() if G.nodes[n]['node_type'] == 'paper')}"
        )
        print(f"   ğŸ”— Published_in edges: {G.number_of_edges()}")
        print(f"   ğŸ“ˆ Graph density: {nx.density(G):.6f}")

        return G

    def analyze_graph_properties(self, G):
        """ê·¸ë˜í”„ ì†ì„± ë¶„ì„"""
        print("ğŸ“ˆ Analyzing journal-paper graph properties...")

        # ë…¸ë“œë³„ í†µê³„
        journal_nodes = [n for n in G.nodes() if G.nodes[n]["node_type"] == "journal"]
        paper_nodes = [n for n in G.nodes() if G.nodes[n]["node_type"] == "paper"]

        # ê¸°ë³¸ í†µê³„
        stats = {
            "basic_stats": {
                "num_journals": len(journal_nodes),
                "num_papers": len(paper_nodes),
                "num_edges": G.number_of_edges(),
                "density": float(nx.density(G)),
                "avg_papers_per_journal": (
                    G.number_of_edges() / len(journal_nodes) if journal_nodes else 0
                ),
            }
        }

        # ì €ë„ë³„ ë…¼ë¬¸ ìˆ˜ ë¶„ì„
        journal_paper_counts = {}
        for journal in journal_nodes:
            paper_count = G.in_degree(journal)  # ë“¤ì–´ì˜¤ëŠ” ì—£ì§€ = ë°œí‘œëœ ë…¼ë¬¸ ìˆ˜
            journal_paper_counts[journal] = paper_count

        stats["journal_analysis"] = {
            "top_journals_by_papers": sorted(
                journal_paper_counts.items(), key=lambda x: x[1], reverse=True
            )[:10],
            "journal_paper_distribution": {
                "mean": float(np.mean(list(journal_paper_counts.values()))),
                "median": float(np.median(list(journal_paper_counts.values()))),
                "max": max(journal_paper_counts.values()),
                "min": min(journal_paper_counts.values()),
            },
        }

        # ì €ë„ íƒ€ì…ë³„ ë¶„ì„
        journal_type_analysis = self.analyze_journal_types(G)
        stats["journal_types"] = journal_type_analysis

        # ì‹œê°„ì  ë¶„ì„
        temporal_analysis = self.analyze_temporal_patterns(G)
        stats["temporal_patterns"] = temporal_analysis

        return stats

    def analyze_journal_types(self, G):
        """ì €ë„ íƒ€ì…ë³„ ë¶„ì„"""
        type_counts = Counter()
        type_papers = defaultdict(int)

        for node in G.nodes():
            if G.nodes[node]["node_type"] == "journal":
                journal_type = G.nodes[node]["journal_type"]
                paper_count = G.in_degree(node)

                type_counts[journal_type] += 1
                type_papers[journal_type] += paper_count

        return {
            "type_distribution": dict(type_counts),
            "papers_by_type": dict(type_papers),
        }

    def analyze_temporal_patterns(self, G):
        """ì‹œê°„ì  íŒ¨í„´ ë¶„ì„"""
        papers_by_year = defaultdict(int)
        journals_by_year = defaultdict(set)

        for paper_id in G.nodes():
            if G.nodes[paper_id]["node_type"] == "paper":
                year = G.nodes[paper_id].get("year", "")
                if year and str(year).isdigit():
                    year_int = int(year)
                    papers_by_year[year_int] += 1

                    # í•´ë‹¹ ë…¼ë¬¸ì´ ë°œí‘œëœ ì €ë„ ì°¾ê¸°
                    for journal in G.successors(paper_id):
                        journals_by_year[year_int].add(journal)

        # ë…„ë„ë³„ ì €ë„ ìˆ˜ ê³„ì‚°
        journals_count_by_year = {
            year: len(journals) for year, journals in journals_by_year.items()
        }

        return {
            "papers_by_year": dict(papers_by_year),
            "journals_count_by_year": journals_count_by_year,
            "active_years": list(papers_by_year.keys()) if papers_by_year else [],
        }

    def create_node_info_maps(self, G):
        """ë…¸ë“œ ì •ë³´ë¥¼ ì‰½ê²Œ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ë§µ ìƒì„±"""
        journal_info = {}
        paper_info = {}

        # ì €ë„ ì •ë³´
        for node in G.nodes():
            if G.nodes[node]["node_type"] == "journal":
                node_data = G.nodes[node]
                journal_info[node] = {
                    "name": node,
                    "journal_type": node_data.get("journal_type", ""),
                    "paper_count": G.in_degree(node),
                    "unique_authors": node_data.get("unique_authors", 0),
                    "first_year": node_data.get("first_year", ""),
                    "last_year": node_data.get("last_year", ""),
                    "active_years": node_data.get("active_years", 0),
                    "top_keywords": node_data.get("top_keywords", [])[:5],  # ìƒìœ„ 5ê°œë§Œ
                }

        # ë…¼ë¬¸ ì •ë³´
        for node in G.nodes():
            if G.nodes[node]["node_type"] == "paper":
                node_data = G.nodes[node]
                # í•´ë‹¹ ë…¼ë¬¸ì˜ ì €ë„ ì°¾ê¸°
                journals = list(G.successors(node))
                journal_name = journals[0] if journals else "Unknown"

                paper_info[node] = {
                    "title": node_data.get("title", ""),
                    "abstract": node_data.get("abstract", ""),
                    "journal": journal_name,
                    "year": node_data.get("year", ""),
                    "author_count": node_data.get("author_count", 0),
                    "keyword_count": node_data.get("keyword_count", 0),
                    "authors": node_data.get("authors", []),
                    "has_abstract": node_data.get("has_abstract", False),
                    "abstract_length": node_data.get("abstract_length", 0),
                }

        return journal_info, paper_info

    def save_journal_paper_graph_and_analysis(self, G, stats, output_dir):
        """Journal-paper ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(output_dir)

        # 1. NetworkX ê·¸ë˜í”„ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (GraphRAGìš©)
        graph_data = {"nodes": [], "edges": []}

        # ë…¸ë“œ ì •ë³´
        for node in G.nodes():
            node_data = G.nodes[node].copy()
            node_data["id"] = node
            # ë¦¬ìŠ¤íŠ¸ í•„ë“œë“¤ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ìœ ì§€
            graph_data["nodes"].append(node_data)

        # ì—£ì§€ ì •ë³´
        for edge in G.edges():
            edge_data = G.edges[edge].copy()
            edge_data["source"] = edge[0]
            edge_data["target"] = edge[1]
            graph_data["edges"].append(edge_data)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        graph_file = output_dir / "journal_paper_graph.json"
        with open(graph_file, "w", encoding="utf-8") as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)

        # 2. âœ… GraphML íŒŒì¼ë¡œ ì €ì¥ (XML í˜¸í™˜ì„± ê°œì„ )
        try:
            G_graphml = G.copy()

            # XML í˜¸í™˜ ë¬¸ìì—´ ì •ì œ í•¨ìˆ˜
            def clean_xml_string(text):
                if not isinstance(text, str):
                    text = str(text)

                # NULL ë°”ì´íŠ¸ ë° ì œì–´ ë¬¸ì ì œê±°
                import re

                # XML 1.0ì—ì„œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ìë“¤ ì œê±°
                text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", "", text)

                # XML íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
                text = text.replace("&", "&amp;")
                text = text.replace("<", "&lt;")
                text = text.replace(">", "&gt;")
                text = text.replace('"', "&quot;")
                text = text.replace("'", "&apos;")

                return text

            # ëª¨ë“  ë…¸ë“œ ì†ì„± ì •ì œ
            for node in G_graphml.nodes():
                for attr_name, attr_value in G_graphml.nodes[node].items():
                    if isinstance(attr_value, list):
                        # ë¦¬ìŠ¤íŠ¸ë¥¼ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì •ì œ
                        cleaned_list = [clean_xml_string(str(v)) for v in attr_value]
                        G_graphml.nodes[node][attr_name] = ";".join(cleaned_list)
                    elif isinstance(attr_value, str):
                        # ë¬¸ìì—´ ì •ì œ (Abstract í¬í•¨)
                        G_graphml.nodes[node][attr_name] = clean_xml_string(attr_value)
                    elif not isinstance(attr_value, (int, float, bool)):
                        # ê¸°íƒ€ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì •ì œ
                        G_graphml.nodes[node][attr_name] = clean_xml_string(
                            str(attr_value)
                        )

            # ì—£ì§€ ì†ì„±ë„ ì •ì œ
            for u, v in G_graphml.edges():
                for attr_name, attr_value in G_graphml.edges[u, v].items():
                    if isinstance(attr_value, str):
                        G_graphml.edges[u, v][attr_name] = clean_xml_string(attr_value)
                    elif not isinstance(attr_value, (int, float, bool)):
                        G_graphml.edges[u, v][attr_name] = clean_xml_string(
                            str(attr_value)
                        )

            graphml_file = output_dir / "journal_paper_graph.graphml"
            nx.write_graphml(
                G_graphml, graphml_file, encoding="utf-8", prettyprint=True
            )
            print(f"   ğŸ”— Graph (GraphML): {graphml_file}")

        except Exception as e:
            print(f"âš ï¸  GraphML ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“„ JSON íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}")
            graphml_file = None

        # 3. ë¶„ì„ ê²°ê³¼ ì €ì¥
        stats_file = output_dir / "journal_paper_analysis.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        # 4. ë…¸ë“œ ì •ë³´ í…Œì´ë¸”ë“¤ ì €ì¥
        journal_info, paper_info = self.create_node_info_maps(G)

        journal_info_file = output_dir / "journal_paper_journal_info.json"
        with open(journal_info_file, "w", encoding="utf-8") as f:
            json.dump(journal_info, f, ensure_ascii=False, indent=2)

        paper_info_file = output_dir / "journal_paper_paper_info.json"
        with open(paper_info_file, "w", encoding="utf-8") as f:
            json.dump(paper_info, f, ensure_ascii=False, indent=2)

        # 5. CSV í˜•íƒœì˜ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        edge_list = []
        for edge in G.edges():
            edge_info = G.edges[edge].copy()
            edge_info.update(
                {
                    "paper_id": edge[0],
                    "journal_name": edge[1],
                    "paper_title": G.nodes[edge[0]].get("title", ""),
                    "journal_type": G.nodes[edge[1]].get("journal_type", ""),
                    "paper_year": G.nodes[edge[0]].get("year", ""),
                    "has_abstract": G.nodes[edge[0]].get("has_abstract", False),
                    "abstract_length": G.nodes[edge[0]].get("abstract_length", 0),
                }
            )
            edge_list.append(edge_info)

        edge_df = pd.DataFrame(edge_list)
        edge_file = output_dir / "journal_paper_edges.csv"
        edge_df.to_csv(edge_file, index=False, encoding="utf-8")

        print(f"ğŸ’¾ Journal-paper graph results saved:")
        print(f"   ğŸ”— Graph (JSON): {graph_file}")
        if graphml_file:
            print(f"   ğŸ”— Graph (GraphML): {graphml_file}")
        print(f"   ğŸ“Š Analysis: {stats_file}")
        print(f"   ğŸ“° Journal Info: {journal_info_file}")
        print(f"   ğŸ“„ Paper Info: {paper_info_file}")
        print(f"   ğŸ“ˆ Edge List: {edge_file}")

        return graph_file

    def process_papers(self, papers_metadata):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ Starting journal-paper graph construction...")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.papers_metadata = papers_metadata

        # 1. ì €ë„-ë…¼ë¬¸ ê´€ê³„ ì¶”ì¶œ
        self.extract_journal_paper_relationships(papers_metadata)

        # 2. ì €ë„ í†µê³„ ê³„ì‚°
        self.calculate_journal_statistics()

        # 3. í™œë™ ìˆ˜ì¤€ ê¸°ì¤€ í•„í„°ë§
        active_journals = self.filter_journals_by_activity()

        # 4. ì´ë¶„ ê·¸ë˜í”„ êµ¬ì¶•
        G = self.build_journal_paper_graph()

        # 5. ê·¸ë˜í”„ ë¶„ì„
        stats = self.analyze_graph_properties(G)

        return G, stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import RAW_EXTRACTIONS_DIR, GRAPHS_DIR

    # í†µí•© ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_file = RAW_EXTRACTIONS_DIR / "integrated_papers_metadata.json"

    if not metadata_file.exists():
        print(f"âŒ Metadata not found: {metadata_file}")
        print("Please run data processing pipeline first.")
        return

    with open(metadata_file, "r", encoding="utf-8") as f:
        papers_metadata = json.load(f)

    print(f"ğŸ“„ Loaded {len(papers_metadata)} papers metadata")

    # Journal-Paper Graph Builder ì´ˆê¸°í™”
    builder = JournalPaperGraphBuilder(
        min_papers_per_journal=1  # ìµœì†Œ 1í¸ ì´ìƒ ë…¼ë¬¸ì´ ìˆëŠ” ì €ë„ë§Œ í¬í•¨
    )

    # ì „ì²´ ì²˜ë¦¬
    G, stats = builder.process_papers(papers_metadata)

    # ê²°ê³¼ ì €ì¥
    output_file = builder.save_journal_paper_graph_and_analysis(G, stats, GRAPHS_DIR)

    print(f"\nâœ… Journal-paper graph construction completed!")
    print(f"ğŸ“ Main output: {output_file}")

    # ìš”ì•½ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š Final Summary:")
    print(f"   ğŸ“° Total journals: {stats['basic_stats']['num_journals']}")
    print(f"   ğŸ“„ Total papers: {stats['basic_stats']['num_papers']}")
    print(f"   ğŸ”— Published_in relationships: {stats['basic_stats']['num_edges']}")
    print(
        f"   ğŸ“ˆ Avg papers per journal: {stats['basic_stats']['avg_papers_per_journal']:.1f}"
    )

    if stats.get("journal_types"):
        type_dist = stats["journal_types"]["type_distribution"]
        print(f"   ğŸ“‹ Journal types:")
        for journal_type, count in sorted(
            type_dist.items(), key=lambda x: x[1], reverse=True
        ):
            papers_count = stats["journal_types"]["papers_by_type"].get(journal_type, 0)
            print(f"      {journal_type}: {count} journals, {papers_count} papers")

    if stats.get("journal_analysis"):
        top_journals = stats["journal_analysis"]["top_journals_by_papers"][:3]
        print(f"   ğŸ† Top 3 most active journals:")
        for i, (journal, paper_count) in enumerate(top_journals):
            journal_type = G.nodes[journal].get("journal_type", "")
            print(f"      {i+1}. {journal} ({journal_type}) - {paper_count} papers")

    return G, output_file


if __name__ == "__main__":
    main()
