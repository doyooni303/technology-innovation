"""
ì¸ìš© ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Citation Network Graph Construction Module

ê¸°ì¡´ reference_extractor.pyì—ì„œ ì¶”ì¶œí•œ citation ë°ì´í„°ë¥¼
NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from tqdm import tqdm
import matplotlib.pyplot as plt


class CitationGraphBuilder:
    """ì¸ìš© ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, min_citations=1):
        """
        Args:
            min_citations (int): ê·¸ë˜í”„ì— í¬í•¨í•  ìµœì†Œ ì¸ìš© ìˆ˜
        """
        self.min_citations = min_citations
        self.citation_data = None
        self.papers_metadata = None

    def load_citation_data(self, citation_file, metadata_file):
        """ê¸°ì¡´ì— ì¶”ì¶œëœ citation ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ Loading citation data and metadata...")

        # Citation network ë°ì´í„° ë¡œë“œ
        with open(citation_file, "r", encoding="utf-8") as f:
            self.citation_data = json.load(f)

        # Papers ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_file, "r", encoding="utf-8") as f:
            self.papers_metadata = json.load(f)

        print(f"   ğŸ“„ Citation data loaded: {len(self.citation_data)} papers")
        print(f"   ğŸ“š Metadata loaded: {len(self.papers_metadata)} papers")

    def analyze_citation_data(self):
        """Citation ë°ì´í„° ë¶„ì„"""
        print("ğŸ“Š Analyzing citation data...")

        total_citations = sum(
            len(citations) for citations in self.citation_data.values()
        )
        papers_with_citations = sum(
            1 for citations in self.citation_data.values() if citations
        )

        citation_counts = [len(citations) for citations in self.citation_data.values()]

        print(f"   ğŸ”— Total citation relationships: {total_citations}")
        print(f"   ğŸ“„ Papers with outgoing citations: {papers_with_citations}")
        print(
            f"   ğŸ“ˆ Avg citations per paper: {total_citations/len(self.citation_data):.1f}"
        )

        if citation_counts:
            print(f"   ğŸ“Š Citation count distribution:")
            print(f"      Min: {min(citation_counts)}")
            print(f"      Max: {max(citation_counts)}")
            print(f"      Median: {np.median(citation_counts):.1f}")

    def build_citation_graph(self):
        """Citation ë°ì´í„°ë¡œë¶€í„° NetworkX ê·¸ë˜í”„ êµ¬ì¶•"""
        print("ğŸ”— Building citation network graph...")

        # ë°©í–¥ ê·¸ë˜í”„ ìƒì„± (ì¸ìš©ì€ ë°©í–¥ì„±ì´ ìˆìŒ)
        G = nx.DiGraph()

        # ë…¼ë¬¸ ID to ë©”íƒ€ë°ì´í„° ë§¤í•‘ ìƒì„±
        paper_metadata_map = {}
        for i, paper in enumerate(self.papers_metadata):
            paper_id = f"paper_{i}"
            paper_metadata_map[paper_id] = paper

        # ëª¨ë“  ë…¼ë¬¸ì„ ë…¸ë“œë¡œ ì¶”ê°€ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        for paper_id, metadata in paper_metadata_map.items():
            G.add_node(
                paper_id,
                node_type="paper",
                title=metadata.get("title", ""),
                authors=metadata.get("authors", []),
                year=metadata.get("year", ""),
                journal=metadata.get("journal", ""),
                keywords=metadata.get("keywords", []),
                has_pdf=metadata.get("has_pdf", False),
            )

        # Citation ì—£ì§€ ì¶”ê°€
        edges_added = 0
        citation_weights = []

        for citing_paper, citations in self.citation_data.items():
            if not citations:
                continue

            for citation in citations:
                cited_paper = citation["cited_paper_id"]
                similarity = citation.get("similarity", 0.0)

                # ìê¸° ì¸ìš© ì œì™¸
                if citing_paper != cited_paper:
                    # ì–‘ìª½ ë…¼ë¬¸ì´ ëª¨ë‘ ê·¸ë˜í”„ì— ìˆëŠ” ê²½ìš°ë§Œ ì—£ì§€ ì¶”ê°€
                    if G.has_node(citing_paper) and G.has_node(cited_paper):
                        G.add_edge(
                            citing_paper,
                            cited_paper,
                            edge_type="citation",
                            similarity_score=similarity,
                            reference_text=citation.get("reference_text", ""),
                            extracted_title=citation.get("extracted_title", ""),
                        )

                        edges_added += 1
                        citation_weights.append(similarity)

        print(f"âœ… Citation graph constructed:")
        print(f"   ğŸ“„ Nodes (papers): {G.number_of_nodes()}")
        print(f"   ğŸ”— Edges (citations): {G.number_of_edges()}")
        print(f"   ğŸ“ˆ Graph density: {nx.density(G):.6f}")

        if citation_weights:
            print(f"   ğŸ¯ Similarity score stats:")
            print(f"      Mean: {np.mean(citation_weights):.3f}")
            print(f"      Min: {min(citation_weights):.3f}")
            print(f"      Max: {max(citation_weights):.3f}")

        return G

    def analyze_graph_properties(self, G):
        """ê·¸ë˜í”„ ì†ì„± ë¶„ì„"""
        print("ğŸ“ˆ Analyzing citation graph properties...")

        # ê¸°ë³¸ í†µê³„
        stats = {
            "basic_stats": {
                "num_nodes": G.number_of_nodes(),
                "num_edges": G.number_of_edges(),
                "density": nx.density(G),
                "is_weakly_connected": nx.is_weakly_connected(G),
                "is_strongly_connected": nx.is_strongly_connected(G),
            }
        }

        # ì—°ê²° ì„±ë¶„ ë¶„ì„
        weak_components = list(nx.weakly_connected_components(G))
        strong_components = list(nx.strongly_connected_components(G))

        stats["connectivity"] = {
            "num_weak_components": len(weak_components),
            "num_strong_components": len(strong_components),
            "largest_weak_component": len(max(weak_components, key=len)),
            "largest_strong_component": len(max(strong_components, key=len)),
        }

        # ì¤‘ì‹¬ì„± ë¶„ì„ (ê³„ì‚° ë¹„ìš© ê³ ë ¤í•˜ì—¬ ìƒ˜í”Œë§)
        if G.number_of_nodes() > 0:
            in_degree_centrality = nx.in_degree_centrality(G)
            out_degree_centrality = nx.out_degree_centrality(G)

            # í”¼ì¸ìš© ìˆ˜ê°€ ë§ì€ ë…¼ë¬¸ë“¤ (ì˜í–¥ë ¥ ìˆëŠ” ë…¼ë¬¸)
            most_cited = sorted(
                in_degree_centrality.items(), key=lambda x: x[1], reverse=True
            )[:10]

            # ì¸ìš©ì„ ë§ì´ í•˜ëŠ” ë…¼ë¬¸ë“¤ (í¬ê´„ì  ë¦¬ë·° ë…¼ë¬¸)
            most_citing = sorted(
                out_degree_centrality.items(), key=lambda x: x[1], reverse=True
            )[:10]

            stats["centrality"] = {
                "most_cited_papers": most_cited,
                "most_citing_papers": most_citing,
            }

            # ì‹¤ì œ ì¸ìš© ìˆ˜ë¡œë„ ì •ë ¬
            in_degrees = dict(G.in_degree())
            out_degrees = dict(G.out_degree())

            stats["degree_stats"] = {
                "top_cited_by_count": sorted(
                    in_degrees.items(), key=lambda x: x[1], reverse=True
                )[:10],
                "top_citing_by_count": sorted(
                    out_degrees.items(), key=lambda x: x[1], reverse=True
                )[:10],
                "avg_in_degree": np.mean(list(in_degrees.values())),
                "avg_out_degree": np.mean(list(out_degrees.values())),
            }

        # ë…„ë„ë³„ ì¸ìš© íŒ¨í„´ ë¶„ì„
        year_citation_pattern = self.analyze_temporal_patterns(G)
        stats["temporal_patterns"] = year_citation_pattern

        return stats

    def analyze_temporal_patterns(self, G):
        """ì‹œê°„ì  ì¸ìš© íŒ¨í„´ ë¶„ì„"""
        print("ğŸ“… Analyzing temporal citation patterns...")

        # ë…„ë„ë³„ ë…¼ë¬¸ ìˆ˜
        papers_by_year = defaultdict(int)

        # ë…„ë„ë³„ ì¸ìš© íŒ¨í„´
        citations_by_year = defaultdict(list)  # citing_year -> [cited_years]

        for node in G.nodes():
            year = G.nodes[node].get("year", "")
            if year and year.isdigit():
                papers_by_year[int(year)] += 1

        # ì¸ìš© ê´€ê³„ì˜ ì‹œê°„ì  íŒ¨í„´
        for edge in G.edges():
            citing_paper, cited_paper = edge
            citing_year = G.nodes[citing_paper].get("year", "")
            cited_year = G.nodes[cited_paper].get("year", "")

            if (
                citing_year
                and cited_year
                and citing_year.isdigit()
                and cited_year.isdigit()
            ):
                citing_year, cited_year = int(citing_year), int(cited_year)
                citations_by_year[citing_year].append(cited_year)

        # í‰ê·  citation lag ê³„ì‚°
        citation_lags = []
        for citing_year, cited_years in citations_by_year.items():
            for cited_year in cited_years:
                if citing_year > cited_year:  # ê³¼ê±° ë…¼ë¬¸ì„ ì¸ìš©í•˜ëŠ” ê²½ìš°
                    citation_lags.append(citing_year - cited_year)

        temporal_stats = {
            "papers_by_year": dict(papers_by_year),
            "citation_count_by_year": {
                year: len(cited_years)
                for year, cited_years in citations_by_year.items()
            },
            "avg_citation_lag": np.mean(citation_lags) if citation_lags else 0,
            "median_citation_lag": np.median(citation_lags) if citation_lags else 0,
            "citation_lag_distribution": citation_lags[:100],  # ì²˜ìŒ 100ê°œë§Œ ì €ì¥
        }

        return temporal_stats

    def create_paper_info_map(self, G):
        """ë…¼ë¬¸ ì •ë³´ë¥¼ ì‰½ê²Œ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ë§µ ìƒì„±"""
        paper_info = {}

        for node in G.nodes():
            node_data = G.nodes[node]
            paper_info[node] = {
                "title": node_data.get("title", ""),
                "authors": ", ".join(node_data.get("authors", [])),
                "year": node_data.get("year", ""),
                "journal": node_data.get("journal", ""),
                "keywords": ", ".join(node_data.get("keywords", [])),
                "in_degree": G.in_degree(node),  # í”¼ì¸ìš© ìˆ˜
                "out_degree": G.out_degree(node),  # ì¸ìš© ìˆ˜
                "has_pdf": node_data.get("has_pdf", False),
            }

        return paper_info

    def save_citation_graph_and_analysis(self, G, stats, output_dir):
        """Citation ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(output_dir)

        # 1. NetworkX ê·¸ë˜í”„ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (GraphRAGìš©)
        graph_data = {"nodes": [], "edges": []}

        # ë…¸ë“œ ì •ë³´
        for node in G.nodes():
            node_data = G.nodes[node].copy()
            node_data["id"] = node
            # ë¦¬ìŠ¤íŠ¸ íƒ€ì… í•„ë“œë“¤ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
            if "authors" in node_data and isinstance(node_data["authors"], list):
                node_data["authors"] = list(node_data["authors"])
            if "keywords" in node_data and isinstance(node_data["keywords"], list):
                node_data["keywords"] = list(node_data["keywords"])
            graph_data["nodes"].append(node_data)

        # ì—£ì§€ ì •ë³´
        for edge in G.edges():
            edge_data = G.edges[edge].copy()
            edge_data["source"] = edge[0]
            edge_data["target"] = edge[1]
            graph_data["edges"].append(edge_data)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        graph_file = output_dir / "citation_network_graph.json"
        with open(graph_file, "w", encoding="utf-8") as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)

        # 2. GraphML íŒŒì¼ë¡œ ì €ì¥
        try:
            # GraphML í˜¸í™˜ì„ ìœ„í•´ ê·¸ë˜í”„ ë³µì‚¬ ë° ì†ì„± ë³€í™˜
            G_graphml = G.copy()

            # ë…¸ë“œ ì†ì„±ì„ GraphML í˜¸í™˜ í˜•íƒœë¡œ ë³€í™˜
            for node in G_graphml.nodes():
                # ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì†ì„±ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                if "authors" in G_graphml.nodes[node]:
                    authors_list = G_graphml.nodes[node]["authors"]
                    if isinstance(authors_list, (list, set)):
                        G_graphml.nodes[node]["authors"] = ";".join(
                            str(a) for a in authors_list
                        )

                if "keywords" in G_graphml.nodes[node]:
                    keywords_list = G_graphml.nodes[node]["keywords"]
                    if isinstance(keywords_list, (list, set)):
                        G_graphml.nodes[node]["keywords"] = ";".join(
                            str(k) for k in keywords_list
                        )

                # ê¸°íƒ€ ë³µì¡í•œ íƒ€ì…ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                for attr_name, attr_value in G_graphml.nodes[node].items():
                    if isinstance(attr_value, (list, set, dict)):
                        if isinstance(attr_value, dict):
                            G_graphml.nodes[node][attr_name] = json.dumps(attr_value)
                        else:
                            G_graphml.nodes[node][attr_name] = ";".join(
                                str(v) for v in attr_value
                            )
                    elif not isinstance(attr_value, (str, int, float, bool)):
                        G_graphml.nodes[node][attr_name] = str(attr_value)

            # ì—£ì§€ ì†ì„±ë„ í™•ì¸
            for edge in G_graphml.edges():
                for attr_name, attr_value in G_graphml.edges[edge].items():
                    if isinstance(attr_value, (list, set, dict)):
                        if isinstance(attr_value, dict):
                            G_graphml.edges[edge][attr_name] = json.dumps(attr_value)
                        else:
                            G_graphml.edges[edge][attr_name] = ";".join(
                                str(v) for v in attr_value
                            )
                    elif not isinstance(attr_value, (str, int, float, bool)):
                        G_graphml.edges[edge][attr_name] = str(attr_value)

            graphml_file = output_dir / "citation_network_graph.graphml"
            nx.write_graphml(G_graphml, graphml_file)

        except Exception as e:
            print(f"âš ï¸  GraphML ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“„ JSON íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}")
            graphml_file = None

        # 3. ë¶„ì„ ê²°ê³¼ ì €ì¥
        stats_file = output_dir / "citation_network_analysis.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        # 4. ë…¼ë¬¸ ì •ë³´ í…Œì´ë¸” ì €ì¥
        paper_info = self.create_paper_info_map(G)
        paper_info_file = output_dir / "citation_papers_info.json"
        with open(paper_info_file, "w", encoding="utf-8") as f:
            json.dump(paper_info, f, ensure_ascii=False, indent=2)

        # 5. CSV í˜•íƒœì˜ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        edge_list = []
        for edge in G.edges():
            edge_info = G.edges[edge].copy()
            edge_info.update(
                {
                    "citing_paper": edge[0],
                    "cited_paper": edge[1],
                    "citing_title": G.nodes[edge[0]].get("title", ""),
                    "cited_title": G.nodes[edge[1]].get("title", ""),
                    "citing_year": G.nodes[edge[0]].get("year", ""),
                    "cited_year": G.nodes[edge[1]].get("year", ""),
                }
            )
            edge_list.append(edge_info)

        edge_df = pd.DataFrame(edge_list)
        edge_file = output_dir / "citation_network_edges.csv"
        edge_df.to_csv(edge_file, index=False, encoding="utf-8")

        print(f"ğŸ’¾ Citation graph results saved:")
        print(f"   ğŸ”— Graph (JSON): {graph_file}")
        if graphml_file:
            print(f"   ğŸ”— Graph (GraphML): {graphml_file}")
        print(f"   ğŸ“Š Analysis: {stats_file}")
        print(f"   ğŸ“„ Paper Info: {paper_info_file}")
        print(f"   ğŸ“ˆ Edge List: {edge_file}")

        return graph_file

    def process_citation_data(self, citation_file, metadata_file):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ Starting citation network graph construction...")

        # 1. ë°ì´í„° ë¡œë“œ
        self.load_citation_data(citation_file, metadata_file)

        # 2. ë°ì´í„° ë¶„ì„
        self.analyze_citation_data()

        # 3. ê·¸ë˜í”„ êµ¬ì¶•
        G = self.build_citation_graph()

        # 4. ê·¸ë˜í”„ ë¶„ì„
        stats = self.analyze_graph_properties(G)

        return G, stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import RAW_EXTRACTIONS_DIR, GRAPHS_DIR

    # í•„ìš”í•œ íŒŒì¼ë“¤ í™•ì¸
    citation_file = RAW_EXTRACTIONS_DIR / "citation_network_simple.json"
    metadata_file = RAW_EXTRACTIONS_DIR / "integrated_papers_metadata.json"

    if not citation_file.exists():
        print(f"âŒ Citation data not found: {citation_file}")
        print("Please run reference_extractor.py first.")
        return

    if not metadata_file.exists():
        print(f"âŒ Metadata not found: {metadata_file}")
        print("Please run data processing pipeline first.")
        return

    print(f"ğŸ“‚ Citation file: {citation_file}")
    print(f"ğŸ“‚ Metadata file: {metadata_file}")

    # Citation Graph Builder ì´ˆê¸°í™”
    builder = CitationGraphBuilder(min_citations=1)

    # ì „ì²´ ì²˜ë¦¬
    G, stats = builder.process_citation_data(citation_file, metadata_file)

    # ê²°ê³¼ ì €ì¥
    output_file = builder.save_citation_graph_and_analysis(G, stats, GRAPHS_DIR)

    print(f"\nâœ… Citation network graph construction completed!")
    print(f"ğŸ“ Main output: {output_file}")

    # ìš”ì•½ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š Final Summary:")
    print(f"   ğŸ“„ Total papers: {G.number_of_nodes()}")
    print(f"   ğŸ”— Citation relationships: {G.number_of_edges()}")
    print(f"   ğŸ“ˆ Graph density: {nx.density(G):.6f}")

    if stats.get("degree_stats"):
        top_cited = stats["degree_stats"]["top_cited_by_count"][:3]
        print(f"   ğŸ† Top 3 most cited papers:")
        for i, (paper_id, citation_count) in enumerate(top_cited):
            title = G.nodes[paper_id].get("title", "Unknown")[:50] + "..."
            print(f"      {i+1}. {title} ({citation_count} citations)")

    return G, output_file


if __name__ == "__main__":
    main()
