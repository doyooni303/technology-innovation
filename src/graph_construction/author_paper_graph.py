"""
ì €ì-ë…¼ë¬¸ ì´ë¶„ ê·¸ë˜í”„ êµ¬ì¶• ëª¨ë“ˆ
Author-Paper Bipartite Graph Construction Module
"""

import json
import numpy as np
import pandas as pd
import networkx as nx
from pathlib import Path
from collections import Counter, defaultdict
from tqdm import tqdm
import re


class AuthorPaperGraphBuilder:
    """ì €ì-ë…¼ë¬¸ ì´ë¶„ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, min_papers_per_author=1, min_authors_per_paper=1):
        """
        Args:
            min_papers_per_author (int): ê·¸ë˜í”„ì— í¬í•¨í•  ì €ìì˜ ìµœì†Œ ë…¼ë¬¸ ìˆ˜
            min_authors_per_paper (int): ê·¸ë˜í”„ì— í¬í•¨í•  ë…¼ë¬¸ì˜ ìµœì†Œ ì €ì ìˆ˜
        """
        self.min_papers_per_author = min_papers_per_author
        self.min_authors_per_paper = min_authors_per_paper

        # ê²°ê³¼ ì €ì¥ìš©
        self.author_papers = defaultdict(list)  # ì €ìë³„ ë…¼ë¬¸ ëª©ë¡
        self.paper_authors = {}  # ë…¼ë¬¸ë³„ ì €ì ëª©ë¡
        self.author_stats = {}
        self.paper_stats = {}
        self.papers_metadata = None

    def clean_author_name(self, author_name):
        """ì €ìëª… ì •ì œ í•¨ìˆ˜"""
        if not author_name or not isinstance(author_name, str):
            return None

        # ê¸°ë³¸ ì •ì œ
        author_name = author_name.strip()

        # ë„ˆë¬´ ì§§ì€ ì´ë¦„ ì œì™¸
        if len(author_name) < 2:
            return None

        # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if author_name.isdigit():
            return None

        # ê¸°ë³¸ì ì¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬)
        author_name = re.sub(r"[^\w\s\.\-]", " ", author_name)
        author_name = re.sub(r"\s+", " ", author_name).strip()

        # ì¼ë°˜ì ì¸ í˜•íƒœë¡œ ì •ê·œí™”
        # "Smith, John" â†’ "John Smith" í˜•íƒœë¡œ ë³€í™˜
        if "," in author_name:
            parts = author_name.split(",", 1)
            if len(parts) == 2:
                last_name = parts[0].strip()
                first_name = parts[1].strip()
                author_name = f"{first_name} {last_name}".strip()

        # ì—°ì†ëœ ê³µë°± ì œê±°
        author_name = re.sub(r"\s+", " ", author_name)

        return author_name

    def extract_author_paper_relationships(self, papers_metadata):
        """ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì €ì-ë…¼ë¬¸ ê´€ê³„ ì¶”ì¶œ"""
        print("ğŸ” Extracting author-paper relationships...")

        valid_papers = 0
        valid_authors = 0
        total_authorship_relations = 0

        for i, paper in enumerate(tqdm(papers_metadata, desc="Processing papers")):
            paper_id = f"paper_{i}"
            title = paper.get("title", "")
            abstract = paper.get("abstract", "")
            authors = paper.get("authors", [])
            year = paper.get("year", "")
            journal = paper.get("journal", "")
            keywords = paper.get("keywords", [])

            if not authors:
                continue

            # ì €ìëª… ì •ì œ
            clean_authors = []
            for author in authors:
                clean_author = self.clean_author_name(author)
                if clean_author:
                    clean_authors.append(clean_author)

            if len(clean_authors) < self.min_authors_per_paper:
                continue

            # ë…¼ë¬¸-ì €ì ë§¤í•‘ ì €ì¥
            self.paper_authors[paper_id] = {
                "title": title,
                "abstract": abstract,
                "authors": clean_authors,
                "year": year,
                "journal": journal,
                "keywords": keywords,
                "author_count": len(clean_authors),
                "keyword_count": len(keywords) if keywords else 0,
            }

            # ì €ì-ë…¼ë¬¸ ë§¤í•‘ ì €ì¥
            for author in clean_authors:
                self.author_papers[author].append(
                    {
                        "paper_id": paper_id,
                        "title": title,
                        "abstract": abstract,
                        "year": year,
                        "journal": journal,
                        "keywords": keywords,
                        "authorship_position": clean_authors.index(author),  # ì €ì ìˆœì„œ
                        "is_first_author": clean_authors.index(author) == 0,
                        "is_last_author": clean_authors.index(author)
                        == len(clean_authors) - 1,
                        "co_author_count": len(clean_authors) - 1,
                    }
                )
                total_authorship_relations += 1

            valid_papers += 1

        valid_authors = len(self.author_papers)

        print(f"âœ… Author-paper relationship extraction completed:")
        print(f"   ğŸ“„ Papers with authors: {valid_papers}")
        print(f"   ğŸ‘¥ Unique authors: {valid_authors}")
        print(f"   ğŸ”— Total authorship relations: {total_authorship_relations}")
        print(
            f"   ğŸ“ˆ Avg authors per paper: {total_authorship_relations/valid_papers:.1f}"
        )
        print(
            f"   ğŸ“ˆ Avg papers per author: {total_authorship_relations/valid_authors:.1f}"
        )

    def calculate_author_statistics(self):
        """ì €ìë³„ í†µê³„ ê³„ì‚°"""
        print("ğŸ“Š Calculating author statistics...")

        for author, papers in self.author_papers.items():
            # ê¸°ë³¸ í†µê³„
            paper_count = len(papers)
            first_author_count = sum(1 for p in papers if p["is_first_author"])
            last_author_count = sum(1 for p in papers if p["is_last_author"])
            single_author_count = sum(1 for p in papers if p["co_author_count"] == 0)

            # í™œë™ ê¸°ê°„
            years = [p["year"] for p in papers if p["year"]]
            if years:
                try:
                    year_ints = [int(y) for y in years if str(y).isdigit()]
                    if year_ints:
                        first_year = min(year_ints)
                        last_year = max(year_ints)
                        active_years = last_year - first_year + 1
                        papers_by_year = Counter(year_ints)
                    else:
                        first_year = last_year = active_years = 0
                        papers_by_year = {}
                except:
                    first_year = last_year = active_years = 0
                    papers_by_year = {}
            else:
                first_year = last_year = active_years = 0
                papers_by_year = {}

            # ì €ë„ ë¶„ì„
            journals = [p["journal"] for p in papers if p["journal"]]
            unique_journals = len(set(journals)) if journals else 0
            most_frequent_journal = (
                Counter(journals).most_common(1)[0][0] if journals else ""
            )

            # í‚¤ì›Œë“œ ë¶„ì„
            all_keywords = []
            for paper in papers:
                all_keywords.extend(paper.get("keywords", []))
            keyword_frequency = Counter(all_keywords)
            top_keywords = keyword_frequency.most_common(10)

            # í˜‘ì—… íŒ¨í„´
            total_co_authors = sum(p["co_author_count"] for p in papers)
            avg_co_authors = total_co_authors / paper_count if paper_count > 0 else 0

            self.author_stats[author] = {
                "name": author,
                "paper_count": paper_count,
                "first_author_count": first_author_count,
                "last_author_count": last_author_count,
                "single_author_count": single_author_count,
                "first_year": first_year,
                "last_year": last_year,
                "active_years": active_years,
                "unique_journals": unique_journals,
                "most_frequent_journal": most_frequent_journal,
                "avg_co_authors": avg_co_authors,
                "papers_by_year": dict(papers_by_year),
                "top_keywords": top_keywords,
                "productivity_type": self.classify_author_productivity(
                    paper_count, active_years, first_author_count
                ),
            }

        print(f"âœ… Author statistics calculated for {len(self.author_stats)} authors")

    def calculate_paper_statistics(self):
        """ë…¼ë¬¸ë³„ í†µê³„ ê³„ì‚°"""
        print("ğŸ“Š Calculating paper statistics...")

        for paper_id, paper_info in self.paper_authors.items():
            authors = paper_info["authors"]
            author_count = len(authors)

            # ë…¼ë¬¸ íƒ€ì… ë¶„ë¥˜
            if author_count == 1:
                collaboration_type = "Single Author"
            elif author_count <= 3:
                collaboration_type = "Small Team"
            elif author_count <= 6:
                collaboration_type = "Medium Team"
            else:
                collaboration_type = "Large Team"

            # ì €ìë“¤ì˜ ê²½ë ¥ ë¶„ì„ (ì´ì „ ë…¼ë¬¸ ìˆ˜ ê¸°ì¤€)
            author_experience_levels = []
            for author in authors:
                author_paper_count = len(self.author_papers.get(author, []))
                if author_paper_count == 1:
                    author_experience_levels.append("Novice")
                elif author_paper_count <= 5:
                    author_experience_levels.append("Intermediate")
                else:
                    author_experience_levels.append("Experienced")

            experience_distribution = Counter(author_experience_levels)

            self.paper_stats[paper_id] = {
                "title": paper_info["title"],
                "abstract": paper_info["abstract"],
                "author_count": author_count,
                "collaboration_type": collaboration_type,
                "year": paper_info["year"],
                "journal": paper_info["journal"],
                "keyword_count": paper_info["keyword_count"],
                "authors": authors,
                "experience_distribution": dict(experience_distribution),
                "has_experienced_authors": "Experienced" in author_experience_levels,
                "is_interdisciplinary": self.assess_interdisciplinarity(
                    paper_info["keywords"]
                ),
            }

        print(f"âœ… Paper statistics calculated for {len(self.paper_stats)} papers")

    def classify_author_productivity(
        self, paper_count, active_years, first_author_count
    ):
        """ì €ì ìƒì‚°ì„± íƒ€ì… ë¶„ë¥˜"""
        if paper_count >= 10 and active_years >= 5:
            return "Highly Productive"
        elif paper_count >= 5 and first_author_count >= 2:
            return "Leading Researcher"
        elif paper_count >= 3:
            return "Active Researcher"
        elif paper_count == 1:
            return "Newcomer"
        else:
            return "Occasional Contributor"

    def assess_interdisciplinarity(self, keywords):
        """í‚¤ì›Œë“œ ê¸°ë°˜ í•™ì œê°„ ì—°êµ¬ ì—¬ë¶€ í‰ê°€"""
        if not keywords or len(keywords) < 3:
            return False

        # ê°„ë‹¨í•œ ë„ë©”ì¸ ë¶„ë¥˜
        ai_keywords = {
            "machine learning",
            "deep learning",
            "neural network",
            "ai",
            "artificial intelligence",
        }
        energy_keywords = {"battery", "energy", "power", "electric", "charging"}
        engineering_keywords = {
            "system",
            "control",
            "optimization",
            "design",
            "manufacturing",
        }

        keyword_set = set(kw.lower() for kw in keywords)

        domains = 0
        if ai_keywords & keyword_set:
            domains += 1
        if energy_keywords & keyword_set:
            domains += 1
        if engineering_keywords & keyword_set:
            domains += 1

        return domains >= 2

    def filter_by_activity(self):
        """í™œë™ ìˆ˜ì¤€ ê¸°ì¤€ìœ¼ë¡œ ì €ì/ë…¼ë¬¸ í•„í„°ë§"""
        print(
            f"ğŸ” Filtering authors/papers (min papers per author: {self.min_papers_per_author})..."
        )

        # ìµœì†Œ ë…¼ë¬¸ ìˆ˜ ì´ìƒì˜ ì €ìë§Œ ì„ íƒ
        active_authors = {
            author
            for author, stats in self.author_stats.items()
            if stats["paper_count"] >= self.min_papers_per_author
        }

        print(f"   ğŸ“Š Authors before filtering: {len(self.author_stats)}")
        print(f"   âœ… Authors after filtering: {len(active_authors)}")

        # ì €ì í†µê³„ í•„í„°ë§
        self.author_stats = {
            author: stats
            for author, stats in self.author_stats.items()
            if author in active_authors
        }

        self.author_papers = {
            author: papers
            for author, papers in self.author_papers.items()
            if author in active_authors
        }

        # í•„í„°ë§ëœ ì €ìë“¤ê³¼ ê´€ë ¨ëœ ë…¼ë¬¸ë“¤ë§Œ ìœ ì§€
        active_papers = set()
        for author in active_authors:
            for paper_info in self.author_papers[author]:
                active_papers.add(paper_info["paper_id"])

        filtered_paper_authors = {}
        filtered_paper_stats = {}

        for paper_id in active_papers:
            if paper_id in self.paper_authors:
                # ë…¼ë¬¸ì˜ ì €ì ëª©ë¡ì„ active_authorsë¡œ í•„í„°ë§
                original_authors = self.paper_authors[paper_id]["authors"]
                filtered_authors = [a for a in original_authors if a in active_authors]

                if len(filtered_authors) >= self.min_authors_per_paper:
                    paper_info = self.paper_authors[paper_id].copy()
                    paper_info["authors"] = filtered_authors
                    paper_info["author_count"] = len(filtered_authors)

                    filtered_paper_authors[paper_id] = paper_info

                    if paper_id in self.paper_stats:
                        filtered_paper_stats[paper_id] = self.paper_stats[
                            paper_id
                        ].copy()
                        filtered_paper_stats[paper_id]["author_count"] = len(
                            filtered_authors
                        )
                        filtered_paper_stats[paper_id]["authors"] = filtered_authors

        self.paper_authors = filtered_paper_authors
        self.paper_stats = filtered_paper_stats

        print(f"   ğŸ“„ Active authors remaining: {len(self.author_stats)}")
        print(f"   ğŸ“° Active papers remaining: {len(self.paper_authors)}")

        return active_authors

    def build_author_paper_graph(self):
        """ì €ì-ë…¼ë¬¸ ì´ë¶„ ê·¸ë˜í”„ êµ¬ì¶•"""
        print(f"ğŸ”— Building author-paper bipartite graph...")

        # ë¬´ë°©í–¥ ê·¸ë˜í”„ ìƒì„± (ì €ì-ë…¼ë¬¸ ê´€ê³„ëŠ” ë°©í–¥ì„±ì´ ì—†ìŒ)
        G = nx.Graph()

        # ì €ì ë…¸ë“œ ì¶”ê°€
        for author, stats in self.author_stats.items():
            G.add_node(
                author,
                node_type="author",
                name=author,
                paper_count=stats["paper_count"],
                first_author_count=stats["first_author_count"],
                last_author_count=stats["last_author_count"],
                single_author_count=stats["single_author_count"],
                first_year=stats["first_year"],
                last_year=stats["last_year"],
                active_years=stats["active_years"],
                unique_journals=stats["unique_journals"],
                most_frequent_journal=stats["most_frequent_journal"],
                avg_co_authors=stats["avg_co_authors"],
                productivity_type=stats["productivity_type"],
                top_keywords=[kw for kw, count in stats["top_keywords"]][
                    :5
                ],  # ìƒìœ„ 5ê°œë§Œ
            )

        # ë…¼ë¬¸ ë…¸ë“œ ì¶”ê°€
        for paper_id, stats in self.paper_stats.items():
            G.add_node(
                paper_id,
                node_type="paper",
                title=stats["title"],
                abstract=stats.get("abstract", ""),  # âœ… Abstract ì¶”ê°€
                author_count=stats["author_count"],
                collaboration_type=stats["collaboration_type"],
                year=stats["year"],
                journal=stats["journal"],
                keyword_count=stats["keyword_count"],
                has_experienced_authors=stats["has_experienced_authors"],
                is_interdisciplinary=stats["is_interdisciplinary"],
            )

        # ì €ì-ë…¼ë¬¸ ì—£ì§€ ì¶”ê°€
        edges_added = 0

        for author, papers in self.author_papers.items():
            if G.has_node(author):
                for paper_info in papers:
                    paper_id = paper_info["paper_id"]
                    if G.has_node(paper_id):
                        G.add_edge(
                            author,
                            paper_id,
                            edge_type="authored",
                            authorship_position=paper_info["authorship_position"],
                            is_first_author=paper_info["is_first_author"],
                            is_last_author=paper_info["is_last_author"],
                            co_author_count=paper_info["co_author_count"],
                            year=paper_info["year"],
                            weight=1.0,  # ëª¨ë“  authorship ê´€ê³„ëŠ” ë™ë“±í•œ ê°€ì¤‘ì¹˜
                        )
                        edges_added += 1

        print(f"âœ… Author-paper bipartite graph constructed:")
        print(
            f"   ğŸ‘¥ Author nodes: {sum(1 for n in G.nodes() if G.nodes[n]['node_type'] == 'author')}"
        )
        print(
            f"   ğŸ“„ Paper nodes: {sum(1 for n in G.nodes() if G.nodes[n]['node_type'] == 'paper')}"
        )
        print(f"   ğŸ”— Authorship edges: {G.number_of_edges()}")
        print(f"   ğŸ“ˆ Graph density: {nx.density(G):.6f}")

        return G

    def analyze_graph_properties(self, G):
        """ê·¸ë˜í”„ ì†ì„± ë¶„ì„"""
        print("ğŸ“ˆ Analyzing author-paper graph properties...")

        # ë…¸ë“œë³„ í†µê³„
        author_nodes = [n for n in G.nodes() if G.nodes[n]["node_type"] == "author"]
        paper_nodes = [n for n in G.nodes() if G.nodes[n]["node_type"] == "paper"]

        # ê¸°ë³¸ í†µê³„
        stats = {
            "basic_stats": {
                "num_authors": len(author_nodes),
                "num_papers": len(paper_nodes),
                "num_edges": G.number_of_edges(),
                "density": float(nx.density(G)),
                "avg_papers_per_author": (
                    G.number_of_edges() / len(author_nodes) if author_nodes else 0
                ),
                "avg_authors_per_paper": (
                    G.number_of_edges() / len(paper_nodes) if paper_nodes else 0
                ),
            }
        }

        # ì €ìë³„ ë…¼ë¬¸ ìˆ˜ ë¶„ì„
        author_paper_counts = {}
        for author in author_nodes:
            paper_count = G.degree(author)  # ì—°ê²°ëœ ë…¼ë¬¸ ìˆ˜
            author_paper_counts[author] = paper_count

        # ë…¼ë¬¸ë³„ ì €ì ìˆ˜ ë¶„ì„
        paper_author_counts = {}
        for paper in paper_nodes:
            author_count = G.degree(paper)  # ì—°ê²°ëœ ì €ì ìˆ˜
            paper_author_counts[paper] = author_count

        stats["productivity_analysis"] = {
            "top_authors_by_papers": sorted(
                author_paper_counts.items(), key=lambda x: x[1], reverse=True
            )[:10],
            "author_productivity_distribution": {
                "mean": float(np.mean(list(author_paper_counts.values()))),
                "median": float(np.median(list(author_paper_counts.values()))),
                "max": max(author_paper_counts.values()),
                "min": min(author_paper_counts.values()),
            },
            "collaboration_size_distribution": {
                "mean": float(np.mean(list(paper_author_counts.values()))),
                "median": float(np.median(list(paper_author_counts.values()))),
                "max": max(paper_author_counts.values()),
                "min": min(paper_author_counts.values()),
            },
        }

        # ì €ì ìƒì‚°ì„± íƒ€ì…ë³„ ë¶„ì„
        productivity_analysis = self.analyze_productivity_types(G)
        stats["productivity_types"] = productivity_analysis

        # í˜‘ì—… íŒ¨í„´ ë¶„ì„
        collaboration_analysis = self.analyze_collaboration_patterns(G)
        stats["collaboration_patterns"] = collaboration_analysis

        # ì‹œê°„ì  ë¶„ì„
        temporal_analysis = self.analyze_temporal_patterns(G)
        stats["temporal_patterns"] = temporal_analysis

        return stats

    def analyze_productivity_types(self, G):
        """ì €ì ìƒì‚°ì„± íƒ€ì…ë³„ ë¶„ì„"""
        type_counts = Counter()
        type_papers = defaultdict(int)

        for node in G.nodes():
            if G.nodes[node]["node_type"] == "author":
                productivity_type = G.nodes[node]["productivity_type"]
                paper_count = G.degree(node)

                type_counts[productivity_type] += 1
                type_papers[productivity_type] += paper_count

        return {
            "type_distribution": dict(type_counts),
            "papers_by_type": dict(type_papers),
        }

    def analyze_collaboration_patterns(self, G):
        """í˜‘ì—… íŒ¨í„´ ë¶„ì„"""
        collaboration_types = Counter()

        for node in G.nodes():
            if G.nodes[node]["node_type"] == "paper":
                collab_type = G.nodes[node]["collaboration_type"]
                collaboration_types[collab_type] += 1

        # ì²« ë²ˆì§¸/ë§ˆì§€ë§‰ ì €ì ë¶„ì„
        first_author_papers = 0
        last_author_papers = 0
        single_author_papers = 0

        for edge in G.edges():
            edge_data = G.edges[edge]
            if edge_data.get("is_first_author", False):
                first_author_papers += 1
            if edge_data.get("is_last_author", False):
                last_author_papers += 1
            if edge_data.get("co_author_count", 0) == 0:
                single_author_papers += 1

        return {
            "collaboration_type_distribution": dict(collaboration_types),
            "authorship_patterns": {
                "first_author_papers": first_author_papers,
                "last_author_papers": last_author_papers,
                "single_author_papers": single_author_papers,
            },
        }

    def analyze_temporal_patterns(self, G):
        """ì‹œê°„ì  íŒ¨í„´ ë¶„ì„"""
        papers_by_year = defaultdict(int)
        authors_by_year = defaultdict(set)

        for edge in G.edges():
            year = G.edges[edge].get("year", "")
            if year and str(year).isdigit():
                year_int = int(year)
                papers_by_year[year_int] += 1

                # ì—£ì§€ì˜ ì €ìì™€ ë…¼ë¬¸ ì°¾ê¸°
                node1, node2 = edge
                author = node1 if G.nodes[node1]["node_type"] == "author" else node2
                authors_by_year[year_int].add(author)

        # ë…„ë„ë³„ í™œë™ ì €ì ìˆ˜ ê³„ì‚°
        authors_count_by_year = {
            year: len(authors) for year, authors in authors_by_year.items()
        }

        return {
            "papers_by_year": dict(papers_by_year),
            "authors_count_by_year": authors_count_by_year,
            "active_years": list(papers_by_year.keys()) if papers_by_year else [],
        }

    def create_node_info_maps(self, G):
        """ë…¸ë“œ ì •ë³´ë¥¼ ì‰½ê²Œ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ë§µ ìƒì„±"""
        author_info = {}
        paper_info = {}

        # ì €ì ì •ë³´
        for node in G.nodes():
            if G.nodes[node]["node_type"] == "author":
                node_data = G.nodes[node]
                author_info[node] = {
                    "name": node,
                    "productivity_type": node_data.get("productivity_type", ""),
                    "paper_count": G.degree(node),
                    "first_author_count": node_data.get("first_author_count", 0),
                    "last_author_count": node_data.get("last_author_count", 0),
                    "single_author_count": node_data.get("single_author_count", 0),
                    "first_year": node_data.get("first_year", ""),
                    "last_year": node_data.get("last_year", ""),
                    "active_years": node_data.get("active_years", 0),
                    "unique_journals": node_data.get("unique_journals", 0),
                    "most_frequent_journal": node_data.get("most_frequent_journal", ""),
                    "top_keywords": node_data.get("top_keywords", [])[:3],  # ìƒìœ„ 3ê°œë§Œ
                }

        # ë…¼ë¬¸ ì •ë³´
        for node in G.nodes():
            if G.nodes[node]["node_type"] == "paper":
                node_data = G.nodes[node]

                # í•´ë‹¹ ë…¼ë¬¸ì˜ ì €ìë“¤ ì°¾ê¸°
                authors = [
                    neighbor
                    for neighbor in G.neighbors(node)
                    if G.nodes[neighbor]["node_type"] == "author"
                ]

                paper_info[node] = {
                    "title": node_data.get("title", ""),
                    "abstract": node_data.get("abstract", ""),
                    "author_count": G.degree(node),
                    "collaboration_type": node_data.get("collaboration_type", ""),
                    "year": node_data.get("year", ""),
                    "journal": node_data.get("journal", ""),
                    "keyword_count": node_data.get("keyword_count", 0),
                    "has_experienced_authors": node_data.get(
                        "has_experienced_authors", False
                    ),
                    "is_interdisciplinary": node_data.get(
                        "is_interdisciplinary", False
                    ),
                    "authors": authors[:5],  # ì²˜ìŒ 5ëª…ë§Œ
                }

        return author_info, paper_info

    def save_author_paper_graph_and_analysis(self, G, stats, output_dir):
        """Author-paper ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(output_dir)

        # 1. NetworkX ê·¸ë˜í”„ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (GraphRAGìš©)
        graph_data = {"nodes": [], "edges": []}

        # ë…¸ë“œ ì •ë³´
        for node in G.nodes():
            node_data = G.nodes[node].copy()
            node_data["id"] = node
            # ë¦¬ìŠ¤íŠ¸ í•„ë“œë“¤ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ìœ ì§€
            graph_data["nodes"].append(node_data)

        # ì—£ì§€ ì •ë³´
        for edge in G.edges():
            edge_data = G.edges[edge].copy()
            edge_data["source"] = edge[0]
            edge_data["target"] = edge[1]
            graph_data["edges"].append(edge_data)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        graph_file = output_dir / "author_paper_graph.json"
        with open(graph_file, "w", encoding="utf-8") as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)

        # 2. GraphML íŒŒì¼ë¡œ ì €ì¥
        try:
            # GraphML í˜¸í™˜ì„ ìœ„í•´ ê·¸ë˜í”„ ë³µì‚¬ ë° ì†ì„± ë³€í™˜
            G_graphml = G.copy()

            # ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì†ì„±ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            for node in G_graphml.nodes():
                for attr_name, attr_value in G_graphml.nodes[node].items():
                    if isinstance(attr_value, list):
                        G_graphml.nodes[node][attr_name] = ";".join(
                            str(v) for v in attr_value
                        )
                    elif not isinstance(attr_value, (str, int, float, bool)):
                        G_graphml.nodes[node][attr_name] = str(attr_value)

            graphml_file = output_dir / "author_paper_graph.graphml"
            nx.write_graphml(G_graphml, graphml_file)

        except Exception as e:
            print(f"âš ï¸  GraphML ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“„ JSON íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {graph_file}")
            graphml_file = None

        # 3. ë¶„ì„ ê²°ê³¼ ì €ì¥
        stats_file = output_dir / "author_paper_analysis.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        # 4. ë…¸ë“œ ì •ë³´ í…Œì´ë¸”ë“¤ ì €ì¥
        author_info, paper_info = self.create_node_info_maps(G)

        author_info_file = output_dir / "author_paper_author_info.json"
        with open(author_info_file, "w", encoding="utf-8") as f:
            json.dump(author_info, f, ensure_ascii=False, indent=2)

        paper_info_file = output_dir / "author_paper_paper_info.json"
        with open(paper_info_file, "w", encoding="utf-8") as f:
            json.dump(paper_info, f, ensure_ascii=False, indent=2)

        # 5. CSV í˜•íƒœì˜ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        edge_list = []
        for edge in G.edges():
            edge_info = G.edges[edge].copy()

            # ì €ìì™€ ë…¼ë¬¸ êµ¬ë¶„
            node1, node2 = edge
            if G.nodes[node1]["node_type"] == "author":
                author_id, paper_id = node1, node2
            else:
                author_id, paper_id = node2, node1

            edge_info.update(
                {
                    "author_id": author_id,
                    "paper_id": paper_id,
                    "author_name": author_id,
                    "paper_title": G.nodes[paper_id].get("title", ""),
                    "paper_year": G.nodes[paper_id].get("year", ""),
                    "author_productivity_type": G.nodes[author_id].get(
                        "productivity_type", ""
                    ),
                    "collaboration_type": G.nodes[paper_id].get(
                        "collaboration_type", ""
                    ),
                }
            )
            edge_list.append(edge_info)

        edge_df = pd.DataFrame(edge_list)
        edge_file = output_dir / "author_paper_edges.csv"
        edge_df.to_csv(edge_file, index=False, encoding="utf-8")

        print(f"ğŸ’¾ Author-paper graph results saved:")
        print(f"   ğŸ”— Graph (JSON): {graph_file}")
        if graphml_file:
            print(f"   ğŸ”— Graph (GraphML): {graphml_file}")
        print(f"   ğŸ“Š Analysis: {stats_file}")
        print(f"   ğŸ‘¥ Author Info: {author_info_file}")
        print(f"   ğŸ“„ Paper Info: {paper_info_file}")
        print(f"   ğŸ“ˆ Edge List: {edge_file}")

        return graph_file

    def process_papers(self, papers_metadata):
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ Starting author-paper graph construction...")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.papers_metadata = papers_metadata

        # 1. ì €ì-ë…¼ë¬¸ ê´€ê³„ ì¶”ì¶œ
        self.extract_author_paper_relationships(papers_metadata)

        # 2. ì €ì í†µê³„ ê³„ì‚°
        self.calculate_author_statistics()

        # 3. ë…¼ë¬¸ í†µê³„ ê³„ì‚°
        self.calculate_paper_statistics()

        # 4. í™œë™ ìˆ˜ì¤€ ê¸°ì¤€ í•„í„°ë§
        active_authors = self.filter_by_activity()

        # 5. ì´ë¶„ ê·¸ë˜í”„ êµ¬ì¶•
        G = self.build_author_paper_graph()

        # 6. ê·¸ë˜í”„ ë¶„ì„
        stats = self.analyze_graph_properties(G)

        return G, stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from src import RAW_EXTRACTIONS_DIR, GRAPHS_DIR

    # í†µí•© ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_file = RAW_EXTRACTIONS_DIR / "integrated_papers_metadata.json"

    if not metadata_file.exists():
        print(f"âŒ Metadata not found: {metadata_file}")
        print("Please run data processing pipeline first.")
        return

    with open(metadata_file, "r", encoding="utf-8") as f:
        papers_metadata = json.load(f)

    print(f"ğŸ“„ Loaded {len(papers_metadata)} papers metadata")

    # Author-Paper Graph Builder ì´ˆê¸°í™”
    builder = AuthorPaperGraphBuilder(
        min_papers_per_author=1,  # ìµœì†Œ 1í¸ ì´ìƒ ë…¼ë¬¸ì´ ìˆëŠ” ì €ìë§Œ í¬í•¨
        min_authors_per_paper=1,  # ìµœì†Œ 1ëª… ì´ìƒ ì €ìê°€ ìˆëŠ” ë…¼ë¬¸ë§Œ í¬í•¨
    )

    # ì „ì²´ ì²˜ë¦¬
    G, stats = builder.process_papers(papers_metadata)

    # ê²°ê³¼ ì €ì¥
    output_file = builder.save_author_paper_graph_and_analysis(G, stats, GRAPHS_DIR)

    print(f"\nâœ… Author-paper graph construction completed!")
    print(f"ğŸ“ Main output: {output_file}")

    # ìš”ì•½ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š Final Summary:")
    print(f"   ğŸ‘¥ Total authors: {stats['basic_stats']['num_authors']}")
    print(f"   ğŸ“„ Total papers: {stats['basic_stats']['num_papers']}")
    print(f"   ğŸ”— Authorship relationships: {stats['basic_stats']['num_edges']}")
    print(
        f"   ğŸ“ˆ Avg papers per author: {stats['basic_stats']['avg_papers_per_author']:.1f}"
    )
    print(
        f"   ğŸ“ˆ Avg authors per paper: {stats['basic_stats']['avg_authors_per_paper']:.1f}"
    )

    if stats.get("productivity_types"):
        type_dist = stats["productivity_types"]["type_distribution"]
        print(f"   ğŸ“‹ Author productivity types:")
        for prod_type, count in sorted(
            type_dist.items(), key=lambda x: x[1], reverse=True
        ):
            papers_count = stats["productivity_types"]["papers_by_type"].get(
                prod_type, 0
            )
            print(f"      {prod_type}: {count} authors, {papers_count} papers")

    if stats.get("collaboration_patterns"):
        collab_dist = stats["collaboration_patterns"]["collaboration_type_distribution"]
        print(f"   ğŸ¤ Collaboration patterns:")
        for collab_type, count in sorted(
            collab_dist.items(), key=lambda x: x[1], reverse=True
        ):
            print(f"      {collab_type}: {count} papers")

    if stats.get("productivity_analysis"):
        top_authors = stats["productivity_analysis"]["top_authors_by_papers"][:3]
        print(f"   ğŸ† Top 3 most productive authors:")
        for i, (author, paper_count) in enumerate(top_authors):
            productivity_type = G.nodes[author].get("productivity_type", "")
            print(f"      {i+1}. {author} ({productivity_type}) - {paper_count} papers")

    return G, output_file


if __name__ == "__main__":
    main()
