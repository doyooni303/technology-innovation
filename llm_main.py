#!/usr/bin/env python3
"""
LLM ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ - ë‹¨ì¼ íŒŒì¼ ì™„ì „íŒ
í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì¿¼ë¦¬ë¥¼ ì½ì–´ì™€ì„œ LLMì— ì§ì ‘ ì…ë ¥í•˜ê³  ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
"""

import os
import csv
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import warnings

warnings.filterwarnings("ignore")

# Hugging Face imports
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
    BitsAndBytesConfig,
)
import torch

os.environ["TOKENIZERS_PARALLELISM"] = "true"  # í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # GPU ì‚¬ìš© ì„¤ì • (1ë²ˆ GPU)
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # CUDA ë™ê¸°í™” ëª¨ë“œ (ë””ë²„ê¹… ìš©ë„)
# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class LLMBatchProcessor:
    """LLM ë°°ì¹˜ ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        model_name: str = "meta-llama/Llama-2-7b-chat-hf",
        use_quantization: bool = True,
        max_new_tokens: int = 512,
        temperature: float = 0.1,
    ):
        self.model_name = model_name
        self.use_quantization = use_quantization
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.pipeline = None

    def initialize_model(self) -> bool:
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            logger.info(f"ğŸ¤– Initializing LLM: {self.model_name}")

            # ì–‘ìí™” ì„¤ì •
            quantization_config = None
            if self.use_quantization:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                )

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, trust_remote_code=True
            )
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
            )

            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            self.pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=self.max_new_tokens,
                temperature=self.temperature,
                do_sample=False,
                return_full_text=False,
            )

            logger.info("âœ… LLM model initialized successfully!")
            return True

        except Exception as e:
            logger.error(f"âŒ Model initialization failed: {e}")
            return False

    def create_prompt(self, query: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = """Please answer the following question in Korean based on the provided context. 
Provide a comprehensive and technical answer that is easy to understand.

Question: {query}

Please provide your answer in Korean:"""

    def generate_response(self, query: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        if self.pipeline is None:
            return {"success": False, "error": "Model not initialized"}

        start_time = time.time()

        try:
            prompt = self.create_prompt(query)
            result = self.pipeline(prompt)

            if result and len(result) > 0:
                response = result[0]["generated_text"].strip()
                processing_time = time.time() - start_time

                return {
                    "success": True,
                    "response": response,
                    "processing_time": processing_time,
                    "prompt": prompt,
                    "error": "",
                }
            else:
                return {"success": False, "error": "No response generated"}

        except Exception as e:
            processing_time = time.time() - start_time
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
                "response": "",
                "prompt": "",
            }

    def load_queries(self, query_file: str) -> List[str]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì¿¼ë¦¬ ë¡œë“œ"""
        query_path = Path(query_file)
        if not query_path.exists():
            raise FileNotFoundError(f"Query file not found: {query_file}")

        queries = []
        with open(query_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                query = line.strip()
                if query and not query.startswith("#"):
                    queries.append(query)

        logger.info(f"âœ… Loaded {len(queries)} queries from {query_file}")
        return queries

    def save_csv_results(self, results: List[Dict], output_path: Path):
        """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥ (í•œê¸€ ì§€ì› - ë‹¤ì¤‘ ì¸ì½”ë”©)"""
        fieldnames = [
            "query_index",
            "query",
            "response",
            "prompt",
            "processing_time",
            "success",
            "error_msg",
            "timestamp",
        ]

        # 1. UTF-8 BOMìœ¼ë¡œ ì €ì¥ (Excel í˜¸í™˜)
        with open(output_path, "w", newline="", encoding="utf-8-sig") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for result in results:
                writer.writerow(result)

        # 2. UTF-8 ë²„ì „ë„ ì €ì¥ (í”„ë¡œê·¸ë˜ë° ìš©ë„)
        utf8_path = output_path.with_name(output_path.stem + "_utf8.csv")
        with open(utf8_path, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for result in results:
                writer.writerow(result)

        # 3. CP949 ì €ì¥ (í•œêµ­ì–´ Windows í˜¸í™˜)
        try:
            cp949_path = output_path.with_name(output_path.stem + "_cp949.csv")
            with open(cp949_path, "w", newline="", encoding="cp949") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for result in results:
                    # CP949ë¡œ ì¸ì½”ë”©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì²˜ë¦¬
                    safe_row = {}
                    for key, value in result.items():
                        if isinstance(value, str):
                            try:
                                value.encode("cp949")
                                safe_row[key] = value
                            except UnicodeEncodeError:
                                # ì¸ì½”ë”© ë¶ˆê°€ëŠ¥í•œ ë¬¸ìëŠ” ëŒ€ì²´
                                safe_row[key] = value.encode(
                                    "cp949", errors="replace"
                                ).decode("cp949")
                        else:
                            safe_row[key] = value
                    writer.writerow(safe_row)

            logger.info(
                f"âœ… Results saved to {output_path}, {utf8_path}, and {cp949_path}"
            )

        except Exception as e:
            logger.warning(f"âš ï¸ CP949 encoding failed: {e}")
            logger.info(f"âœ… Results saved to {output_path} and {utf8_path}")

    def process_batch(self, query_file: str, output_file: str):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        if self.pipeline is None:
            raise ValueError("Model not initialized. Call initialize_model() first.")

        # ì¿¼ë¦¬ ë¡œë“œ
        queries = self.load_queries(query_file)

        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        results_dir = Path("./results/LLM")
        results_dir.mkdir(parents=True, exist_ok=True)
        output_path = results_dir / output_file

        logger.info(f"ğŸš€ Starting batch processing of {len(queries)} queries...")

        all_results = []
        success_count = 0
        start_time = time.time()

        for i, query in enumerate(queries):
            logger.info(f"ğŸ”„ Processing query {i+1}/{len(queries)}: {query[:50]}...")

            # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            result = self.generate_response(query)
            timestamp = datetime.now().isoformat()

            # CSV í–‰ ë°ì´í„° ì¤€ë¹„
            csv_row = {
                "query_index": i + 1,
                "query": query,
                "response": result.get("response", ""),
                "prompt": result.get("prompt", "")[:500] + "...",  # ê¸¸ì´ ì œí•œ
                "processing_time": round(result.get("processing_time", 0), 2),
                "success": result["success"],
                "error_msg": result.get("error", ""),
                "timestamp": timestamp,
            }

            all_results.append(csv_row)

            if result["success"]:
                success_count += 1
                logger.info(
                    f"âœ… Query {i+1} completed in {csv_row['processing_time']}s"
                )
            else:
                logger.error(f"âŒ Query {i+1} failed: {result['error']}")

            # ì§„í–‰ë¥  í‘œì‹œ
            progress = (i + 1) / len(queries) * 100
            logger.info(f"ğŸ“Š Progress: {i+1}/{len(queries)} ({progress:.1f}%)")

            # ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            if i < len(queries) - 1:
                time.sleep(1.0)

        # ê²°ê³¼ ì €ì¥
        self.save_csv_results(all_results, output_path)

        # í†µê³„ ì¶œë ¥
        total_time = time.time() - start_time
        success_rate = success_count / len(queries) * 100 if queries else 0

        logger.info(f"ğŸ‰ Batch processing completed!")
        logger.info(f"   ğŸ“Š Total time: {total_time:.2f}s")
        logger.info(
            f"   âœ… Success rate: {success_count}/{len(queries)} ({success_rate:.1f}%)"
        )
        logger.info(f"   ğŸ“„ Results saved to: {output_path}")


def check_prerequisites() -> bool:
    """ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    logger.info("ğŸ” Checking prerequisites...")

    try:
        # GPU í™•ì¸
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"   GPU: âœ… {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            logger.info("   GPU: âŒ Using CPU (will be slower)")

        # ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        import transformers

        logger.info(f"   Transformers: âœ… v{transformers.__version__}")

        return True

    except Exception as e:
        logger.error(f"âŒ Prerequisites check failed: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="LLM Batch Processing")
    parser.add_argument(
        "query_file",
        default="./queries.txt",
        help="Input text file with queries (one per line)",
    )
    parser.add_argument(
        "-o", "--output", default="llm_batch_results.csv", help="Output CSV file name"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="meta-llama/Llama-3.1-8B-Instruct",
        help="LLM ëª¨ë¸ ì´ë¦„",
    )
    parser.add_argument(
        "--max-tokens", type=int, default=512, help="Maximum new tokens to generate"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.1, help="Temperature for generation"
    )
    parser.add_argument(
        "--no-quantization", action="store_true", help="Disable model quantization"
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose logging")

    args = parser.parse_args()

    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    logger.info("ğŸš€ LLM Batch Processing Started")
    logger.info(f"   Input file: {args.query_file}")
    logger.info(f"   Output file: ./results/LLM/{args.output}")
    logger.info(f"   Model: {args.model}")
    logger.info(f"   Max tokens: {args.max_tokens}")
    logger.info(f"   Temperature: {args.temperature}")
    logger.info(f"   Quantization: {'disabled' if args.no_quantization else 'enabled'}")

    try:
        # ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not check_prerequisites():
            logger.error("âŒ Prerequisites check failed. Exiting.")
            return 1

        # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        processor = LLMBatchProcessor(
            model_name=args.model,
            use_quantization=not args.no_quantization,
            max_new_tokens=args.max_tokens,
            temperature=args.temperature,
        )

        # ëª¨ë¸ ì´ˆê¸°í™”
        if not processor.initialize_model():
            logger.error("âŒ Model initialization failed. Exiting.")
            return 1

        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        processor.process_batch(args.query_file, args.output)

        logger.info("ğŸ‰ Batch processing completed successfully!")
        logger.info("ğŸ’¡ File usage guide:")
        logger.info("   - Main file (.csv): UTF-8 BOM, opens directly in Excel")
        logger.info("   - _utf8.csv: Pure UTF-8 for programming")

        return 0

    except KeyboardInterrupt:
        logger.warning("âš ï¸ Process interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"âŒ Batch processing failed: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================
"""
ğŸ“ ì¿¼ë¦¬ íŒŒì¼ í˜•ì‹ (queries.txt):
ë°°í„°ë¦¬ SOC ì˜ˆì¸¡ì— ì‚¬ìš©ëœ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì˜ í•µì‹¬ì€ ë¬´ì—‡ì¸ê°€ìš”?
ì‹¤í—˜ ê²°ê³¼ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì€ ë¬´ì—‡ì¸ê°€ìš”?
# ì´ê²ƒì€ ì£¼ì„ì…ë‹ˆë‹¤ (ë¬´ì‹œë¨)
ì–´ë–¤ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆë‚˜ìš”?

ğŸš€ ì‹¤í–‰ ë°©ë²•:
1. ê¸°ë³¸ ì‹¤í–‰:
   python simple_llm_batch.py queries.txt

2. ì¶œë ¥ íŒŒì¼ëª… ì§€ì •:
   python simple_llm_batch.py queries.txt -o my_results.csv

3. ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©:
   python simple_llm_batch.py queries.txt --model "microsoft/DialoGPT-large"

4. ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •:
   python simple_llm_batch.py queries.txt --max-tokens 1024 --temperature 0.3

5. ìƒì„¸ ë¡œê·¸:
   python simple_llm_batch.py queries.txt -v

ğŸ“ ê²°ê³¼ íŒŒì¼:
- ./results/LLM/llm_batch_results.csv (UTF-8 BOM - Excelìš©)
- ./results/LLM/llm_batch_results_utf8.csv (UTF-8 - í”„ë¡œê·¸ë˜ë°ìš©)

ğŸ”§ ì£¼ìš” íŠ¹ì§•:
âœ… ë‹¨ì¼ íŒŒì¼ë¡œ ì™„ì „ ì‹¤í–‰ ê°€ëŠ¥
âœ… í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì¿¼ë¦¬ ë¡œë“œ (í•œ ì¤„ì”©)
âœ… í•œê¸€ CSV ì €ì¥ (Excel í˜¸í™˜)
âœ… 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
âœ… ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
âœ… GPU/CPU ìë™ ê°ì§€
âœ… ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
"""
