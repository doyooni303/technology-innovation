# qa_chain_debug_patch.py - QA Chainì— ë””ë²„ê¹… ê¸°ëŠ¥ ì¶”ê°€

import logging
from pathlib import Path

logger = logging.getLogger(__name__)


def patch_qa_chain_for_debugging():
    """QA Chainì— ë””ë²„ê¹… ê¸°ëŠ¥ ì¶”ê°€í•˜ëŠ” íŒ¨ì¹˜"""

    from src.graphrag.langchain.qa_chain_builder import GraphRAGQAChain

    # ì›ë³¸ ë©”ì„œë“œ ë°±ì—…
    original_process_basic_qa = GraphRAGQAChain._process_basic_qa
    original_process_graph_enhanced_qa = GraphRAGQAChain._process_graph_enhanced_qa

    def debug_process_basic_qa(self, question: str, run_manager=None):
        """ë””ë²„ê¹…ì´ ì¶”ê°€ëœ ê¸°ë³¸ QA ì²˜ë¦¬"""

        print(f"\nğŸ” [DEBUG] Basic QA ì²˜ë¦¬ ì‹œì‘")
        print(f"ì§ˆë¬¸: {question}")
        print("-" * 60)

        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            print("ğŸ“‹ 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰")
            docs = self.retriever.get_relevant_documents(question)
            print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

            for i, doc in enumerate(docs[:3]):  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                print(f"\në¬¸ì„œ {i+1}:")
                print(f"  ë‚´ìš©: {doc.page_content[:150]}...")
                print(f"  ë©”íƒ€ë°ì´í„°: {doc.metadata}")

            # 2. ì»¨í…ìŠ¤íŠ¸ ê²°í•©
            print(f"\nğŸ“ 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ê²°í•©")
            context = "\n\n".join([doc.page_content for doc in docs[:5]])
            print(f"ê²°í•©ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")

            if len(context) > 0:
                print(f"ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n{context[:200]}...")
            else:
                print("âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            print(f"\nğŸ¯ 3ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±")
            prompt_inputs = {
                "context": context,
                "input": question,
            }

            formatted_prompt = self.prompt_template.format(**prompt_inputs)

            print(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(formatted_prompt)} ë¬¸ì")
            print(f"\nğŸ“„ ìµœì¢… í”„ë¡¬í”„íŠ¸:")
            print("=" * 60)
            print(formatted_prompt)
            print("=" * 60)

            # 4. LLM í˜¸ì¶œ
            print(f"\nğŸ¤– 4ë‹¨ê³„: LLM í˜¸ì¶œ")
            answer = self.llm.invoke(formatted_prompt)
            answer_str = answer if isinstance(answer, str) else str(answer)

            print(f"LLM ì‘ë‹µ ê¸¸ì´: {len(answer_str)} ë¬¸ì")
            print(f"LLM ì‘ë‹µ: {answer_str[:200]}...")

            # ì›ë³¸ ë©”ì„œë“œ ê²°ê³¼ì™€ ë™ì¼í•œ í˜•íƒœë¡œ ë°˜í™˜
            from src.graphrag.langchain.qa_chain_builder import process_source_documents

            processed_context = process_source_documents(docs)

            result = {
                "answer": answer_str,
                "source_documents": processed_context,
            }

            print(f"\nâœ… Basic QA ì²˜ë¦¬ ì™„ë£Œ")
            return result

        except Exception as e:
            print(f"\nâŒ Basic QA ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()

            return {
                "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "source_documents": [],
            }

    def debug_process_graph_enhanced_qa(
        self, question: str, query_analysis, run_manager
    ):
        """ë””ë²„ê¹…ì´ ì¶”ê°€ëœ GraphRAG QA ì²˜ë¦¬"""

        print(f"\nğŸ” [DEBUG] Graph Enhanced QA ì²˜ë¦¬ ì‹œì‘")
        print(f"ì§ˆë¬¸: {question}")
        if query_analysis:
            print(
                f"ì¿¼ë¦¬ ë¶„ì„: {query_analysis.query_type.value} ({query_analysis.complexity.value})"
            )
        print("-" * 60)

        try:
            if self.retrieval_chain:
                print("ğŸ“‹ LangChain Retrieval Chain ì‚¬ìš©")
                result = self.retrieval_chain.invoke({"input": question})

                # context ì²˜ë¦¬ ë””ë²„ê¹…
                context_docs = result.get("context", [])
                print(f"Retrieval Chain ê²°ê³¼ ë¬¸ì„œ ìˆ˜: {len(context_docs)}")

                for i, doc in enumerate(context_docs[:2]):
                    print(f"ë¬¸ì„œ {i+1}: {doc.page_content[:100]}...")

                from src.graphrag.langchain.qa_chain_builder import (
                    process_source_documents,
                )

                processed_context = process_source_documents(context_docs)

            else:
                print("ğŸ“‹ ì§ì ‘ Retriever ì‚¬ìš©")
                docs = self.retriever.get_relevant_documents(question)
                print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

                context = "\n\n".join([doc.page_content for doc in docs])
                print(f"ê²°í•©ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")

                # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ë””ë²„ê¹…
                if hasattr(self.prompt_template, "input_variables"):
                    print(f"í”„ë¡¬í”„íŠ¸ ì…ë ¥ ë³€ìˆ˜: {self.prompt_template.input_variables}")

                    if "question" in self.prompt_template.input_variables:
                        prompt_inputs = {"context": context, "question": question}
                    else:
                        prompt_inputs = {"context": context, "input": question}
                else:
                    prompt_inputs = {"context": context, "question": question}

                print(f"í”„ë¡¬í”„íŠ¸ ì…ë ¥: {list(prompt_inputs.keys())}")

                formatted_prompt = self.prompt_template.format(**prompt_inputs)
                print(f"\nğŸ“„ ìµœì¢… í”„ë¡¬í”„íŠ¸ (ì• 300ì):")
                print(formatted_prompt[:300] + "...")

                answer = self.llm.invoke(formatted_prompt)

                from src.graphrag.langchain.qa_chain_builder import (
                    process_source_documents,
                )

                processed_context = process_source_documents(docs)
                result = {"answer": str(answer), "context": processed_context}

            print(f"\nâœ… Graph Enhanced QA ì²˜ë¦¬ ì™„ë£Œ")
            return {
                "answer": result.get("answer", ""),
                "source_documents": result.get("context", processed_context),
            }

        except Exception as e:
            print(f"\nâŒ Graph Enhanced QA ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()

            return {
                "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "source_documents": [],
            }

    # ë©”ì„œë“œ íŒ¨ì¹˜ ì ìš©
    GraphRAGQAChain._process_basic_qa = debug_process_basic_qa
    GraphRAGQAChain._process_graph_enhanced_qa = debug_process_graph_enhanced_qa

    print("âœ… QA Chain ë””ë²„ê¹… íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")


def create_debug_qa_chain():
    """ë””ë²„ê¹…ì´ í™œì„±í™”ëœ QA Chain ìƒì„±"""

    # íŒ¨ì¹˜ ì ìš©
    patch_qa_chain_for_debugging()

    # QA Chain Builder ìƒì„±
    from src.graphrag.langchain.qa_chain_builder import QAChainBuilder

    builder = QAChainBuilder(
        unified_graph_path="data/processed/graphs/unified/unified_knowledge_graph.json",
        vector_store_path="data/processed/vector_store",
    )

    # ë””ë²„ê¹…ìš© ì„¤ì •
    qa_chain = builder.create_basic_chain(
        max_docs=10,
        min_relevance_score=0.2,  # ë‚®ì€ ì„ê³„ê°’
        enable_memory=False,
        return_source_documents=True,
    )

    return qa_chain


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸš€ QA Chain ë””ë²„ê¹… íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸")

    # ë””ë²„ê¹… QA Chain ìƒì„±
    qa_chain = create_debug_qa_chain()

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    question = "What machine learning techniques are used for battery SoC prediction?"

    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {question}")
    print("=" * 80)

    # QA Chain ì‹¤í–‰ (ë””ë²„ê¹… ì¶œë ¥ê³¼ í•¨ê»˜)
    result = qa_chain.invoke({"question": question})

    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"ë‹µë³€: {result.get('answer', 'N/A')}")
    print(f"ì†ŒìŠ¤ ë¬¸ì„œ ìˆ˜: {len(result.get('source_documents', []))}")
