# quick_fix_retrieval.py - ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ retrieval ë¬¸ì œ í•´ê²°

import logging
from pathlib import Path


def test_different_thresholds_and_models():
    """ë‹¤ì–‘í•œ ì„ê³„ê°’ê³¼ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸"""

    print("ğŸ”§ Retrieval ë¬¸ì œ í•´ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 80)

    from src.graphrag.langchain.custom_retriever import create_graphrag_retriever

    # 1. ì„ê³„ê°’ì„ ë‹¨ê³„ì ìœ¼ë¡œ ë‚®ì¶°ì„œ í…ŒìŠ¤íŠ¸
    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]
    models = [
        "sentence-transformers/all-MiniLM-L6-v2",  # ê°€ì¥ ì¼ë°˜ì 
        "sentence-transformers/all-mpnet-base-v2",  # ê³ ì„±ëŠ¥
        "sentence-transformers/paraphrase-MiniLM-L6-v2",  # ë¹ ë¦„
        "auto",  # ê¸°ë³¸ê°’
    ]

    question = "What machine learning techniques are used for battery SoC prediction?"

    best_result = None
    best_score = 0

    for model in models:
        print(f"\nğŸ¤– í…ŒìŠ¤íŠ¸ ëª¨ë¸: {model}")
        print("-" * 60)

        for threshold in thresholds:
            try:
                print(f"   ì„ê³„ê°’ {threshold}: ", end="")

                retriever = create_graphrag_retriever(
                    unified_graph_path="data/processed/graphs/unified/unified_knowledge_graph.json",
                    vector_store_path="data/processed/vector_store",
                    embedding_model=model,
                    max_docs=10,
                    min_relevance_score=threshold,
                    enable_caching=False,
                )

                docs = retriever.get_relevant_documents(question)

                if docs:
                    confidence = docs[0].metadata.get("confidence_score", 0.0)
                    total_nodes = docs[0].metadata.get("total_nodes", 0)

                    print(
                        f"âœ… {len(docs)}ê°œ ë¬¸ì„œ, ì‹ ë¢°ë„: {confidence:.3f}, ë…¸ë“œ: {total_nodes}"
                    )

                    # ìµœê³  ê²°ê³¼ ì¶”ì 
                    if confidence > best_score:
                        best_score = confidence
                        best_result = {
                            "model": model,
                            "threshold": threshold,
                            "docs": docs,
                            "confidence": confidence,
                        }
                else:
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

            except Exception as e:
                print(f"âŒ ì—ëŸ¬: {str(e)[:50]}...")

    # ìµœê³  ê²°ê³¼ ì¶œë ¥
    if best_result:
        print(f"\nğŸ† ìµœê³  ê²°ê³¼:")
        print(f"   ëª¨ë¸: {best_result['model']}")
        print(f"   ì„ê³„ê°’: {best_result['threshold']}")
        print(f"   ì‹ ë¢°ë„: {best_result['confidence']:.3f}")
        print(f"   ë¬¸ì„œ ìˆ˜: {len(best_result['docs'])}")

        # ìµœê³  ê²°ê³¼ì˜ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
        best_doc = best_result["docs"][0]
        print(f"\nğŸ“„ ìµœê³  ê²°ê³¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
        print(f"{best_doc.page_content[:300]}...")

        return best_result
    else:
        print("\nâŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return None


def test_simplified_queries():
    """ë‹¨ìˆœí™”ëœ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"""

    print(f"\nğŸ” ë‹¨ìˆœí™”ëœ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸")
    print("-" * 60)

    simple_queries = [
        "battery",
        "machine learning",
        "SoC prediction",
        "neural network",
        "deep learning",
        "artificial intelligence",
        "energy",
        "lithium",
    ]

    from src.graphrag.langchain.custom_retriever import create_graphrag_retriever

    # ê°€ì¥ ê´€ëŒ€í•œ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    retriever = create_graphrag_retriever(
        unified_graph_path="data/processed/graphs/unified/unified_knowledge_graph.json",
        vector_store_path="data/processed/vector_store",
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        max_docs=5,
        min_relevance_score=0.1,  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
        enable_caching=False,
    )

    results = []

    for query in simple_queries:
        try:
            docs = retriever.get_relevant_documents(query)
            if docs:
                confidence = docs[0].metadata.get("confidence_score", 0.0)
                total_nodes = docs[0].metadata.get("total_nodes", 0)
                print(
                    f"   '{query}': âœ… {len(docs)}ê°œ, ì‹ ë¢°ë„: {confidence:.3f}, ë…¸ë“œ: {total_nodes}"
                )
                results.append((query, docs, confidence))
            else:
                print(f"   '{query}': âŒ ê²°ê³¼ ì—†ìŒ")
        except Exception as e:
            print(f"   '{query}': âŒ ì—ëŸ¬: {str(e)[:30]}...")

    return results


def inspect_vector_store_directly():
    """ë²¡í„° ìŠ¤í† ì–´ ì§ì ‘ ê²€ì‚¬"""

    print(f"\nğŸ” ë²¡í„° ìŠ¤í† ì–´ ì§ì ‘ ê²€ì‚¬")
    print("-" * 60)

    try:
        # FAISS ì¸ë±ìŠ¤ ì§ì ‘ ë¡œë“œ
        import faiss
        import numpy as np
        from pathlib import Path

        index_path = Path("data/processed/vector_store/faiss/faiss/faiss_index.bin")

        if index_path.exists():
            print(f"âœ… FAISS ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬: {index_path}")

            # ì¸ë±ìŠ¤ ë¡œë“œ
            index = faiss.read_index(str(index_path))
            print(f"   ë²¡í„° ìˆ˜: {index.ntotal}")
            print(f"   ì°¨ì›: {index.d}")

            # ì„ë² ë”© ë©”íƒ€ë°ì´í„° í™•ì¸
            embeddings_dir = Path("data/processed/vector_store/embeddings")
            if embeddings_dir.exists():
                embedding_files = list(embeddings_dir.glob("*.json"))
                print(f"   ì„ë² ë”© ë©”íƒ€ë°ì´í„° íŒŒì¼ ìˆ˜: {len(embedding_files)}")

                # ì²« ë²ˆì§¸ íŒŒì¼ ë‚´ìš© í™•ì¸
                if embedding_files:
                    import json

                    with open(embedding_files[0], "r", encoding="utf-8") as f:
                        sample = json.load(f)
                    print(f"   ìƒ˜í”Œ ë©”íƒ€ë°ì´í„°: {list(sample.keys())[:5]}")

            return True
        else:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {index_path}")
            return False

    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        return False


def create_working_retriever():
    """ì‘ë™í•˜ëŠ” retriever ì„¤ì • ìƒì„±"""

    print(f"\nğŸ› ï¸ ì‘ë™í•˜ëŠ” Retriever ì„¤ì • ìƒì„±")
    print("-" * 60)

    from src.graphrag.langchain.custom_retriever import create_graphrag_retriever

    # ìµœì í™”ëœ ì„¤ì •
    optimal_config = {
        "unified_graph_path": "data/processed/graphs/unified/unified_knowledge_graph.json",
        "vector_store_path": "data/processed/vector_store",
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "max_docs": 15,
        "min_relevance_score": 0.1,  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
        "enable_caching": True,
        "enable_query_analysis": True,
    }

    try:
        retriever = create_graphrag_retriever(**optimal_config)

        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ë¡œ ê²€ì¦
        test_queries = [
            "battery SoC prediction machine learning",
            "neural network energy storage",
            "deep learning lithium battery",
        ]

        working_queries = []

        for query in test_queries:
            docs = retriever.get_relevant_documents(query)
            if docs and docs[0].metadata.get("total_nodes", 0) > 0:
                working_queries.append(query)
                print(f"âœ… ì‘ë™: '{query}' â†’ {len(docs)}ê°œ ë¬¸ì„œ")
            else:
                print(f"âŒ ì‹¤íŒ¨: '{query}'")

        if working_queries:
            print(f"\nğŸ‰ ì‘ë™í•˜ëŠ” ì„¤ì • ë°œê²¬!")
            print(f"   ëª¨ë¸: {optimal_config['embedding_model']}")
            print(f"   ì„ê³„ê°’: {optimal_config['min_relevance_score']}")
            print(f"   ì‘ë™í•˜ëŠ” ì§ˆë¬¸: {len(working_queries)}ê°œ")

            # ì„¤ì •ì„ íŒŒì¼ë¡œ ì €ì¥
            import json

            with open("working_retriever_config.json", "w") as f:
                json.dump(optimal_config, f, indent=2)
            print(f"   ì„¤ì • ì €ì¥: working_retriever_config.json")

            return retriever, optimal_config
        else:
            print(f"âŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì—ì„œ ì‹¤íŒ¨")
            return None, None

    except Exception as e:
        print(f"âŒ Retriever ìƒì„± ì‹¤íŒ¨: {e}")
        return None, None


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ Retrieval ë¬¸ì œ í•´ê²° ì‹œì‘")
    print("=" * 80)

    # 1. ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸
    vector_store_ok = inspect_vector_store_directly()

    if vector_store_ok:
        # 2. ë‹¤ì–‘í•œ ì„ê³„ê°’ê³¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        best_result = test_different_thresholds_and_models()

        # 3. ë‹¨ìˆœ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        simple_results = test_simplified_queries()

        # 4. ì‘ë™í•˜ëŠ” ì„¤ì • ìƒì„±
        working_retriever, config = create_working_retriever()

        print(f"\nâœ… ë¬¸ì œ í•´ê²° ì™„ë£Œ!")

        if working_retriever and config:
            print(f"ì¶”ì²œ ì„¤ì •:")
            print(f"  - ëª¨ë¸: {config['embedding_model']}")
            print(f"  - ì„ê³„ê°’: {config['min_relevance_score']}")
            print(f"  - ìµœëŒ€ ë¬¸ì„œ: {config['max_docs']}")

            print(f"\nğŸ“ ì‚¬ìš©ë²•:")
            print(f"retriever = create_graphrag_retriever(**config)")
    else:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ì„ë² ë”©ì„ ë‹¤ì‹œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
